
@inproceedings{guo_detection_2019,
	address = {Shanghai, China},
	title = {Detection of {Occluded} {Road} {Signs} on {Autonomous} {Driving} {Vehicles}},
	isbn = {978-1-5386-9552-4},
	url = {https://ieeexplore.ieee.org/document/8784797/},
	doi = {10.1109/ICME.2019.00152},
	abstract = {Autonomous driving vehicle relies heavily on its perception system to sense surrounding environments and make driving decisions. One important task on autonomous driving vehicles is to correctly recognize different trafﬁc signs. However, the trafﬁc signs in the wild can be in various conditions, e.g., occluded, deteriorated, or vandalized, and not all of them are recognizable. In this work, we propose a novel system that leverages the perception system on autonomous vehicle to identify occluded road signs in real time. Based on transfer learning, we propose the occluded sign classiﬁcation network (OSCN) that is able to achieve a precision of 96.34\% on a real-world dataset.},
	language = {en},
	urldate = {2023-03-20},
	booktitle = {2019 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Guo, Jingda and Cheng, Xianwei and Chen, Qi and Yang, Qing},
	month = jul,
	year = {2019},
	pages = {856--861},
	file = {Detection_of_Occluded_Road_Signs_on_Autonomous_Driving_Vehicles.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Detection_of_Occluded_Road_Signs_on_Autonomous_Driving_Vehicles.pdf:application/pdf},
}

@inproceedings{bayzidi_traffic_2022,
	address = {Aachen, Germany},
	title = {Traffic {Sign} {Classifiers} {Under} {Physical} {World} {Realistic} {Sticker} {Occlusions}: {A} {Cross} {Analysis} {Study}},
	isbn = {978-1-66548-821-1},
	shorttitle = {Traffic {Sign} {Classifiers} {Under} {Physical} {World} {Realistic} {Sticker} {Occlusions}},
	url = {https://ieeexplore.ieee.org/document/9827143/},
	doi = {10.1109/IV51971.2022.9827143},
	abstract = {Recent adversarial attacks with real world applications are capable of deceiving deep neural networks (DNN), which often appear as printed stickers applied to objects in physical world. Though achieving high success rate in lab tests and limited field tests, such attacks have not been tested on multiple DNN architectures with a standard setup to unveil the common robustness and weakness points of both the DNNs and the attacks. Furthermore, realistic looking stickers applied by normal people as acts of vandalism are not studied to discover their potential risks as well the risk of optimizing the location of such realistic stickers to achieve the maximum performance drop. In this paper, (a) we study the case of realistic looking sticker application effects on traffic sign detectors performance; (b) we use traffic sign image classification as our use case and train and attack 11 of the modern architectures for our analysis; (c) by considering different factors like brightness, blurriness and contrast of the train images in our sticker application procedure, we show that simple image processing techniques can help realistic looking stickers fit into their background to mimic real world tests; (d) by performing structured synthetic and real-world evaluations, we study the difference of various traffic sign classes in terms of their crucial distinctive features among the tested DNNs.},
	language = {en},
	urldate = {2023-03-20},
	booktitle = {2022 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	publisher = {IEEE},
	author = {Bayzidi, Yasin and Smajic, Alen and Huger, Fabian and Moritz, Ruby and Varghese, Serin and Schlicht, Peter and Knoll, Alois},
	month = jun,
	year = {2022},
	pages = {644--650},
	file = {Traffic_Sign_Classifiers_Under_Physical_World_Realistic_Sticker_Occlusions_A_Cross_Analysis_Study.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Traffic_Sign_Classifiers_Under_Physical_World_Realistic_Sticker_Occlusions_A_Cross_Analysis_Study.pdf:application/pdf},
}

@article{dewi_yolo_2021,
	title = {Yolo {V4} for {Advanced} {Traffic} {Sign} {Recognition} {With} {Synthetic} {Training} {Data} {Generated} by {Various} {GAN}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9471877/},
	doi = {10.1109/ACCESS.2021.3094201},
	abstract = {Convolutional Neural Networks (CNN) achieves perfection in trafﬁc sign identiﬁcation with enough annotated training data. The dataset determines the quality of the complete visual system based on CNN. Unfortunately, databases for trafﬁc signs from the majority of the world’s nations are few. In this scenario, Generative Adversarial Networks (GAN) may be employed to produce more realistic and varied training pictures to supplement the actual arrangement of images. The purpose of this research is to describe how the quality of synthetic pictures created by DCGAN, LSGAN, and WGAN is determined. Our work combines synthetic images with original images to enhance datasets and verify the effectiveness of synthetic datasets. We use different numbers and sizes of images for training. Likewise, the Structural Similarity Index (SSIM) and Mean Square Error (MSE) were employed to assess picture quality. Our study quantiﬁes the SSIM difference between the synthetic and actual images. When additional images are used for training, the synthetic image exhibits a high degree of resemblance to the genuine image. The highest SSIM value was achieved when using 200 total images as input and 32 × 32 image size. Further, we augment the original picture dataset with synthetic pictures and compare the original image model to the synthesis image model. For this experiment, we are using the latest iterations of Yolo, Yolo V3, and Yolo V4. After mixing the real image with the synthesized image produced by LSGAN, the recognition performance has been improved, achieving an accuracy of 84.9\% on Yolo V3 and an accuracy of 89.33\% on Yolo V4.},
	language = {en},
	urldate = {2023-03-20},
	journal = {IEEE Access},
	author = {Dewi, Christine and Chen, Rung-Ching and Liu, Yan-Ting and Jiang, Xiaoyi and Hartomo, Kristoko Dwi},
	year = {2021},
	pages = {97228--97242},
	file = {Yolo_V4_for_Advanced_Traffic_Sign_Recognition_With_Synthetic_Training_Data_Generated_by_Various_GAN.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Yolo_V4_for_Advanced_Traffic_Sign_Recognition_With_Synthetic_Training_Data_Generated_by_Various_GAN.pdf:application/pdf},
}

@article{liu_machine_2019,
	title = {Machine {Vision} {Based} {Traffic} {Sign} {Detection} {Methods}: {Review}, {Analyses} and {Perspectives}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Machine {Vision} {Based} {Traffic} {Sign} {Detection} {Methods}},
	url = {https://ieeexplore.ieee.org/document/8746141/},
	doi = {10.1109/ACCESS.2019.2924947},
	abstract = {Trafﬁc signs recognition (TSR) is an important part of some advanced driver-assistance systems (ADASs) and auto driving systems (ADSs). As the ﬁrst key step of TSR, trafﬁc sign detection (TSD) is a challenging problem because of different types, small sizes, complex driving scenes, and occlusions. In recent years, there have been a large number of TSD algorithms based on machine vision and pattern recognition. In this paper, a comprehensive review of the literature on TSD is presented. We divide the reviewed detection methods into ﬁve main categories: color-based methods, shape-based methods, color- and shape-based methods, machine-learning-based methods, and LIDAR-based methods. The methods in each category are also classiﬁed into different subcategories for understanding and summarizing the mechanisms of different methods. For some reviewed methods that lack comparisons on public datasets, we reimplemented part of these methods for comparison. The experimental comparisons and analyses are presented on the reported performance and the performance of our reimplemented methods. Furthermore, future directions and recommendations of the TSD research are given to promote the development of the TSD.},
	language = {en},
	urldate = {2023-03-20},
	journal = {IEEE Access},
	author = {Liu, Chunsheng and Li, Shuang and Chang, Faliang and Wang, Yinhai},
	year = {2019},
	pages = {86578--86596},
	file = {Machine_Vision_Based_Traffic_Sign_Detection_Methods_Review_Analyses_and_Perspectives.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Machine_Vision_Based_Traffic_Sign_Detection_Methods_Review_Analyses_and_Perspectives.pdf:application/pdf},
}

@article{liu_tsingnet_2021,
	title = {{TSingNet}: {Scale}-aware and context-rich feature learning for traffic sign detection and recognition in the wild},
	volume = {447},
	issn = {09252312},
	shorttitle = {{TSingNet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221004239},
	doi = {10.1016/j.neucom.2021.03.049},
	abstract = {Trafﬁc sign detection and recognition in the wild is a challenging task. Existing techniques are often incapable of detecting small or occluded trafﬁc signs because of the scale variation and context loss, which causes semantic gaps between multiple scales. We propose a new trafﬁc sign detection network (TSingNet), which learns scale-aware and context-rich features to effectively detect and recognize small and occluded trafﬁc signs in the wild. Speciﬁcally, TSingNet ﬁrst constructs an attention-driven bilateral feature pyramid network, which draws on both bottom-up and top-down subnets to dually circulate low-, mid-, and high-level foreground semantics in scale self-attention learning. This is to learn scaleaware foreground features and thus narrow down the semantic gaps between multiple scales. An adaptive receptive ﬁeld fusion block with variable dilation rates is then introduced to exploit context-rich representation and suppress the inﬂuence of occlusion at each scale. TSingNet is end-to-end trainable by joint minimization of the scale-aware loss and multi-branch fusion losses, this adds a few parameters but signiﬁcantly improves the detection performance. In extensive experiments with three challenging trafﬁc sign datasets (TT100K, STSD and DFG), TSingNet outperformed state-of-the-art methods for trafﬁc sign detection and recognition in the wild.},
	language = {en},
	urldate = {2023-03-20},
	journal = {Neurocomputing},
	author = {Liu, Yuanyuan and Peng, Jiyao and Xue, Jing-Hao and Chen, Yongquan and Fu, Zhang-Hua},
	month = aug,
	year = {2021},
	pages = {10--22},
	file = {1-s2.0-S0925231221004239-main.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/1-s2.0-S0925231221004239-main.pdf:application/pdf},
}

@article{temel_traffic_2020,
	title = {Traffic {Sign} {Detection} under {Challenging} {Conditions}: {A} {Deeper} {Look} {Into} {Performance} {Variations} and {Spectral} {Characteristics}},
	volume = {21},
	issn = {1524-9050, 1558-0016},
	shorttitle = {Traffic {Sign} {Detection} under {Challenging} {Conditions}},
	url = {http://arxiv.org/abs/1908.11262},
	doi = {10.1109/TITS.2019.2931429},
	abstract = {Trafﬁc signs are critical for maintaining the safety and efﬁciency of our roads. Therefore, we need to carefully assess the capabilities and limitations of automated trafﬁc sign detection systems. Existing trafﬁc sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. To overcome the shortcomings in existing datasets, we introduced the CURE-TSDReal dataset, which is based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. We test the performance of two benchmark algorithms and show that severe conditions can result in an average performance degradation of 29\% in precision and 68\% in recall. We investigate the effect of challenging conditions through spectral analysis and show that challenging conditions can lead to distinct magnitude spectrum characteristics. Moreover, we show that mean magnitude spectrum of changes in video sequences under challenging conditions can be an indicator of detection performance. CURE-TSD-Real dataset is available online at https://github.com/olivesgatech/CURE-TSD.},
	language = {en},
	number = {9},
	urldate = {2023-03-20},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
	month = sep,
	year = {2020},
	note = {arXiv:1908.11262 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing, I.2, I.4, I.5},
	pages = {3663--3673},
	annote = {Comment: 13 pages, 9 figures, 4 tables. IEEE Transactions on Intelligent Transportation Systems, 2019},
	file = {1908.11262.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/1908.11262.pdf:application/pdf},
}

@article{lin_transfer_2018,
	title = {Transfer {Learning} {Based} {Traffic} {Sign} {Recognition} {Using} {Inception}-v3 {Model}},
	volume = {47},
	issn = {1587-3811, 0303-7800},
	url = {https://pp.bme.hu/tr/article/view/11480},
	doi = {10.3311/PPtr.11480},
	abstract = {Traffic sign recognition is critical for advanced driver assistant system and road infrastructure survey. Traditional traffic sign recognition algorithms can't efficiently recognize traffic signs due to its limitation, yet deep learning-based technique requires huge amount of training data before its use, which is time consuming and labor intensive. In this study, transfer learning-based method is introduced for traffic sign recognition and classification, which significantly reduces the amount of training data and alleviates computation expense using Inception-v3 model. In our experiment, Belgium Traffic Sign Database is chosen and augmented by data pre-processing technique. Subsequently the layer-wise features extracted using different convolution and pooling operations are compared and analyzed. Finally transfer learning-based model is repetitively retrained several times with fine-tuning parameters at different learning rate, and excellent reliability and repeatability are observed based on statistical analysis. The results show that transfer learning model can achieve a high-level recognition performance in traffic sign recognition, which is up to 99.18 \% of recognition accuracy at 0.05 learning rate (average accuracy of 99.09 \%). This study would be beneficial in other traffic infrastructure recognition such as road lane marking and roadside protection facilities, and so on.},
	language = {en},
	number = {3},
	urldate = {2023-03-20},
	journal = {Periodica Polytechnica Transportation Engineering},
	author = {Lin, Chunmian and Li, Lin and Luo, Wenting and Wang, Kelvin C. P. and Guo, Jiangang},
	month = aug,
	year = {2018},
	pages = {242--250},
	file = {11480-Article Text PDF-41838-3-10-20190515.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/11480-Article Text PDF-41838-3-10-20190515.pdf:application/pdf},
}

@article{haut_hyperspectral_2019,
	title = {Hyperspectral {Image} {Classification} {Using} {Random} {Occlusion} {Data} {Augmentation}},
	volume = {16},
	issn = {1545-598X, 1558-0571},
	url = {https://ieeexplore.ieee.org/document/8694852/},
	doi = {10.1109/LGRS.2019.2909495},
	abstract = {Convolutional neural networks (CNNs) have become a powerful tool for remotely sensed hyperspectral image (HSI) classiﬁcation due to their great generalization ability and high accuracy. However, owing to the huge amount of parameters that need to be learned and to the complex nature of HSI data itself, these approaches must deal with the important problem of overﬁtting, which can lead to inadequate generalization and loss of accuracy. In order to mitigate this problem, in this letter, we adopt random occlusion, a recently developed data augmentation (DA) method for training CNNs, in which the pixels of different rectangular spatial regions in the HSI are randomly occluded, generating training images with various levels of occlusion and reducing the risk of overﬁtting. Our results with two well-known HSIs reveal that the proposed method helps to achieve better classiﬁcation accuracy with low computational cost.},
	language = {en},
	number = {11},
	urldate = {2023-03-20},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Haut, Juan Mario and Paoletti, Mercedes E. and Plaza, Javier and Plaza, Antonio and Plaza, Li},
	month = nov,
	year = {2019},
	pages = {1751--1755},
	file = {Hyperspectral_Image_Classification_Using_Random_Occlusion_Data_Augmentation.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Hyperspectral_Image_Classification_Using_Random_Occlusion_Data_Augmentation.pdf:application/pdf},
}

@article{liu_mr-cnn_2019,
	title = {{MR}-{CNN}: {A} {Multi}-{Scale} {Region}-{Based} {Convolutional} {Neural} {Network} for {Small} {Traffic} {Sign} {Recognition}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{MR}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8701699/},
	doi = {10.1109/ACCESS.2019.2913882},
	abstract = {Small trafﬁc sign recognition is a challenging problem in computer vision, and its accuracy is important to the safety of intelligent transportation systems (ITS). In this paper, we propose the multiscale region-based convolutional neural network (MR-CNN). At the detection stage, MR-CNN uses a multiscale deconvolution operation to up-sample the features of the deeper convolution layers and concatenates them to those of the shallow layer to construct the fused feature map. The fused feature map has the ability to generate fewer region proposals and achieve higher recall values. At the classiﬁcation stage, we leverage the multi-scale contextual regions to exploit the information surrounding a given object proposal and construct the fused feature for the fully connected layers. The fused feature map inside the region proposal network (RPN) focuses primarily on improving the image resolution and semantic information for small trafﬁc sign detection, while outside the RPN, the fused feature enhances the feature representation by leveraging the contextual information. Finally, we evaluated MR-CNN on the largest dataset, TsinghuaTencent 100K, which is suitable for our problem and more challenging than the GTSDB and GTSRB datasets. The ﬁnal experimental results indicate that the MR-CNN is superior at detecting small trafﬁc signs, and that it achieves the state-of-the-art performance compared with other methods.},
	language = {en},
	urldate = {2023-03-20},
	journal = {IEEE Access},
	author = {Liu, Zhigang and Du, Juan and Tian, Feng and Wen, Jiazheng},
	year = {2019},
	pages = {57120--57128},
	file = {MR-CNN_A_Multi-Scale_Region-Based_Convolutional_Neural_Network_for_Small_Traffic_Sign_Recognition.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/MR-CNN_A_Multi-Scale_Region-Based_Convolutional_Neural_Network_for_Small_Traffic_Sign_Recognition.pdf:application/pdf},
}

@article{bangquan_real-time_2019,
	title = {Real-{Time} {Embedded} {Traffic} {Sign} {Recognition} {Using} {Efficient} {Convolutional} {Neural} {Network}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8698449/},
	doi = {10.1109/ACCESS.2019.2912311},
	abstract = {Trafﬁc sign recognition(TSR) based on deep learning is rapidly developing. Speciﬁcally, TSR contains two technologies, namely, trafﬁc sign classiﬁcation (TSC) and trafﬁc sign detection (TSD). However, the challenge of TSR is to ensure its efﬁciency, which means adequate accuracy, generalization, and speed in real-time by a computationally limited platform. In this paper, we will introduce a new efﬁcient TSC network called ENet (efﬁcient network) and a TSD network called EmdNet (efﬁcient network using multiscale operation and depthwise separable convolution). We used data mining and multiscale operation to improve the accuracy and generalization ability and used depthwise separable convolution (DSC) to improve the speed. The resulting ENet possesses 0.9 M parameters (1/15 the parameters of the start-of-the-art method) while still achieving an accuracy of 98.6 \% on the German Trafﬁc Sign Recognition benchmark (GTSRB). In addition, we design EmdNet’ s backbone network according to the principles of ENet. The EmdNet with the SDD Framework possesses only 6.3 M parameters, which is similar to MobileNet’s scale.},
	language = {en},
	urldate = {2023-03-20},
	journal = {IEEE Access},
	author = {Bangquan, Xie and Xiao Xiong, Weng},
	year = {2019},
	pages = {53330--53346},
	file = {Real-Time_Embedded_Traffic_Sign_Recognition_Using_Efficient_Convolutional_Neural_Network-2.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Real-Time_Embedded_Traffic_Sign_Recognition_Using_Efficient_Convolutional_Neural_Network-2.pdf:application/pdf},
}

@article{zhang_automated_2019,
	title = {Automated {Visual} {Recognizability} {Evaluation} of {Traffic} {Sign} {Based} on {3D} {LiDAR} {Point} {Clouds}},
	volume = {11},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/12/1453},
	doi = {10.3390/rs11121453},
	abstract = {Maintaining the high visual recognizability of trafﬁc signs for trafﬁc safety is a key matter for road network management. Mobile Laser Scanning (MLS) systems provide efﬁcient way of 3D measurement over large-scale trafﬁc environment. This paper presents a quantitative visual recognizability evaluation method for trafﬁc signs in large-scale trafﬁc environment based on trafﬁc recognition theory and MLS 3D point clouds. We ﬁrst propose the Visibility Evaluation Model (VEM) to quantitatively describe the visibility of trafﬁc sign from any given viewpoint, then we proposed the concept of visual recognizability ﬁeld and Trafﬁc Sign Visual Recognizability Evaluation Model (TSVREM) to measure the visual recognizability of a trafﬁc sign. Finally, we present an automatic TSVREM calculation algorithm for MLS 3D point clouds. Experimental results on real MLS 3D point clouds show that the proposed method is feasible and efﬁcient.},
	language = {en},
	number = {12},
	urldate = {2023-03-20},
	journal = {Remote Sensing},
	author = {Zhang, Shanxin and Wang, Cheng and Lin, Lili and Wen, Chenglu and Yang, Chenhui and Zhang, Zhemin and Li, Jonathan},
	month = jun,
	year = {2019},
	pages = {1453},
	file = {remotesensing-11-01453.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/remotesensing-11-01453.pdf:application/pdf},
}

@article{tabernik_deep_2020,
	title = {Deep {Learning} for {Large}-{Scale} {Traffic}-{Sign} {Detection} and {Recognition}},
	volume = {21},
	issn = {1524-9050, 1558-0016},
	url = {https://ieeexplore.ieee.org/document/8709983/},
	doi = {10.1109/TITS.2019.2913588},
	language = {en},
	number = {4},
	urldate = {2023-03-20},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Tabernik, Domen and Skocaj, Danijel},
	month = apr,
	year = {2020},
	pages = {1427--1440},
	file = {Deep_Learning_for_Large-Scale_Traffic-Sign_Detection_and_Recognition.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Deep_Learning_for_Large-Scale_Traffic-Sign_Detection_and_Recognition.pdf:application/pdf},
}

@inproceedings{zhu_traffic-sign_2016,
	address = {Las Vegas, NV, USA},
	title = {Traffic-{Sign} {Detection} and {Classification} in the {Wild}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780601/},
	doi = {10.1109/CVPR.2016.232},
	abstract = {Although promising results have been achieved in the areas of trafﬁc-sign detection and classiﬁcation, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large trafﬁc-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 images containing 30000 trafﬁc-sign instances. These images cover large variations in illuminance and weather conditions. Each trafﬁc-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify trafﬁcsigns. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the trafﬁc-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available1.},
	language = {en},
	urldate = {2023-03-20},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhu, Zhe and Liang, Dun and Zhang, Songhai and Huang, Xiaolei and Li, Baoli and Hu, Shimin},
	month = jun,
	year = {2016},
	pages = {2110--2118},
	file = {Zhu et al. - 2016 - Traffic-Sign Detection and Classification in the W.pdf:/Users/reyvababtista/Zotero/storage/54VXU7JV/Zhu et al. - 2016 - Traffic-Sign Detection and Classification in the W.pdf:application/pdf},
}

@article{cui_context-aware_2022,
	title = {Context-{Aware} {Block} {Net} for {Small} {Object} {Detection}},
	volume = {52},
	issn = {2168-2267, 2168-2275},
	url = {https://ieeexplore.ieee.org/document/9151360/},
	doi = {10.1109/TCYB.2020.3004636},
	abstract = {State-of-the-art object detectors usually progressively downsample the input image until it is represented by small feature maps, which loses the spatial information and compromises the representation of small objects. In this article, we propose a context-aware block net (CAB Net) to improve small object detection by building high-resolution and strong semantic feature maps. To internally enhance the representation capacity of feature maps with high spatial resolution, we delicately design the context-aware block (CAB). CAB exploits pyramidal dilated convolutions to incorporate multilevel contextual information without losing the original resolution of feature maps. Then, we assemble CAB to the end of the truncated backbone network (e.g., VGG16) with a relatively small downsampling factor (e.g., 8) and cast off all following layers. CAB Net can capture both basic visual patterns as well as semantical information of small objects, thus improving the performance of small object detection. Experiments conducted on the benchmark Tsinghua-Tencent 100K and the Airport dataset show that CAB Net outperforms other top-performing detectors by a large margin while keeping real-time speed, which demonstrates the effectiveness of CAB Net for small object detection.},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {IEEE Transactions on Cybernetics},
	author = {Cui, Lisha and Lv, Pei and Jiang, Xiaoheng and Gao, Zhimin and Zhou, Bing and Zhang, Luming and Shao, Ling and Xu, Mingliang},
	month = apr,
	year = {2022},
	pages = {2300--2313},
	file = {Context-Aware_Block_Net_for_Small_Object_Detection.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/2023 Spring/8690 Computer Vision/Assignments/final/Context-Aware_Block_Net_for_Small_Object_Detection.pdf:application/pdf},
}

@article{vega_reproducible_2021,
	title = {Reproducible {Analysis} {Pipeline} for {Data} {Streams}: {Open}-{Source} {Software} to {Process} {Data} {Collected} {With} {Mobile} {Devices}},
	volume = {3},
	issn = {2673-253X},
	shorttitle = {Reproducible {Analysis} {Pipeline} for {Data} {Streams}},
	url = {https://www.frontiersin.org/articles/10.3389/fdgth.2021.769823/full},
	doi = {10.3389/fdgth.2021.769823},
	abstract = {Smartphone and wearable devices are widely used in behavioral and clinical research to collect longitudinal data that, along with ground truth data, are used to create models of human behavior. Mobile sensing researchers often program data processing and analysis code from scratch even though many research teams collect data from similar mobile sensors, platforms, and devices. This leads to signiﬁcant inefﬁciency in not being able to replicate and build on others’ work, inconsistency in quality of code and results, and lack of transparency when code is not shared alongside publications. We provide an overview of Reproducible Analysis Pipeline for Data Streams (RAPIDS), a reproducible pipeline to standardize the preprocessing, feature extraction, analysis, visualization, and reporting of data streams coming from mobile sensors. RAPIDS is formed by a group of R and Python scripts that are executed on top of reproducible virtual environments, orchestrated by a workﬂow management system, and organized following a consistent ﬁle structure for data science projects. We share open source, documented, extensible and tested code to preprocess, extract, and visualize behavioral features from data collected with any Android or iOS smartphone sensing app as well as Fitbit and Empatica wearable devices. RAPIDS allows researchers to process mobile sensor data in a rigorous and reproducible way. This saves time and effort during the data analysis phase of a project and facilitates sharing analysis workﬂows alongside publications.},
	language = {en},
	urldate = {2023-03-22},
	journal = {Frontiers in Digital Health},
	author = {Vega, Julio and Li, Meng and Aguillera, Kwesi and Goel, Nikunj and Joshi, Echhit and Khandekar, Kirtiraj and Durica, Krina C. and Kunta, Abhineeth R. and Low, Carissa A.},
	month = nov,
	year = {2021},
	pages = {769823},
	file = {Reproducible Analysis Pipeline for Data Streams Open-Source Software to Process Data Collected With Mobile Devices.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/Distributed & Intelligent Computing Lab/Papers/RAPS references/Reproducible Analysis Pipeline for Data Streams Open-Source Software to Process Data Collected With Mobile Devices.pdf:application/pdf},
}

@inproceedings{rogers_deep_2019,
	address = {Las Vegas, NV, USA},
	title = {Deep {Learning} at {Your} {Fingertips}},
	isbn = {978-1-5386-5553-5},
	url = {https://ieeexplore.ieee.org/document/8651868/},
	doi = {10.1109/CCNC.2019.8651868},
	abstract = {From SurveyMonkey to Google Forms, online surveys have become a cornerstone of modern research. However, these survey platforms lack the ability to provide advanced analysis to researchers, often requiring expensive third-party analytics software to rectify their shortcomings. We propose to solve this problem by adding data analysis capabilities onto TigerAware, an existing data collection platform. TigerAware offers a generic and customizable tool which allows researchers without technical expertise to create, manage, and deploy custom mobile surveys to participants in real-time. We seek to add data analysis functionalities to the TigerAware platform ranging from basic statistics functions to emotion recognition via deep learning. Our analysis platform uses data collected by TigerAware to give researchers real-time analytics throughout the duration of their study. Through our additions to the TigerAware platform, we present a novel all-in-one tool for researchers to facilitate effective survey creation, survey administration, data collection, and data analysis.},
	language = {en},
	urldate = {2023-03-22},
	booktitle = {2019 16th {IEEE} {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	publisher = {IEEE},
	author = {Rogers, Jonathan and Simmons, Dylan and Shah, Milesh and Rowland, Connor and Shang, Yi},
	month = jan,
	year = {2019},
	pages = {1--4},
	file = {Deep_Learning_at_Your_Fingertips.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/Distributed & Intelligent Computing Lab/Papers/RAPS references/Deep_Learning_at_Your_Fingertips.pdf:application/pdf},
}

@inproceedings{morrison_tigeraware_2018,
	address = {Guangzhou},
	title = {{TigerAware}: {An} {Innovative} {Mobile} {Survey} and {Sensor} {Data} {Collection} and {Analytics} {System}},
	isbn = {978-1-5386-4210-8},
	shorttitle = {{TigerAware}},
	url = {https://ieeexplore.ieee.org/document/8411846/},
	doi = {10.1109/DSC.2018.00025},
	abstract = {From health to community assessment, mobile phones have become a cornerstone of data collection across many areas of research. However, mobile phone-based studies are difﬁcult to develop and deploy, often requiring in house development teams and large portions of research budgets. In this paper, an innovative system called TigerAware is presented to address this issue. TigerAware is developed to offer a generic and customizable tool, which allows researchers to create surveys to collect a wide range of data, including but not limited to question responses, on device sensor data, such as GPS data, and external sensor data, such as blood alcohol level from a Bluetooth breathalyzer. TigerAware is highly modular and uses advanced Web and mobile technologies to incorporate diverse data sources with a rich set of survey question types, requiring little development work by researchers for their individualized studies. TigerAware has been applied to a focus group and several pilot studies and shown excellent capabilities to be easily adapted and deployed for new types of data collection and analytics tasks and a wide range of research ﬁelds.},
	language = {en},
	urldate = {2023-03-22},
	booktitle = {2018 {IEEE} {Third} {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	publisher = {IEEE},
	author = {Morrison, William and Guerdan, Luke and Kanugo, Jayanth and Trull, Timothy and Shang, Yi},
	month = jun,
	year = {2018},
	pages = {115--122},
	file = {TigerAware_An_Innovative_Mobile_Survey_and_Sensor_Data_Collection_and_Analytics_System.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/Distributed & Intelligent Computing Lab/Papers/RAPS references/TigerAware_An_Innovative_Mobile_Survey_and_Sensor_Data_Collection_and_Analytics_System.pdf:application/pdf},
}

@inproceedings{yu_reproducible_2022,
	address = {Atlanta, GA, USA},
	title = {Reproducible {Workflows} for {Exploring} and {Modeling} {EMA} {Data}},
	isbn = {978-1-66547-300-2},
	url = {https://ieeexplore.ieee.org/document/10061712/},
	doi = {10.1109/CIC56439.2022.00026},
	abstract = {Improper use of substances like cannabis may lead to physical, emotional, economic, and social problems. Therefore, it is significant to elucidate the inter-individual and intraindividual influences along with contextual influences that predict the use of cannabis. TigerAware is a mobile survey data collection platform that holds unique promise to advance research in addiction and substance use. This paper presents a novel method to support Ecological Momentary Assessment (EMA) studies. We propose to extract useful information from TigerAware survey data using data mining and machine learning methods, and structure customizable survey analyses into reproducible workflows. Through our analysis pipeline for EMA, researchers are able to discover meaningful information from survey data with minimal duplication of effort and improve the efficiency and rigor of the process.},
	language = {en},
	urldate = {2023-03-22},
	booktitle = {2022 {IEEE} 8th {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	publisher = {IEEE},
	author = {Yu, Ching-Yun and Shang, Yi and Trull, Timothy},
	month = dec,
	year = {2022},
	pages = {117--124},
	file = {CIC_Chingyun_0914.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/Distributed & Intelligent Computing Lab/Papers/RAPS references/CIC_Chingyun_0914.pdf:application/pdf},
}

@article{koster_snakemakescalable_2012,
	title = {Snakemake—a scalable bioinformatics workflow engine},
	volume = {28},
	issn = {1367-4811, 1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/28/19/2520/290322},
	doi = {10.1093/bioinformatics/bts480},
	abstract = {Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.},
	language = {en},
	number = {19},
	urldate = {2023-03-22},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2012},
	pages = {2520--2522},
	file = {Snakemake—a scalable bioinformatics workflow engine.pdf:/Users/reyvababtista/Library/Mobile Documents/com~apple~CloudDocs/Grad Study/Distributed & Intelligent Computing Lab/Papers/RAPS references/Snakemake—a scalable bioinformatics workflow engine.pdf:application/pdf},
}
