
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation, Columbia University , and all contributors. Donate
arxiv logo > cs > arXiv:2005.11401

Help | Advanced Search
Search
Computer Science > Computation and Language
(cs)
[Submitted on 22 May 2020 ( v1 ), last revised 12 Apr 2021 (this version, v4)]
Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
Authors: Patrick Lewis , Ethan Perez , Aleksandra Piktus , Fabio Petroni , Vladimir Karpukhin , Naman Goyal , Heinrich Küttler , Mike Lewis , Wen-tau Yih , Tim Rocktäschel , Sebastian Riedel , Douwe Kiela
Download a PDF of the paper titled Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, by Patrick Lewis and 11 other authors
Download PDF

    Abstract: Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 

Comments: 	Accepted at NeurIPS 2020
Subjects: 	Computation and Language (cs.CL) ; Machine Learning (cs.LG)
Cite as: 	arXiv:2005.11401 [cs.CL]
  	(or arXiv:2005.11401v4 [cs.CL] for this version)
  	https://doi.org/10.48550/arXiv.2005.11401
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Patrick Lewis [ view email ]
[v1] Fri, 22 May 2020 21:34:34 UTC (698 KB)
[v2] Mon, 7 Dec 2020 16:23:06 UTC (767 KB)
[v3] Mon, 29 Mar 2021 10:12:16 UTC (767 KB)
[v4] Mon, 12 Apr 2021 15:42:18 UTC (767 KB)
Full-text links:
Access Paper:

    Download a PDF of the paper titled Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, by Patrick Lewis and 11 other authors
    Download PDF
    Other Formats 

view license
Current browse context:
cs.CL
< prev   |   next >
new | recent | 2005
Change to browse by:
cs
cs.LG
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

8 blog links
( what is this? )
DBLP - CS Bibliography
listing | bibtex
Ethan Perez
Aleksandra Piktus
Fabio Petroni
Vladimir Karpukhin
Naman Goyal
…
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

