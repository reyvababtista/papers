
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation, Columbia University , and all contributors. Donate
arxiv logo > cs > arXiv:2312.10997

Help | Advanced Search
Search
Computer Science > Computation and Language
(cs)
[Submitted on 18 Dec 2023 ( v1 ), last revised 5 Jan 2024 (this version, v4)]
Title: Retrieval-Augmented Generation for Large Language Models: A Survey
Authors: Yunfan Gao , Yun Xiong , Xinyu Gao , Kangxiang Jia , Jinliu Pan , Yuxi Bi , Yi Dai , Jiawei Sun , Qianyu Guo , Meng Wang , Haofen Wang
Download a PDF of the paper titled Retrieval-Augmented Generation for Large Language Models: A Survey, by Yunfan Gao and 9 other authors
Download PDF HTML (experimental)

    Abstract: Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem. 

Comments: 	Ongoing Work
Subjects: 	Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI)
Cite as: 	arXiv:2312.10997 [cs.CL]
  	(or arXiv:2312.10997v4 [cs.CL] for this version)
  	https://doi.org/10.48550/arXiv.2312.10997
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Yunfan Gao [ view email ]
[v1] Mon, 18 Dec 2023 07:47:33 UTC (7,541 KB)
[v2] Fri, 29 Dec 2023 18:25:00 UTC (6,421 KB)
[v3] Wed, 3 Jan 2024 17:04:40 UTC (7,508 KB)
[v4] Fri, 5 Jan 2024 01:18:27 UTC (7,508 KB)
Full-text links:
Access Paper:

    Download a PDF of the paper titled Retrieval-Augmented Generation for Large Language Models: A Survey, by Yunfan Gao and 9 other authors
    Download PDF
    HTML (experimental)
    Other Formats 

view license
Current browse context:
cs.CL
< prev   |   next >
new | recent | 2312
Change to browse by:
cs
cs.AI
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

2 blog links
( what is this? )
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

