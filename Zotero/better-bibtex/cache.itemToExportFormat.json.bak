{"name":"itemToExportFormat","data":[{"itemID":943,"item":{"key":"23KQMQTN","version":1295,"itemType":"blogPost","title":"Inventing the Cloud Century","blogTitle":"Inventing the Cloud Century","creators":[{"name":"Marcus Oppitz","creatorType":"author"}],"tags":[],"collections":[],"relations":{},"dateAdded":"2023-11-07T02:04:39Z","dateModified":"2023-11-07T02:04:56Z","uri":"http://zotero.org/users/11367251/items/23KQMQTN","itemID":943,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400271,"version":0},"$loki":1},{"itemID":1653,"item":{"key":"25KC4JGA","version":2077,"itemType":"conferencePaper","title":"XGBoost: A Scalable Tree Boosting System","date":"2016-08-13","language":"en","shortTitle":"XGBoost","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/2939672.2939785","accessDate":"2023-12-04T00:28:11Z","place":"San Francisco California USA","publisher":"ACM","ISBN":"978-1-4503-4232-2","pages":"785-794","proceedingsTitle":"Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","conferenceName":"KDD '16: The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","DOI":"10.1145/2939672.2939785","creators":[{"firstName":"Tianqi","lastName":"Chen","creatorType":"author"},{"firstName":"Carlos","lastName":"Guestrin","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T00:28:11Z","dateModified":"2023-12-04T00:28:11Z","uri":"http://zotero.org/users/11367251/items/25KC4JGA","itemID":1653,"attachments":[{"key":"NKB5K3ZA","version":2077,"itemType":"attachment","title":"chenXGBoostScalableTree2016.pdf","parentItem":"25KC4JGA","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenXGBoostScalableTree2016.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_NKB5K3ZA/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/2\">Tree Boosting in a NutShell</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/2\">Regularized Learning Objective</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/2\">Gradient Tree Boosting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/3\">Shrinkage and Column Subsampling</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/3\">Split Finding Algorithms</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/3\">Basic Exact Greedy Algorithm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/3\">Approximate Algorithm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/4\">Weighted Quantile Sketch</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/4\">Sparsity-aware Split Finding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/5\">System Design</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/5\">Column Block for Parallel Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/6\">Cache-aware Access</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/7\">Blocks for Out-of-core Computation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/7\">Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/7\">End to End Evaluations</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/7\">System Implementation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/7\">Dataset and Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/8\">Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/8\">Learning to Rank</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/9\">Out-of-core Experiment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/9\">Distributed Experiment</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/10\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_NKB5K3ZA/10\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-12-04T00:28:13Z","dateModified":"2023-12-04T00:28:13Z","uri":"http://zotero.org/users/11367251/items/NKB5K3ZA","localPath":"/Users/reyvababtista/Projects/Papers/chenXGBoostScalableTree2016.pdf","defaultPath":"files/1655/chenXGBoostScalableTree2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400274,"version":0},"$loki":2},{"itemID":264,"item":{"key":"25ZQ6VJF","version":254,"itemType":"conferencePaper","title":"TigerAware: An Innovative Mobile Survey and Sensor Data Collection and Analytics System","abstractNote":"From health to community assessment, mobile phones have become a cornerstone of data collection across many areas of research. However, mobile phone-based studies are difﬁcult to develop and deploy, often requiring in house development teams and large portions of research budgets. In this paper, an innovative system called TigerAware is presented to address this issue. TigerAware is developed to offer a generic and customizable tool, which allows researchers to create surveys to collect a wide range of data, including but not limited to question responses, on device sensor data, such as GPS data, and external sensor data, such as blood alcohol level from a Bluetooth breathalyzer. TigerAware is highly modular and uses advanced Web and mobile technologies to incorporate diverse data sources with a rich set of survey question types, requiring little development work by researchers for their individualized studies. TigerAware has been applied to a focus group and several pilot studies and shown excellent capabilities to be easily adapted and deployed for new types of data collection and analytics tasks and a wide range of research ﬁelds.","date":"6/2018","language":"en","shortTitle":"TigerAware","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8411846/","accessDate":"2023-03-22T23:37:37Z","place":"Guangzhou","publisher":"IEEE","ISBN":"978-1-5386-4210-8","pages":"115-122","proceedingsTitle":"2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)","conferenceName":"2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)","DOI":"10.1109/DSC.2018.00025","creators":[{"firstName":"William","lastName":"Morrison","creatorType":"author"},{"firstName":"Luke","lastName":"Guerdan","creatorType":"author"},{"firstName":"Jayanth","lastName":"Kanugo","creatorType":"author"},{"firstName":"Timothy","lastName":"Trull","creatorType":"author"},{"firstName":"Yi","lastName":"Shang","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-22T23:37:37Z","dateModified":"2023-03-22T23:37:38Z","uri":"http://zotero.org/users/11367251/items/25ZQ6VJF","itemID":264,"attachments":[{"key":"YMVDCJID","version":256,"itemType":"attachment","title":"morrisonTigerAwareInnovativeMobile2018.pdf","parentItem":"25ZQ6VJF","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/morrisonTigerAwareInnovativeMobile2018.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T23:37:35Z","dateModified":"2023-03-22T23:38:37Z","uri":"http://zotero.org/users/11367251/items/YMVDCJID","localPath":"/Users/reyvababtista/Projects/Papers/morrisonTigerAwareInnovativeMobile2018.pdf","defaultPath":"files/263/morrisonTigerAwareInnovativeMobile2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400277,"version":0},"$loki":3},{"itemID":52,"item":{"key":"2ARGMY5P","version":201,"itemType":"conferencePaper","title":"Traffic-Sign Detection and Classification in the Wild","abstractNote":"Although promising results have been achieved in the areas of trafﬁc-sign detection and classiﬁcation, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large trafﬁc-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 images containing 30000 trafﬁc-sign instances. These images cover large variations in illuminance and weather conditions. Each trafﬁc-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify trafﬁcsigns. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the trafﬁc-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available1.","date":"6/2016","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/7780601/","accessDate":"2023-03-20T16:47:52Z","place":"Las Vegas, NV, USA","publisher":"IEEE","ISBN":"978-1-4673-8851-1","pages":"2110-2118","proceedingsTitle":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2016.232","creators":[{"firstName":"Zhe","lastName":"Zhu","creatorType":"author"},{"firstName":"Dun","lastName":"Liang","creatorType":"author"},{"firstName":"Songhai","lastName":"Zhang","creatorType":"author"},{"firstName":"Xiaolei","lastName":"Huang","creatorType":"author"},{"firstName":"Baoli","lastName":"Li","creatorType":"author"},{"firstName":"Shimin","lastName":"Hu","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/B995DFN7"]},"dateAdded":"2023-03-20T16:47:52Z","dateModified":"2023-03-22T17:03:44Z","uri":"http://zotero.org/users/11367251/items/2ARGMY5P","itemID":52,"attachments":[{"key":"7F46LJ3N","version":241,"itemType":"attachment","title":"zhuTrafficSignDetectionClassification2016.pdf","parentItem":"2ARGMY5P","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhuTrafficSignDetectionClassification2016.pdf","tags":[],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/IE84E4CX"]},"dateAdded":"2023-03-22T18:52:09Z","dateModified":"2023-03-22T18:52:24Z","uri":"http://zotero.org/users/11367251/items/7F46LJ3N","localPath":"/Users/reyvababtista/Projects/Papers/zhuTrafficSignDetectionClassification2016.pdf","defaultPath":"files/256/zhuTrafficSignDetectionClassification2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400280,"version":0},"$loki":4},{"itemID":584,"item":{"key":"2FGDLGL2","version":782,"itemType":"attachment","title":"2604.pdf","linkMode":"imported_file","contentType":"application/pdf","charset":"","filename":"2604.pdf","tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:37:51Z","dateModified":"2023-09-05T13:37:51Z","uri":"http://zotero.org/users/11367251/items/2FGDLGL2","itemID":584,"localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/2FGDLGL2/2604.pdf","defaultPath":"files/584/2604.pdf"},"meta":{"revision":0,"created":1709832400282,"version":0},"$loki":5},{"itemID":1115,"item":{"key":"2G8UTJUS","version":1415,"itemType":"preprint","title":"CIDEr: Consensus-based Image Description Evaluation","abstractNote":"Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classiﬁcation, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate ﬁve state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDErD is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.","date":"2015-06-02","language":"en","shortTitle":"CIDEr","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1411.5726","accessDate":"2023-11-08T15:26:02Z","extra":"arXiv:1411.5726 [cs]","repository":"arXiv","archiveID":"arXiv:1411.5726","creators":[{"firstName":"Ramakrishna","lastName":"Vedantam","creatorType":"author"},{"firstName":"C. Lawrence","lastName":"Zitnick","creatorType":"author"},{"firstName":"Devi","lastName":"Parikh","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Information Retrieval","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T15:26:02Z","dateModified":"2023-11-08T15:26:02Z","uri":"http://zotero.org/users/11367251/items/2G8UTJUS","itemID":1115,"attachments":[{"key":"FMIL3YWL","version":1427,"itemType":"attachment","title":"vedantamCIDErConsensusbasedImage2015.pdf","parentItem":"2G8UTJUS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/vedantamCIDErConsensusbasedImage2015.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T15:34:15Z","dateModified":"2023-11-08T15:34:15Z","uri":"http://zotero.org/users/11367251/items/FMIL3YWL","localPath":"/Users/reyvababtista/Projects/Papers/vedantamCIDErConsensusbasedImage2015.pdf","defaultPath":"files/1121/vedantamCIDErConsensusbasedImage2015.pdf"}],"notes":[{"key":"BMSLJE2R","version":1415,"itemType":"note","parentItem":"2G8UTJUS","note":"Comment: To appear in CVPR 2015","tags":[],"relations":{},"dateAdded":"2023-11-08T15:26:02Z","dateModified":"2023-11-08T15:26:02Z","uri":"http://zotero.org/users/11367251/items/BMSLJE2R"}]},"meta":{"revision":0,"created":1709832400284,"version":0},"$loki":6},{"itemID":1801,"item":{"key":"2KAAD9SM","version":2243,"itemType":"preprint","title":"Measuring and Narrowing the Compositionality Gap in Language Models","abstractNote":"We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.","date":"2023-10-17","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2210.03350","accessDate":"2024-01-18T20:30:03Z","extra":"arXiv:2210.03350 [cs]","repository":"arXiv","archiveID":"arXiv:2210.03350","creators":[{"firstName":"Ofir","lastName":"Press","creatorType":"author"},{"firstName":"Muru","lastName":"Zhang","creatorType":"author"},{"firstName":"Sewon","lastName":"Min","creatorType":"author"},{"firstName":"Ludwig","lastName":"Schmidt","creatorType":"author"},{"firstName":"Noah A.","lastName":"Smith","creatorType":"author"},{"firstName":"Mike","lastName":"Lewis","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2024-01-18T20:30:03Z","dateModified":"2024-01-18T20:30:03Z","uri":"http://zotero.org/users/11367251/items/2KAAD9SM","itemID":1801,"attachments":[{"key":"U4DRSC9B","version":2245,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2210.03350","accessDate":"2024-01-18T20:30:10Z","parentItem":"2KAAD9SM","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2210.html","tags":[],"relations":{},"dateAdded":"2024-01-18T20:30:10Z","dateModified":"2024-01-18T20:30:10Z","uri":"http://zotero.org/users/11367251/items/U4DRSC9B","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/U4DRSC9B/2210.html","defaultPath":"files/1805/2210.html"},{"key":"2XFPWUPW","version":2243,"itemType":"attachment","title":"pressMeasuringNarrowingCompositionality2023.pdf","parentItem":"2KAAD9SM","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/pressMeasuringNarrowingCompositionality2023.pdf","tags":[],"relations":{},"dateAdded":"2024-01-18T20:30:06Z","dateModified":"2024-01-18T20:30:06Z","uri":"http://zotero.org/users/11367251/items/2XFPWUPW","localPath":"/Users/reyvababtista/Projects/Papers/pressMeasuringNarrowingCompositionality2023.pdf","defaultPath":"files/1804/pressMeasuringNarrowingCompositionality2023.pdf"}],"notes":[{"key":"66P4XBVC","version":2243,"itemType":"note","parentItem":"2KAAD9SM","note":"Comment: To appear at Findings of EMNLP 2023","tags":[],"relations":{},"dateAdded":"2024-01-18T20:30:03Z","dateModified":"2024-01-18T20:30:03Z","uri":"http://zotero.org/users/11367251/items/66P4XBVC"}]},"meta":{"revision":0,"created":1709832400290,"version":0},"$loki":7},{"itemID":1630,"item":{"key":"2TAQFTPM","version":2187,"itemType":"webpage","title":"Stochastic Gradient Descent (v.2)","date":"2011","url":"https://leon.bottou.org/projects/sgd","accessDate":"2023-11-28","websiteTitle":"Stochastic Gradient Descent (v.2)","creators":[{"name":"Leon Bottou","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T05:27:54Z","dateModified":"2023-12-08T17:18:47Z","uri":"http://zotero.org/users/11367251/items/2TAQFTPM","itemID":1630,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400292,"version":0},"$loki":8},{"itemID":559,"item":{"key":"2YR2UC9A","version":748,"itemType":"preprint","title":"TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT","abstractNote":"Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework’s adaptability to specific use cases.","date":"2023-08-07","language":"en","shortTitle":"TableGPT","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2307.08674","accessDate":"2023-08-30T13:17:35Z","extra":"arXiv:2307.08674 [cs]","repository":"arXiv","archiveID":"arXiv:2307.08674","creators":[{"firstName":"Liangyu","lastName":"Zha","creatorType":"author"},{"firstName":"Junlin","lastName":"Zhou","creatorType":"author"},{"firstName":"Liyao","lastName":"Li","creatorType":"author"},{"firstName":"Rui","lastName":"Wang","creatorType":"author"},{"firstName":"Qingyi","lastName":"Huang","creatorType":"author"},{"firstName":"Saisai","lastName":"Yang","creatorType":"author"},{"firstName":"Jing","lastName":"Yuan","creatorType":"author"},{"firstName":"Changbao","lastName":"Su","creatorType":"author"},{"firstName":"Xiang","lastName":"Li","creatorType":"author"},{"firstName":"Aofeng","lastName":"Su","creatorType":"author"},{"firstName":"Tao","lastName":"Zhang","creatorType":"author"},{"firstName":"Chen","lastName":"Zhou","creatorType":"author"},{"firstName":"Kaizhe","lastName":"Shou","creatorType":"author"},{"firstName":"Miao","lastName":"Wang","creatorType":"author"},{"firstName":"Wufang","lastName":"Zhu","creatorType":"author"},{"firstName":"Guoshan","lastName":"Lu","creatorType":"author"},{"firstName":"Chao","lastName":"Ye","creatorType":"author"},{"firstName":"Yali","lastName":"Ye","creatorType":"author"},{"firstName":"Wentao","lastName":"Ye","creatorType":"author"},{"firstName":"Yiming","lastName":"Zhang","creatorType":"author"},{"firstName":"Xinglong","lastName":"Deng","creatorType":"author"},{"firstName":"Jie","lastName":"Xu","creatorType":"author"},{"firstName":"Haobo","lastName":"Wang","creatorType":"author"},{"firstName":"Gang","lastName":"Chen","creatorType":"author"},{"firstName":"Junbo","lastName":"Zhao","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-08-30T13:17:36Z","dateModified":"2023-08-30T13:17:36Z","uri":"http://zotero.org/users/11367251/items/2YR2UC9A","itemID":559,"attachments":[{"key":"CX4VI4WU","version":751,"itemType":"attachment","title":"zhaTableGPTUnifyingTables2023.pdf","parentItem":"2YR2UC9A","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhaTableGPTUnifyingTables2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CX4VI4WU/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/4\">TableGPT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/4\">Model Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/4\">Global Representation of Table</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/5\">Chain-of-Command</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/6\">Domain Data Processing Pipeline</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/6\">Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/6\">Commands supported by TableGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/6\">Comparison with previous command-using LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/7\">Case Study</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CX4VI4WU/7\">Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-08-30T13:18:00Z","dateModified":"2023-08-30T13:18:01Z","uri":"http://zotero.org/users/11367251/items/CX4VI4WU","localPath":"/Users/reyvababtista/Projects/Papers/zhaTableGPTUnifyingTables2023.pdf","defaultPath":"files/561/zhaTableGPTUnifyingTables2023.pdf"}],"notes":[{"key":"8XQUWXQR","version":748,"itemType":"note","parentItem":"2YR2UC9A","note":"Comment: Technical Report","tags":[],"relations":{},"dateAdded":"2023-08-30T13:17:36Z","dateModified":"2023-08-30T13:17:36Z","uri":"http://zotero.org/users/11367251/items/8XQUWXQR"}]},"meta":{"revision":0,"created":1709832400294,"version":0},"$loki":9},{"itemID":1380,"item":{"key":"2ZAZCBT8","version":1512,"itemType":"preprint","title":"InfographicVQA","abstractNote":"Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering technique.To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at http://docvqa.org","date":"2021-08-22","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2104.12756","accessDate":"2023-11-08T22:36:26Z","extra":"arXiv:2104.12756 [cs]","repository":"arXiv","archiveID":"arXiv:2104.12756","creators":[{"firstName":"Minesh","lastName":"Mathew","creatorType":"author"},{"firstName":"Viraj","lastName":"Bagal","creatorType":"author"},{"firstName":"Rubèn Pérez","lastName":"Tito","creatorType":"author"},{"firstName":"Dimosthenis","lastName":"Karatzas","creatorType":"author"},{"firstName":"Ernest","lastName":"Valveny","creatorType":"author"},{"firstName":"C. V.","lastName":"Jawahar","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:36:27Z","dateModified":"2023-11-08T22:36:27Z","uri":"http://zotero.org/users/11367251/items/2ZAZCBT8","itemID":1380,"attachments":[{"key":"HLDDZJUY","version":1515,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2104.12756","accessDate":"2023-11-08T22:36:39Z","parentItem":"2ZAZCBT8","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2104.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:36:39Z","dateModified":"2023-11-08T22:36:39Z","uri":"http://zotero.org/users/11367251/items/HLDDZJUY","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/HLDDZJUY/2104.html","defaultPath":"files/1383/2104.html"},{"key":"T8CZ43YP","version":1513,"itemType":"attachment","title":"mathewInfographicVQA2021.pdf","parentItem":"2ZAZCBT8","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/mathewInfographicVQA2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:36:33Z","dateModified":"2023-11-08T22:36:33Z","uri":"http://zotero.org/users/11367251/items/T8CZ43YP","localPath":"/Users/reyvababtista/Projects/Papers/mathewInfographicVQA2021.pdf","defaultPath":"files/1382/mathewInfographicVQA2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400296,"version":0},"$loki":10},{"itemID":1688,"item":{"key":"2ZJ9UPL4","version":2148,"itemType":"journalArticle","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding","abstractNote":"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.","date":"2022","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2205.11487","accessDate":"2023-12-06T23:03:57Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.2205.11487","creators":[{"firstName":"Chitwan","lastName":"Saharia","creatorType":"author"},{"firstName":"William","lastName":"Chan","creatorType":"author"},{"firstName":"Saurabh","lastName":"Saxena","creatorType":"author"},{"firstName":"Lala","lastName":"Li","creatorType":"author"},{"firstName":"Jay","lastName":"Whang","creatorType":"author"},{"firstName":"Emily","lastName":"Denton","creatorType":"author"},{"firstName":"Seyed Kamyar Seyed","lastName":"Ghasemipour","creatorType":"author"},{"firstName":"Burcu Karagol","lastName":"Ayan","creatorType":"author"},{"firstName":"S. Sara","lastName":"Mahdavi","creatorType":"author"},{"firstName":"Rapha Gontijo","lastName":"Lopes","creatorType":"author"},{"firstName":"Tim","lastName":"Salimans","creatorType":"author"},{"firstName":"Jonathan","lastName":"Ho","creatorType":"author"},{"firstName":"David J","lastName":"Fleet","creatorType":"author"},{"firstName":"Mohammad","lastName":"Norouzi","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:03:57Z","dateModified":"2023-12-06T23:03:57Z","uri":"http://zotero.org/users/11367251/items/2ZJ9UPL4","itemID":1688,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400298,"version":0},"$loki":11},{"itemID":1456,"item":{"key":"32SHKU32","version":1599,"itemType":"preprint","title":"Compressive Transformers for Long-Range Sequence Modelling","abstractNote":"We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.","date":"2019-11-13","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1911.05507","accessDate":"2023-11-08T22:55:23Z","extra":"arXiv:1911.05507 [cs, stat]","repository":"arXiv","archiveID":"arXiv:1911.05507","creators":[{"firstName":"Jack W.","lastName":"Rae","creatorType":"author"},{"firstName":"Anna","lastName":"Potapenko","creatorType":"author"},{"firstName":"Siddhant M.","lastName":"Jayakumar","creatorType":"author"},{"firstName":"Timothy P.","lastName":"Lillicrap","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:55:23Z","dateModified":"2023-11-08T22:55:23Z","uri":"http://zotero.org/users/11367251/items/32SHKU32","itemID":1456,"attachments":[{"key":"8MC5CGDQ","version":1602,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1911.05507","accessDate":"2023-11-08T22:55:31Z","parentItem":"32SHKU32","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1911.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:55:31Z","dateModified":"2023-11-08T22:55:31Z","uri":"http://zotero.org/users/11367251/items/8MC5CGDQ","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/8MC5CGDQ/1911.html","defaultPath":"files/1460/1911.html"},{"key":"GJCB2GKE","version":1599,"itemType":"attachment","title":"raeCompressiveTransformersLongRange2019.pdf","parentItem":"32SHKU32","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/raeCompressiveTransformersLongRange2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:55:25Z","dateModified":"2023-11-08T22:55:25Z","uri":"http://zotero.org/users/11367251/items/GJCB2GKE","localPath":"/Users/reyvababtista/Projects/Papers/raeCompressiveTransformersLongRange2019.pdf","defaultPath":"files/1459/raeCompressiveTransformersLongRange2019.pdf"}],"notes":[{"key":"DHXS3EBU","version":1599,"itemType":"note","parentItem":"32SHKU32","note":"Comment: 19 pages, 6 figures, 10 tables","tags":[],"relations":{},"dateAdded":"2023-11-08T22:55:23Z","dateModified":"2023-11-08T22:55:23Z","uri":"http://zotero.org/users/11367251/items/DHXS3EBU"}]},"meta":{"revision":0,"created":1709832400301,"version":0},"$loki":12},{"itemID":932,"item":{"key":"34AC2C7D","version":1234,"itemType":"blogPost","title":"Go beyond the search box: Introducing multisearch","date":"04/07/2022","url":"https://blog.google/products/search/multisearch/","accessDate":"2023-11-05","blogTitle":"Go beyond the search box: Introducing multisearch","creators":[{"name":"Belinda Zeng","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T04:14:44Z","dateModified":"2023-11-06T04:23:52Z","uri":"http://zotero.org/users/11367251/items/34AC2C7D","itemID":932,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400302,"version":0},"$loki":13},{"itemID":601,"item":{"key":"3AK38PSV","version":807,"itemType":"preprint","title":"Language Models are Few-Shot Learners","abstractNote":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.","date":"2020-07-22","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2005.14165","accessDate":"2023-09-05T13:44:59Z","extra":"arXiv:2005.14165 [cs]","repository":"arXiv","archiveID":"arXiv:2005.14165","creators":[{"firstName":"Tom B.","lastName":"Brown","creatorType":"author"},{"firstName":"Benjamin","lastName":"Mann","creatorType":"author"},{"firstName":"Nick","lastName":"Ryder","creatorType":"author"},{"firstName":"Melanie","lastName":"Subbiah","creatorType":"author"},{"firstName":"Jared","lastName":"Kaplan","creatorType":"author"},{"firstName":"Prafulla","lastName":"Dhariwal","creatorType":"author"},{"firstName":"Arvind","lastName":"Neelakantan","creatorType":"author"},{"firstName":"Pranav","lastName":"Shyam","creatorType":"author"},{"firstName":"Girish","lastName":"Sastry","creatorType":"author"},{"firstName":"Amanda","lastName":"Askell","creatorType":"author"},{"firstName":"Sandhini","lastName":"Agarwal","creatorType":"author"},{"firstName":"Ariel","lastName":"Herbert-Voss","creatorType":"author"},{"firstName":"Gretchen","lastName":"Krueger","creatorType":"author"},{"firstName":"Tom","lastName":"Henighan","creatorType":"author"},{"firstName":"Rewon","lastName":"Child","creatorType":"author"},{"firstName":"Aditya","lastName":"Ramesh","creatorType":"author"},{"firstName":"Daniel M.","lastName":"Ziegler","creatorType":"author"},{"firstName":"Jeffrey","lastName":"Wu","creatorType":"author"},{"firstName":"Clemens","lastName":"Winter","creatorType":"author"},{"firstName":"Christopher","lastName":"Hesse","creatorType":"author"},{"firstName":"Mark","lastName":"Chen","creatorType":"author"},{"firstName":"Eric","lastName":"Sigler","creatorType":"author"},{"firstName":"Mateusz","lastName":"Litwin","creatorType":"author"},{"firstName":"Scott","lastName":"Gray","creatorType":"author"},{"firstName":"Benjamin","lastName":"Chess","creatorType":"author"},{"firstName":"Jack","lastName":"Clark","creatorType":"author"},{"firstName":"Christopher","lastName":"Berner","creatorType":"author"},{"firstName":"Sam","lastName":"McCandlish","creatorType":"author"},{"firstName":"Alec","lastName":"Radford","creatorType":"author"},{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"},{"firstName":"Dario","lastName":"Amodei","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:44:59Z","dateModified":"2023-09-05T13:45:00Z","uri":"http://zotero.org/users/11367251/items/3AK38PSV","itemID":601,"attachments":[{"key":"WNY9LZUD","version":817,"itemType":"attachment","title":"brownLanguageModelsare2020.pdf","parentItem":"3AK38PSV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/brownLanguageModelsare2020.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_WNY9LZUD/3\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/6\">2 Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/8\">2.1 Model and Architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/8\">2.2 Training Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/9\">2.3 Training Process</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/10\">2.4 Evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/10\">3 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/11\">3.1 Language Modeling, Cloze, and Completion Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/13\">3.2 Closed Book Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/14\">3.3 Translation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/16\">3.4 Winograd-Style Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/17\">3.5 Common Sense Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/18\">3.6 Reading Comprehension</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/18\">3.7 SuperGLUE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/20\">3.8 NLI</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/21\">3.9 Synthetic and Qualitative Tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/29\">4 Measuring and Preventing Memorization Of Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/33\">5 Limitations</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/34\">6 Broader Impacts</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/35\">6.1 Misuse of Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/36\">6.2 Fairness, Bias, and Representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/39\">6.3 Energy Usage</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/39\">7 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/40\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/43\">A Details of Common Crawl Filtering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/43\">B Details of Model Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/43\">C Details of Test Set Contamination Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/46\">D Total Compute Used to Train Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/46\">E Human Quality Assessment of Synthetic News Articles</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/48\">F Additional Samples from GPT-3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/50\">G Details of Task Phrasing and Specifications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_WNY9LZUD/63\">H Results on All Tasks for All Model Sizes</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:31Z","dateModified":"2023-09-05T13:47:31Z","uri":"http://zotero.org/users/11367251/items/WNY9LZUD","localPath":"/Users/reyvababtista/Projects/Papers/brownLanguageModelsare2020.pdf","defaultPath":"files/621/brownLanguageModelsare2020.pdf"}],"notes":[{"key":"FZ72U5P5","version":807,"itemType":"note","parentItem":"3AK38PSV","note":"Comment: 40+32 pages","tags":[],"relations":{},"dateAdded":"2023-09-05T13:44:59Z","dateModified":"2023-09-05T13:44:59Z","uri":"http://zotero.org/users/11367251/items/FZ72U5P5"}]},"meta":{"revision":0,"created":1709832400304,"version":0},"$loki":14},{"itemID":1646,"item":{"key":"3C54YAW4","version":2073,"itemType":"journalArticle","title":"Random Forests","date":"2001","libraryCatalog":"DOI.org (Crossref)","url":"http://link.springer.com/10.1023/A:1010933404324","accessDate":"2023-12-03T20:20:34Z","volume":"45","pages":"5-32","publicationTitle":"Machine Learning","DOI":"10.1023/A:1010933404324","issue":"1","ISSN":"08856125","creators":[{"firstName":"Leo","lastName":"Breiman","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-03T20:20:34Z","dateModified":"2023-12-03T20:21:04Z","uri":"http://zotero.org/users/11367251/items/3C54YAW4","itemID":1646,"attachments":[{"key":"EGSGZ82Z","version":2071,"itemType":"attachment","title":"breimanNotitlefound2001.pdf","parentItem":"3C54YAW4","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/breimanNotitlefound2001.pdf","tags":[],"relations":{},"dateAdded":"2023-12-03T20:20:35Z","dateModified":"2023-12-03T20:20:35Z","uri":"http://zotero.org/users/11367251/items/EGSGZ82Z","localPath":"/Users/reyvababtista/Projects/Papers/breimanNotitlefound2001.pdf","defaultPath":"files/1648/breimanNotitlefound2001.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400304,"version":0},"$loki":15},{"itemID":1854,"item":{"key":"3C7UR9SZ","version":2358,"itemType":"journalArticle","title":"PAL: Program-aided Language Models","abstractNote":"Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as \"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .","date":"2022","shortTitle":"PAL","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2211.10435","accessDate":"2024-01-26T04:57:53Z","rights":"Creative Commons Zero v1.0 Universal","extra":"Publisher: arXiv\nVersion Number: 2","DOI":"10.48550/ARXIV.2211.10435","creators":[{"firstName":"Luyu","lastName":"Gao","creatorType":"author"},{"firstName":"Aman","lastName":"Madaan","creatorType":"author"},{"firstName":"Shuyan","lastName":"Zhou","creatorType":"author"},{"firstName":"Uri","lastName":"Alon","creatorType":"author"},{"firstName":"Pengfei","lastName":"Liu","creatorType":"author"},{"firstName":"Yiming","lastName":"Yang","creatorType":"author"},{"firstName":"Jamie","lastName":"Callan","creatorType":"author"},{"firstName":"Graham","lastName":"Neubig","creatorType":"author"}],"tags":[{"tag":"Artificial Intelligence (cs.AI)","type":1},{"tag":"Computation and Language (cs.CL)","type":1},{"tag":"FOS: Computer and information sciences","type":1}],"collections":[],"relations":{},"dateAdded":"2024-01-26T04:57:53Z","dateModified":"2024-01-26T04:57:53Z","uri":"http://zotero.org/users/11367251/items/3C7UR9SZ","itemID":1854,"attachments":[{"key":"JZFKGGI5","version":2360,"itemType":"attachment","title":"gaoPALProgramaidedLanguage2022a.pdf","parentItem":"3C7UR9SZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gaoPALProgramaidedLanguage2022a.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JZFKGGI5/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/2\">2 Background: Few-shot Prompting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/3\">3 Program-aided Language Models</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/3\">4 Experimental Setup</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/4\">4.1 Mathematical Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/4\">4.2 Symbolic Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/5\">4.3 Algorithmic Tasks</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/5\">5 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/5\">5.1 Math Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/6\">5.2 Symbolic Reasoning &amp;amp; Algorithmic Tasks Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/7\">6 Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/7\">7 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/9\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/9\">Appendix</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/12\">I Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/13\">A Alternative Prompts without Meaningful Variable Names</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/13\">B Additional analysis on Arithmetic Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/14\">C Effect of Using Language Models of Code</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/14\">D Analyzing the Effect of Increasing Number of Samples on PaL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/17\">E Standard Deviations Across Multiple Order of Prompts</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/17\">F PaL Beyond Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/20\">G Closer Look into Token-level Behaviors of Different Mechanisms</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/20\">H Datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/23\">H.1 Creating gsm-hard</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/23\">H.2 gsm-hard Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/24\">I Generalization of PAL to Least-to-Most Prompting</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/26\">J Prompts</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/26\">J.1 Reasoning about Colored Objects</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/27\">J.2 Penguins in a Table</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/28\">J.3 Date Understanding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/29\">J.4 Math</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/31\">J.5 Object Counting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/32\">J.6 Repeat Copy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/33\">K Success and Failure Modes in Symbolic Tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/33\">K.1 Colored Objects</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/33\">K.2 Penguins in a Table</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZFKGGI5/34\">K.3 Date Understanding</a></li></ul></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2024-01-26T04:58:20Z","dateModified":"2024-01-26T04:58:21Z","uri":"http://zotero.org/users/11367251/items/JZFKGGI5","localPath":"/Users/reyvababtista/Projects/Papers/gaoPALProgramaidedLanguage2022a.pdf","defaultPath":"files/1857/gaoPALProgramaidedLanguage2022a.pdf"}],"notes":[{"key":"RAW44EZU","version":2358,"itemType":"note","parentItem":"3C7UR9SZ","note":"<h2>Other</h2>\nThe first three authors contributed equally. Our code and data are publicly available at http://reasonwithpal.com/","tags":[],"relations":{},"dateAdded":"2024-01-26T04:57:53Z","dateModified":"2024-01-26T04:57:53Z","uri":"http://zotero.org/users/11367251/items/RAW44EZU"}]},"meta":{"revision":0,"created":1709832400306,"version":0},"$loki":16},{"itemID":720,"item":{"key":"3LVXLGRB","version":961,"itemType":"preprint","title":"OpenAGI: When LLM Meets Domain Experts","abstractNote":"Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM’s ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project’s code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.","date":"2023-08-02","language":"en","shortTitle":"OpenAGI","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2304.04370","accessDate":"2023-09-21T21:53:51Z","extra":"arXiv:2304.04370 [cs]","repository":"arXiv","archiveID":"arXiv:2304.04370","creators":[{"firstName":"Yingqiang","lastName":"Ge","creatorType":"author"},{"firstName":"Wenyue","lastName":"Hua","creatorType":"author"},{"firstName":"Kai","lastName":"Mei","creatorType":"author"},{"firstName":"Jianchao","lastName":"Ji","creatorType":"author"},{"firstName":"Juntao","lastName":"Tan","creatorType":"author"},{"firstName":"Shuyuan","lastName":"Xu","creatorType":"author"},{"firstName":"Zelong","lastName":"Li","creatorType":"author"},{"firstName":"Yongfeng","lastName":"Zhang","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-21T21:53:51Z","dateModified":"2023-09-21T21:53:51Z","uri":"http://zotero.org/users/11367251/items/3LVXLGRB","itemID":720,"attachments":[{"key":"6QVK9UI2","version":962,"itemType":"attachment","title":"geOpenAGIWhenLLM2023.pdf","parentItem":"3LVXLGRB","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/geOpenAGIWhenLLM2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_6QVK9UI2/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/3\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/4\">The OpenAGI Platform</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/4\">Benchmark Tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/4\">Domain Expert Model Set</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/5\">Multi-step Tasks and Corresponding Datasets Construction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/5\">Evaluation Metrics</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/6\">Open-ended Tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/7\">Reinforcement Learning from Task Feedback (RLTF)</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/7\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/7\">Backbone LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/7\">Learning Schema of LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/8\">Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/8\">Experimental Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/8\">Effect of Prompts</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/9\">Case Study of Non-linear Planning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/9\">Conclusions and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/13\">Research Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/15\">Original Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/15\">Data Augmentation Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/16\">Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/17\">Dataset Documentation and Data Samples for Benchmark Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/17\">Details of RLTF</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/17\">Constrained Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/18\">Zero- and Few-shot Schema</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/18\">Broader Impacts and Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/19\">Computational Resources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6QVK9UI2/19\">Training Details</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-21T21:53:56Z","dateModified":"2023-09-21T21:53:57Z","uri":"http://zotero.org/users/11367251/items/6QVK9UI2","localPath":"/Users/reyvababtista/Projects/Papers/geOpenAGIWhenLLM2023.pdf","defaultPath":"files/722/geOpenAGIWhenLLM2023.pdf"}],"notes":[{"key":"FFXHYXLS","version":961,"itemType":"note","parentItem":"3LVXLGRB","note":"Comment: 22 pages, 11 figures, 7 tables","tags":[],"relations":{},"dateAdded":"2023-09-21T21:53:51Z","dateModified":"2023-09-21T21:53:51Z","uri":"http://zotero.org/users/11367251/items/FFXHYXLS"}]},"meta":{"revision":0,"created":1709832400308,"version":0},"$loki":17},{"itemID":1426,"item":{"key":"3LZMMLWK","version":1560,"itemType":"preprint","title":"NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions","abstractNote":"We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial scene description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git)","date":"2021-05-23","shortTitle":"NExT-QA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2105.08276","accessDate":"2023-11-08T22:46:07Z","extra":"arXiv:2105.08276 [cs]","repository":"arXiv","archiveID":"arXiv:2105.08276","creators":[{"firstName":"Junbin","lastName":"Xiao","creatorType":"author"},{"firstName":"Xindi","lastName":"Shang","creatorType":"author"},{"firstName":"Angela","lastName":"Yao","creatorType":"author"},{"firstName":"Tat-Seng","lastName":"Chua","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:46:07Z","dateModified":"2023-11-08T22:46:07Z","uri":"http://zotero.org/users/11367251/items/3LZMMLWK","itemID":1426,"attachments":[{"key":"RC2N6QB5","version":1567,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2105.08276","accessDate":"2023-11-08T22:46:21Z","parentItem":"3LZMMLWK","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2105.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:46:21Z","dateModified":"2023-11-08T22:46:21Z","uri":"http://zotero.org/users/11367251/items/RC2N6QB5","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/RC2N6QB5/2105.html","defaultPath":"files/1431/2105.html"},{"key":"B8F243DT","version":1562,"itemType":"attachment","title":"xiaoNExTQANextPhase2021.pdf","parentItem":"3LZMMLWK","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/xiaoNExTQANextPhase2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:46:15Z","dateModified":"2023-11-08T22:46:15Z","uri":"http://zotero.org/users/11367251/items/B8F243DT","localPath":"/Users/reyvababtista/Projects/Papers/xiaoNExTQANextPhase2021.pdf","defaultPath":"files/1430/xiaoNExTQANextPhase2021.pdf"}],"notes":[{"key":"NTEPJRYY","version":1560,"itemType":"note","parentItem":"3LZMMLWK","note":"Comment: To appear at CVPR2021.(Receive one 'Strong Accept'. Review comments: The benchmark will be beneficial for an important video understanding problem. The analysis is comprehensive and provides meaningful insights.)","tags":[],"relations":{},"dateAdded":"2023-11-08T22:46:07Z","dateModified":"2023-11-08T22:46:07Z","uri":"http://zotero.org/users/11367251/items/NTEPJRYY"}]},"meta":{"revision":0,"created":1709832400309,"version":0},"$loki":18},{"itemID":1389,"item":{"key":"3RSQFCPK","version":1523,"itemType":"preprint","title":"A Diagram Is Worth A Dozen Images","abstractNote":"Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.","date":"2016-03-23","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1603.07396","accessDate":"2023-11-08T22:38:28Z","extra":"arXiv:1603.07396 [cs]","repository":"arXiv","archiveID":"arXiv:1603.07396","creators":[{"firstName":"Aniruddha","lastName":"Kembhavi","creatorType":"author"},{"firstName":"Mike","lastName":"Salvato","creatorType":"author"},{"firstName":"Eric","lastName":"Kolve","creatorType":"author"},{"firstName":"Minjoon","lastName":"Seo","creatorType":"author"},{"firstName":"Hannaneh","lastName":"Hajishirzi","creatorType":"author"},{"firstName":"Ali","lastName":"Farhadi","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:38:28Z","dateModified":"2023-11-08T22:38:28Z","uri":"http://zotero.org/users/11367251/items/3RSQFCPK","itemID":1389,"attachments":[{"key":"5TRYR9F5","version":1526,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1603.07396","accessDate":"2023-11-08T22:38:39Z","parentItem":"3RSQFCPK","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1603.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:38:39Z","dateModified":"2023-11-08T22:38:39Z","uri":"http://zotero.org/users/11367251/items/5TRYR9F5","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/5TRYR9F5/1603.html","defaultPath":"files/1392/1603.html"},{"key":"6KNE7BA7","version":1524,"itemType":"attachment","title":"kembhaviDiagramWorthDozen2016.pdf","parentItem":"3RSQFCPK","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kembhaviDiagramWorthDozen2016.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:38:34Z","dateModified":"2023-11-08T22:38:34Z","uri":"http://zotero.org/users/11367251/items/6KNE7BA7","localPath":"/Users/reyvababtista/Projects/Papers/kembhaviDiagramWorthDozen2016.pdf","defaultPath":"files/1391/kembhaviDiagramWorthDozen2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400311,"version":0},"$loki":19},{"itemID":574,"item":{"key":"3S24KP3L","version":769,"itemType":"preprint","title":"Autoencoders","abstractNote":"An autoencoder is a speciﬁc type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the diﬀerent types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.","date":"2021-04-03","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2003.05991","accessDate":"2023-09-05T13:34:29Z","extra":"arXiv:2003.05991 [cs, stat]","repository":"arXiv","archiveID":"arXiv:2003.05991","creators":[{"firstName":"Dor","lastName":"Bank","creatorType":"author"},{"firstName":"Noam","lastName":"Koenigstein","creatorType":"author"},{"firstName":"Raja","lastName":"Giryes","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:34:29Z","dateModified":"2023-09-05T13:34:30Z","uri":"http://zotero.org/users/11367251/items/3S24KP3L","itemID":574,"attachments":[{"key":"6VEVSLPT","version":817,"itemType":"attachment","title":"bankAutoencoders2021.pdf","parentItem":"3S24KP3L","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bankAutoencoders2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_6VEVSLPT/1\">Dor Bank, Noam Koenigstein, Raja Giryes</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/1\">1 Autoencoders</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/2\">2 Regularized autoencoders</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/3\">2.1 Sparse Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/4\">2.2 Denoising Autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/5\">2.3 Contractive Autoencoders</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/5\">3 Variational Autoencoders</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/7\">3.1 The Reparameterization Trick</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/8\">3.2 Example: The Case of Normal Distribution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/8\">3.3 Disentangled Autoencoders</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/8\">4 Applications of autoencoders</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/9\">4.1 Autoencoders as a generative model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/9\">4.2 Use of autoencoders for classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/10\">4.3 Use of autoencoders for clustering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/11\">4.4 Use of autoencoders for anomaly detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/12\">4.5 Use of autoencoders for recommendation systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/13\">4.6 Use of autoencoders for dimensionality reduction</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/14\">5 Advanced autoencoder techniques</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/14\">5.1 Autoencoders and generative adversarial networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/14\">5.2 Adversarially learned inference</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/15\">5.3 Wasserstein autoencoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/16\">5.4 Deep feature consistent variational autoencoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/18\">5.5 Conditional image generation with PixelCNN decoders</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/19\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/19\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6VEVSLPT/19\">References</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:26Z","dateModified":"2023-09-05T13:47:27Z","uri":"http://zotero.org/users/11367251/items/6VEVSLPT","localPath":"/Users/reyvababtista/Projects/Papers/bankAutoencoders2021.pdf","defaultPath":"files/615/bankAutoencoders2021.pdf"}],"notes":[{"key":"UNAELHC3","version":769,"itemType":"note","parentItem":"3S24KP3L","note":"Comment: Book chapter","tags":[],"relations":{},"dateAdded":"2023-09-05T13:34:29Z","dateModified":"2023-09-05T13:34:29Z","uri":"http://zotero.org/users/11367251/items/UNAELHC3"}]},"meta":{"revision":0,"created":1709832400311,"version":0},"$loki":20},{"itemID":939,"item":{"key":"3U2JB92P","version":1274,"itemType":"blogPost","title":"Microsoft and Yahoo seal web deal","date":"07/29/2009","url":"http://news.bbc.co.uk/2/hi/business/8174763.stm","accessDate":"2023-11-06","blogTitle":"Microsoft and Yahoo seal web deal","creators":[{"name":"BBC News","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T01:54:47Z","dateModified":"2023-11-07T01:55:39Z","uri":"http://zotero.org/users/11367251/items/3U2JB92P","itemID":939,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400312,"version":0},"$loki":21},{"itemID":1667,"item":{"key":"3XWPLZCQ","version":2089,"itemType":"journalArticle","title":"Neural networks and physical systems with emergent collective computational abilities.","abstractNote":"Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.","date":"04/1982","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://pnas.org/doi/full/10.1073/pnas.79.8.2554","accessDate":"2023-12-04T04:33:43Z","volume":"79","pages":"2554-2558","publicationTitle":"Proceedings of the National Academy of Sciences","DOI":"10.1073/pnas.79.8.2554","issue":"8","journalAbbreviation":"Proc. Natl. Acad. Sci. U.S.A.","ISSN":"0027-8424, 1091-6490","creators":[{"firstName":"J J","lastName":"Hopfield","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T04:33:43Z","dateModified":"2023-12-04T04:33:43Z","uri":"http://zotero.org/users/11367251/items/3XWPLZCQ","itemID":1667,"attachments":[{"key":"PFXZD5MU","version":2090,"itemType":"attachment","title":"hopfieldNeuralnetworksphysical1982.pdf","parentItem":"3XWPLZCQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hopfieldNeuralnetworksphysical1982.pdf","tags":[],"relations":{},"dateAdded":"2023-12-04T04:33:49Z","dateModified":"2023-12-04T04:33:49Z","uri":"http://zotero.org/users/11367251/items/PFXZD5MU","localPath":"/Users/reyvababtista/Projects/Papers/hopfieldNeuralnetworksphysical1982.pdf","defaultPath":"files/1669/hopfieldNeuralnetworksphysical1982.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400313,"version":0},"$loki":22},{"itemID":1433,"item":{"key":"3ZMPKBGV","version":1570,"itemType":"preprint","title":"Are we done with ImageNet?","abstractNote":"Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition.","date":"2020-06-12","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2006.07159","accessDate":"2023-11-08T22:48:14Z","extra":"arXiv:2006.07159 [cs]","repository":"arXiv","archiveID":"arXiv:2006.07159","creators":[{"firstName":"Lucas","lastName":"Beyer","creatorType":"author"},{"firstName":"Olivier J.","lastName":"Hénaff","creatorType":"author"},{"firstName":"Alexander","lastName":"Kolesnikov","creatorType":"author"},{"firstName":"Xiaohua","lastName":"Zhai","creatorType":"author"},{"firstName":"Aäron van den","lastName":"Oord","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:48:15Z","dateModified":"2023-11-08T22:48:15Z","uri":"http://zotero.org/users/11367251/items/3ZMPKBGV","itemID":1433,"attachments":[{"key":"57VD9BIS","version":1574,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2006.07159","accessDate":"2023-11-08T22:48:27Z","parentItem":"3ZMPKBGV","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2006.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:48:27Z","dateModified":"2023-11-08T22:48:27Z","uri":"http://zotero.org/users/11367251/items/57VD9BIS","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/57VD9BIS/2006.html","defaultPath":"files/1437/2006.html"},{"key":"ZZYBGFWT","version":1571,"itemType":"attachment","title":"beyerArewedone2020.pdf","parentItem":"3ZMPKBGV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/beyerArewedone2020.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:48:22Z","dateModified":"2023-11-08T22:48:22Z","uri":"http://zotero.org/users/11367251/items/ZZYBGFWT","localPath":"/Users/reyvababtista/Projects/Papers/beyerArewedone2020.pdf","defaultPath":"files/1436/beyerArewedone2020.pdf"}],"notes":[{"key":"L2SGNVUE","version":1570,"itemType":"note","parentItem":"3ZMPKBGV","note":"Comment: All five authors contributed equally. New labels at https://github.com/google-research/reassessed-imagenet","tags":[],"relations":{},"dateAdded":"2023-11-08T22:48:15Z","dateModified":"2023-11-08T22:48:15Z","uri":"http://zotero.org/users/11367251/items/L2SGNVUE"}]},"meta":{"revision":0,"created":1709832400314,"version":0},"$loki":23},{"itemID":1814,"item":{"key":"44MNBK8C","version":2253,"itemType":"preprint","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","abstractNote":"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.","date":"2021-04-12","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2005.11401","accessDate":"2024-01-19T02:55:28Z","extra":"arXiv:2005.11401 [cs]","repository":"arXiv","archiveID":"arXiv:2005.11401","creators":[{"firstName":"Patrick","lastName":"Lewis","creatorType":"author"},{"firstName":"Ethan","lastName":"Perez","creatorType":"author"},{"firstName":"Aleksandra","lastName":"Piktus","creatorType":"author"},{"firstName":"Fabio","lastName":"Petroni","creatorType":"author"},{"firstName":"Vladimir","lastName":"Karpukhin","creatorType":"author"},{"firstName":"Naman","lastName":"Goyal","creatorType":"author"},{"firstName":"Heinrich","lastName":"Küttler","creatorType":"author"},{"firstName":"Mike","lastName":"Lewis","creatorType":"author"},{"firstName":"Wen-tau","lastName":"Yih","creatorType":"author"},{"firstName":"Tim","lastName":"Rocktäschel","creatorType":"author"},{"firstName":"Sebastian","lastName":"Riedel","creatorType":"author"},{"firstName":"Douwe","lastName":"Kiela","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2024-01-19T02:55:28Z","dateModified":"2024-01-19T02:55:28Z","uri":"http://zotero.org/users/11367251/items/44MNBK8C","itemID":1814,"attachments":[{"key":"3YIZU7RN","version":2255,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2005.11401","accessDate":"2024-01-19T02:55:39Z","parentItem":"44MNBK8C","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2005.html","tags":[],"relations":{},"dateAdded":"2024-01-19T02:55:39Z","dateModified":"2024-01-19T02:55:39Z","uri":"http://zotero.org/users/11367251/items/3YIZU7RN","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/3YIZU7RN/2005.html","defaultPath":"files/1818/2005.html"},{"key":"SWH2NYZU","version":2253,"itemType":"attachment","title":"lewisRetrievalAugmentedGenerationKnowledgeIntensive2021.pdf","parentItem":"44MNBK8C","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/lewisRetrievalAugmentedGenerationKnowledgeIntensive2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SWH2NYZU/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/2\">2 Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/3\">2.1 Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/3\">2.2 Retriever: DPR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/3\">2.3 Generator: BART</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/3\">2.4 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/4\">2.5 Decoding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/4\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/4\">3.1 Open-domain Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/4\">3.2 Abstractive Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/5\">3.3 Jeopardy Question Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/5\">3.4 Fact Verification</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/5\">4 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/5\">4.1 Open-domain Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/6\">4.2 Abstractive Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/6\">4.3 Jeopardy Question Generation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/6\">4.4 Fact Verification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/7\">4.5 Additional Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/8\">5 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/9\">6 Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/17\">A Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/17\">B Human Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/17\">C Training setup Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/18\">D Further Details on Open-Domain QA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/18\">E Further Details on FEVER</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/18\">F Null Document Probabilities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/18\">G Parameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/19\">H Retrieval Collapse</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SWH2NYZU/19\">I Number of instances per dataset</a></li></ul>","tags":[],"relations":{},"dateAdded":"2024-01-19T02:55:34Z","dateModified":"2024-01-19T02:55:35Z","uri":"http://zotero.org/users/11367251/items/SWH2NYZU","localPath":"/Users/reyvababtista/Projects/Papers/lewisRetrievalAugmentedGenerationKnowledgeIntensive2021.pdf","defaultPath":"files/1817/lewisRetrievalAugmentedGenerationKnowledgeIntensive2021.pdf"}],"notes":[{"key":"ADTDBDBI","version":2253,"itemType":"note","parentItem":"44MNBK8C","note":"Comment: Accepted at NeurIPS 2020","tags":[],"relations":{},"dateAdded":"2024-01-19T02:55:28Z","dateModified":"2024-01-19T02:55:28Z","uri":"http://zotero.org/users/11367251/items/ADTDBDBI"}]},"meta":{"revision":0,"created":1709832400316,"version":0},"$loki":24},{"itemID":956,"item":{"key":"48MMW338","version":1333,"itemType":"preprint","title":"UL2: Unifying Language Learning Paradigms","abstractNote":"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a uniﬁed framework for pre-training models that are universally eﬀective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives – two concepts that are commonly conﬂated. Next, we present a generalized and uniﬁed perspective for self-supervision in NLP and show how diﬀerent pre-training objectives can be cast as one another and how interpolating between diﬀerent objectives can be eﬀective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream ﬁne-tuning is associated with speciﬁc pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and ﬁnd that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 (published paper results) on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On zero-shot MMLU, UL2 20B outperforms T0 and T5 models. Additionally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X model checkpoints for the UL2 20B model and Flan-UL2 20B model at https://github.com/google-research/google-research/tree/master/ul2.","date":"2023-02-28","language":"en","shortTitle":"UL2","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2205.05131","accessDate":"2023-11-07T17:23:34Z","extra":"arXiv:2205.05131 [cs]","repository":"arXiv","archiveID":"arXiv:2205.05131","creators":[{"firstName":"Yi","lastName":"Tay","creatorType":"author"},{"firstName":"Mostafa","lastName":"Dehghani","creatorType":"author"},{"firstName":"Vinh Q.","lastName":"Tran","creatorType":"author"},{"firstName":"Xavier","lastName":"Garcia","creatorType":"author"},{"firstName":"Jason","lastName":"Wei","creatorType":"author"},{"firstName":"Xuezhi","lastName":"Wang","creatorType":"author"},{"firstName":"Hyung Won","lastName":"Chung","creatorType":"author"},{"firstName":"Siamak","lastName":"Shakeri","creatorType":"author"},{"firstName":"Dara","lastName":"Bahri","creatorType":"author"},{"firstName":"Tal","lastName":"Schuster","creatorType":"author"},{"firstName":"Huaixiu Steven","lastName":"Zheng","creatorType":"author"},{"firstName":"Denny","lastName":"Zhou","creatorType":"author"},{"firstName":"Neil","lastName":"Houlsby","creatorType":"author"},{"firstName":"Donald","lastName":"Metzler","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T17:23:34Z","dateModified":"2023-11-07T17:23:34Z","uri":"http://zotero.org/users/11367251/items/48MMW338","itemID":956,"attachments":[{"key":"72C6Y4SG","version":1336,"itemType":"attachment","title":"tayUL2UnifyingLanguage2023.pdf","parentItem":"48MMW338","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/tayUL2UnifyingLanguage2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_72C6Y4SG/4\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/6\">2 Background: Pre-trained Language Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/6\">2.1 Pre-trained Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/7\">2.2 Pre-training Objectives for Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/7\">2.3 Unified Pre-training Proposals</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/7\">3 Unifying Language Learning Paradigms (UL2)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/7\">3.1 Pre-training</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/8\">3.1.1 Unified Perspective for Pre-training Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/9\">3.1.2 Mixture of Denoisers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/10\">3.1.3 Mode Switching</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/10\">3.2 Model Architecture</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/11\">4 Ablative Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/11\">4.1 Baselines</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/12\">4.2 Experimental Setup</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/12\">4.2.1 Datasets and Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/12\">4.2.2 Metrics and Holistic Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/12\">4.2.3 Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/13\">4.3 Overview of Ablative Experimental Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/14\">4.3.1 Decoder Vs Encoder-Decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/14\">4.3.2 Is GPT and/or T5 the optimal setup?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/14\">4.3.3 On the Performance of UniLM and SCLM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/14\">4.3.4 On the Performance of the Proposed UL2</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/15\">4.4 Mode Switching Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/15\">4.5 Mixture-of-Denoisers Ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/16\">4.6 Modestly Scaling Model Size and Pretraining Data</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/16\">5 Scaling to 20B Parameters</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/17\">5.1 Pretraining and Model Configuration</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/17\">5.2 Experiments at 20B scale</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/17\">5.2.1 Setup and Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/18\">5.2.2 Datasets for Supervised Finetuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/19\">5.2.3 Summary of Supervised Finetuning Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/21\">5.2.4 Results on Supervised Finetuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/21\">5.2.5 Tradeoffs between Finetuning and Prompt-based Zero-shot Learning (SuperGLUE)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/21\">5.2.6 Generative Few-shot: XSUM Summarization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/22\">5.2.7 UL2 for chain-of-thought prompting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/23\">5.2.8 Massively Multitask Language Understanding</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/24\">5.3 Instruction Tuned UL2 20B with FLAN</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/24\">5.3.1 Few-shot MMLU and Big-Bench Results after Flan training of UL2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/25\">5.3.2 Comparisons on using Chain-of-thought vs Direct Prompting</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/25\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/26\">7 Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/26\">8 Author Contributions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/36\">9 Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/36\">9.1 Model Release</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/36\">9.2 Implementation Details and UL2 code</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/38\">9.3 Details of Supervised Finetuning SOTA runs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_72C6Y4SG/39\">9.4 Details of Prompts for few-shot and zero-shot</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-07T17:23:39Z","dateModified":"2023-11-07T17:23:40Z","uri":"http://zotero.org/users/11367251/items/72C6Y4SG","localPath":"/Users/reyvababtista/Projects/Papers/tayUL2UnifyingLanguage2023.pdf","defaultPath":"files/958/tayUL2UnifyingLanguage2023.pdf"}],"notes":[{"key":"YPFU3F5B","version":1333,"itemType":"note","parentItem":"48MMW338","note":"Comment: Updated Q1 2023 with Flan-UL2 20B release! :)","tags":[],"relations":{},"dateAdded":"2023-11-07T17:23:34Z","dateModified":"2023-11-07T17:23:34Z","uri":"http://zotero.org/users/11367251/items/YPFU3F5B"}]},"meta":{"revision":0,"created":1709832400317,"version":0},"$loki":25},{"itemID":934,"item":{"key":"4CVV2UYI","version":1247,"itemType":"blogPost","title":"Our latest update to the quality rater guidelines: E-A-T gets an extra E for Experience","date":"12/15/2022","url":"https://developers.google.com/search/blog/2022/12/google-raters-guidelines-e-e-a-t?safe=active","accessDate":"2023-11-05","blogTitle":"Our latest update to the quality rater guidelines: E-A-T gets an extra E for Experience","creators":[{"name":"Elizabeth Tucker","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T04:39:46Z","dateModified":"2023-11-06T04:40:30Z","uri":"http://zotero.org/users/11367251/items/4CVV2UYI","itemID":934,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400317,"version":0},"$loki":26},{"itemID":377,"item":{"key":"4E8JMWVK","version":418,"itemType":"journalArticle","title":"Evaluating machine learning models to classify occupants’ perceptions of their indoor environment and sleep quality from indoor air quality","abstractNote":"A variety of factors can affect a person’s perception of their environment and health, but one factor that is often overlooked in indoor settings is the air quality. To address this gap, we develop and evaluate four Machine Learning (ML) models on two disparate datasets using Indoor Air Quality (IAQ) parameters as primary features and components of self-reported IAQ satisfaction and sleep quality as target variables. In each case, we compare models to each other as well as to a simple model that always predicts the majority outcome. In the first analysis, we use open-source data collected from 93 California residences to predict occupant’s satisfaction with their indoor environ­ ment. Results indicate building ventilation rate, Relative Humidity (RH), and formaldehyde are most influential when predicting IAQ perception and do so with an accuracy greater than the simplified model. The second analysis uses IAQ data gathered from a field study we conducted with 20 participants over 11 weeks to train similar models. We obtain accuracy and F1 scores similar to the simplified model where PM2.5 and TVOCs represent the most important predictors. Our results underscore the ability of IAQ to affect a person’s perception of their built environment and health and highlight the utility of ML models to explore the strength of these relationships.","date":"2022-12-02","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.tandfonline.com/doi/full/10.1080/10962247.2022.2105439","accessDate":"2023-04-11T22:10:31Z","volume":"72","pages":"1381-1397","publicationTitle":"Journal of the Air & Waste Management Association","DOI":"10.1080/10962247.2022.2105439","issue":"12","journalAbbreviation":"Journal of the Air & Waste Management Association","ISSN":"1096-2247, 2162-2906","creators":[{"firstName":"Hagen","lastName":"Fritz","creatorType":"author"},{"firstName":"Mengjia","lastName":"Tang","creatorType":"author"},{"firstName":"Kerry","lastName":"Kinney","creatorType":"author"},{"firstName":"Zoltan","lastName":"Nagy","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-04-11T22:10:31Z","dateModified":"2023-04-11T22:10:31Z","uri":"http://zotero.org/users/11367251/items/4E8JMWVK","itemID":377,"attachments":[{"key":"U2RRVIAE","version":422,"itemType":"attachment","title":"fritzEvaluatingmachinelearning2022.pdf","parentItem":"4E8JMWVK","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/fritzEvaluatingmachinelearning2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U2RRVIAE/2\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/2\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/4\">Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/4\">IAQ perception</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/4\">Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/4\">Pre-Processing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/5\">Self-Reported sleep quality</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/5\">Data collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/5\">Pre-Processing</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/5\">Model training and evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/7\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/7\">Perception of IAQ satisfaction</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/7\">Feature selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/8\">Model optimization and performance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/8\">Analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/10\">Perception of sleep quality</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/10\">Summary of feature measurements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/10\">Model optimization and performance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/12\">Analysis</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/14\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/15\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/15\">Acronyms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/16\">Acknowledgment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/16\">Disclosure statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/16\">About the authors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/16\">ORCID</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/16\">Data Availability Statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U2RRVIAE/16\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-11T22:10:44Z","dateModified":"2023-04-11T22:10:44Z","uri":"http://zotero.org/users/11367251/items/U2RRVIAE","localPath":"/Users/reyvababtista/Projects/Papers/fritzEvaluatingmachinelearning2022.pdf","defaultPath":"files/378/fritzEvaluatingmachinelearning2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400319,"version":0},"$loki":27},{"itemID":801,"item":{"key":"4LVUQBBJ","version":1269,"itemType":"preprint","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models","abstractNote":"Exploring large-scale pretrained foundation models is of signiﬁcant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the ﬁrst half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efﬁciently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-theart performance with zero-shot transfer or minimal task-speciﬁc adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classiﬁcation, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classiﬁcation head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a ﬁnetuned encoder.","date":"2022-06-13","language":"en","shortTitle":"CoCa","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2205.01917","accessDate":"2023-10-28T15:31:06Z","extra":"arXiv:2205.01917 [cs]","repository":"arXiv","archiveID":"arXiv:2205.01917","creators":[{"firstName":"Jiahui","lastName":"Yu","creatorType":"author"},{"firstName":"Zirui","lastName":"Wang","creatorType":"author"},{"firstName":"Vijay","lastName":"Vasudevan","creatorType":"author"},{"firstName":"Legg","lastName":"Yeung","creatorType":"author"},{"firstName":"Mojtaba","lastName":"Seyedhosseini","creatorType":"author"},{"firstName":"Yonghui","lastName":"Wu","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Multimedia","type":1}],"collections":["AXTDM6XB","6X9VU85H"],"relations":{},"dateAdded":"2023-10-28T15:31:06Z","dateModified":"2023-10-28T15:31:06Z","uri":"http://zotero.org/users/11367251/items/4LVUQBBJ","itemID":801,"attachments":[{"key":"3W7I8FKR","version":1068,"itemType":"attachment","title":"yuCoCaContrastiveCaptioners2022.pdf","parentItem":"4LVUQBBJ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/yuCoCaContrastiveCaptioners2022.pdf","tags":[],"relations":{},"dateAdded":"2023-10-28T15:31:24Z","dateModified":"2023-10-28T15:31:24Z","uri":"http://zotero.org/users/11367251/items/3W7I8FKR","localPath":"/Users/reyvababtista/Projects/Papers/yuCoCaContrastiveCaptioners2022.pdf","defaultPath":"files/806/yuCoCaContrastiveCaptioners2022.pdf"}],"notes":[{"key":"5B3A8FQZ","version":1062,"itemType":"note","parentItem":"4LVUQBBJ","note":"Comment: Preprint","tags":[],"relations":{},"dateAdded":"2023-10-28T15:31:06Z","dateModified":"2023-10-28T15:31:06Z","uri":"http://zotero.org/users/11367251/items/5B3A8FQZ"}]},"meta":{"revision":0,"created":1709832400320,"version":0},"$loki":28},{"itemID":435,"item":{"key":"4V8JPCW3","version":539,"itemType":"journalArticle","title":"Examining the effects of motives and gender differences on smartphone addiction","abstractNote":"Smartphones have become increasingly popular in recent years. However, it may be addiction-prone and result in negative outcomes. Given that relevant research remains limited, this study attempts to address two research gaps in the extant information systems literature. First, research on the determinants of smartphone addiction remains scarce. Second, the role of individual characteristics (i.e., gender) in the formation of smartphone addiction is far from clear. To ﬁll these research gaps, this study develops a research model of smartphone addiction from the functionalist perspective and highlights the moderating role of gender with the insight of social orientation. We propose four categories of motives, including enhancement (i.e., perceived enjoyment), social (i.e., social relationship), coping (i.e., mood regulation and pastime), and conformity motives (i.e., conformity). Empirical results from our online survey illustrate that perceived enjoyment, mood regulation, pastime, and conformity positively affect smartphone addiction, whereas social relationship has no signiﬁcant effect. Moreover, we ﬁnd that gender moderates the effects of perceived enjoyment, pastime, and conformity on smartphone addiction. We expect that this study can enrich the theoretical understanding of how motives play different roles in the development of smartphone addiction. Implications are offered for both research and practice.","date":"10/2017","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S0747563217304144","accessDate":"2023-05-06T21:37:59Z","volume":"75","pages":"891-902","publicationTitle":"Computers in Human Behavior","DOI":"10.1016/j.chb.2017.07.002","journalAbbreviation":"Computers in Human Behavior","ISSN":"07475632","creators":[{"firstName":"Chongyang","lastName":"Chen","creatorType":"author"},{"firstName":"Kem Z.K.","lastName":"Zhang","creatorType":"author"},{"firstName":"Xiang","lastName":"Gong","creatorType":"author"},{"firstName":"Sesia J.","lastName":"Zhao","creatorType":"author"},{"firstName":"Matthew K.O.","lastName":"Lee","creatorType":"author"},{"firstName":"Liang","lastName":"Liang","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-05-06T21:37:59Z","dateModified":"2023-05-06T21:37:59Z","uri":"http://zotero.org/users/11367251/items/4V8JPCW3","itemID":435,"attachments":[{"key":"Q7FV5FAH","version":543,"itemType":"attachment","title":"chenExaminingeffectsmotives2017.pdf","parentItem":"4V8JPCW3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenExaminingeffectsmotives2017.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_Q7FV5FAH/1\">1. Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/2\">2. Theoretical background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/2\">2.1. IT addiction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/2\">2.2. Functionalist perspective</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/3\">2.3. Four categories of drinking motives</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/3\">2.4. The role of gender</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/3\">2.4.1. Gender research in IT addiction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/3\">2.4.2. Gender differences in social orientation</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/4\">3. Research model and hypotheses</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/4\">3.1. Perceived enjoyment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/4\">3.2. Social relationship</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/4\">3.3. Mood regulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/4\">3.4. Pastime</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/5\">3.5. Conformity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/5\">3.6. Gender</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/5\">4. Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/5\">4.1. Data collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/6\">4.2. Measures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/6\">4.3. Pilot test</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/6\">5. Data analysis and results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/7\">5.1. Measurement model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/7\">5.2. Structural model</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/8\">6. Discussion and conclusion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/8\">6.1. Implications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/10\">6.2. Limitations and future research</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/10\">Funding sources</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/10\">Appendix A. Group comparison method from Keil et al. (2000)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_Q7FV5FAH/10\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-05-06T21:38:04Z","dateModified":"2023-05-06T21:39:04Z","uri":"http://zotero.org/users/11367251/items/Q7FV5FAH","localPath":"/Users/reyvababtista/Projects/Papers/chenExaminingeffectsmotives2017.pdf","defaultPath":"files/436/chenExaminingeffectsmotives2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400321,"version":0},"$loki":29},{"itemID":1506,"item":{"key":"55EDSIUF","version":1699,"itemType":"blogPost","title":"TED 2011: The 'Panda' That Hates Farms: A Q&A With Google's Top Search Engineers","date":"2011/03/03","url":"https://www.wired.com/2011/03/the-panda-that-hates-farms/","accessDate":"2023-11-09","blogTitle":"TED 2011: The 'Panda' That Hates Farms: A Q&A With Google's Top Search Engineers","creators":[{"name":"Steven Levy","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:27:24Z","dateModified":"2023-11-09T21:28:02Z","uri":"http://zotero.org/users/11367251/items/55EDSIUF","itemID":1506,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400321,"version":0},"$loki":30},{"itemID":1632,"item":{"key":"57CGHYAS","version":2186,"itemType":"webpage","title":"API Reference: Chat","date":"2023","url":"https://platform.openai.com/docs/api-reference/chat","accessDate":"2023-11-29","websiteTitle":"API Reference: Chat","creators":[{"name":"OpenAI","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T06:24:15Z","dateModified":"2023-12-08T17:18:40Z","uri":"http://zotero.org/users/11367251/items/57CGHYAS","itemID":1632,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400322,"version":0},"$loki":31},{"itemID":610,"item":{"key":"5APD84BZ","version":814,"itemType":"preprint","title":"Sequence to Sequence Learning with Neural Networks","abstractNote":"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difﬁculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.","date":"2014-12-14","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1409.3215","accessDate":"2023-09-05T13:46:51Z","extra":"arXiv:1409.3215 [cs]","repository":"arXiv","archiveID":"arXiv:1409.3215","creators":[{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"},{"firstName":"Oriol","lastName":"Vinyals","creatorType":"author"},{"firstName":"Quoc V.","lastName":"Le","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:46:51Z","dateModified":"2023-09-05T13:46:52Z","uri":"http://zotero.org/users/11367251/items/5APD84BZ","itemID":610,"attachments":[{"key":"XAQG2SJ8","version":818,"itemType":"attachment","title":"sutskeverSequenceSequenceLearning2014.pdf","parentItem":"5APD84BZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/sutskeverSequenceSequenceLearning2014.pdf","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:33Z","dateModified":"2023-09-05T13:47:33Z","uri":"http://zotero.org/users/11367251/items/XAQG2SJ8","localPath":"/Users/reyvababtista/Projects/Papers/sutskeverSequenceSequenceLearning2014.pdf","defaultPath":"files/624/sutskeverSequenceSequenceLearning2014.pdf"}],"notes":[{"key":"DTYMMEVE","version":814,"itemType":"note","parentItem":"5APD84BZ","note":"Comment: 9 pages","tags":[],"relations":{},"dateAdded":"2023-09-05T13:46:51Z","dateModified":"2023-09-05T13:46:51Z","uri":"http://zotero.org/users/11367251/items/DTYMMEVE"}]},"meta":{"revision":0,"created":1709832400322,"version":0},"$loki":32},{"itemID":742,"item":{"key":"5HP5N5SG","version":995,"itemType":"preprint","title":"A Unified Sequence Interface for Vision Tasks","abstractNote":"While language tasks are naturally expressed in a single, uniﬁed, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of “core” computer vision tasks can also be uniﬁed if formulated in terms of a shared pixelto-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a uniﬁed interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-speciﬁc customization. To solve a speciﬁc task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-speciﬁc output. We show that such a model can achieve competitive performance compared to well-established task-speciﬁc models.","date":"2022-10-15","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2206.07669","accessDate":"2023-09-30T03:16:10Z","extra":"arXiv:2206.07669 [cs]","repository":"arXiv","archiveID":"arXiv:2206.07669","creators":[{"firstName":"Ting","lastName":"Chen","creatorType":"author"},{"firstName":"Saurabh","lastName":"Saxena","creatorType":"author"},{"firstName":"Lala","lastName":"Li","creatorType":"author"},{"firstName":"Tsung-Yi","lastName":"Lin","creatorType":"author"},{"firstName":"David J.","lastName":"Fleet","creatorType":"author"},{"firstName":"Geoffrey","lastName":"Hinton","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-30T03:16:10Z","dateModified":"2023-09-30T03:16:11Z","uri":"http://zotero.org/users/11367251/items/5HP5N5SG","itemID":742,"attachments":[{"key":"GBCZFHSA","version":996,"itemType":"attachment","title":"chenUnifiedSequenceInterface2022.pdf","parentItem":"5HP5N5SG","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenUnifiedSequenceInterface2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GBCZFHSA/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/2\">2 Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/2\">2.1 A unified interface with tokenization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/3\">2.2 Unified architecture and objective function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/4\">2.3 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/5\">2.4 Inference and de-tokenization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/5\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/5\">3.1 Experimental settings and implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/6\">3.2 Quantitative results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/7\">3.3 Qualitative results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/7\">4 Related work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GBCZFHSA/10\">5 Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-30T03:16:15Z","dateModified":"2023-09-30T03:16:16Z","uri":"http://zotero.org/users/11367251/items/GBCZFHSA","localPath":"/Users/reyvababtista/Projects/Papers/chenUnifiedSequenceInterface2022.pdf","defaultPath":"files/744/chenUnifiedSequenceInterface2022.pdf"}],"notes":[{"key":"RIKPFBYI","version":995,"itemType":"note","parentItem":"5HP5N5SG","note":"Comment: The first three authors contributed equally","tags":[],"relations":{},"dateAdded":"2023-09-30T03:16:10Z","dateModified":"2023-09-30T03:16:10Z","uri":"http://zotero.org/users/11367251/items/RIKPFBYI"}]},"meta":{"revision":0,"created":1709832400323,"version":0},"$loki":33},{"itemID":1503,"item":{"key":"5MWGELKZ","version":1684,"itemType":"blogPost","title":"About knowledge panels","date":"2023","url":"https://support.google.com/knowledgepanel/answer/9163198","accessDate":"2023-11-08","blogTitle":"About knowledge panels","creators":[{"name":"Google","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:18:10Z","dateModified":"2023-11-09T21:19:14Z","uri":"http://zotero.org/users/11367251/items/5MWGELKZ","itemID":1503,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400323,"version":0},"$loki":34},{"itemID":1694,"item":{"key":"5P99R7VL","version":2152,"itemType":"journalArticle","title":"Zero-Shot Text-to-Image Generation","abstractNote":"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.","date":"2021","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2102.12092","accessDate":"2023-12-06T23:09:17Z","rights":"Creative Commons Attribution 4.0 International","extra":"Publisher: arXiv\nVersion Number: 2","DOI":"10.48550/ARXIV.2102.12092","creators":[{"firstName":"Aditya","lastName":"Ramesh","creatorType":"author"},{"firstName":"Mikhail","lastName":"Pavlov","creatorType":"author"},{"firstName":"Gabriel","lastName":"Goh","creatorType":"author"},{"firstName":"Scott","lastName":"Gray","creatorType":"author"},{"firstName":"Chelsea","lastName":"Voss","creatorType":"author"},{"firstName":"Alec","lastName":"Radford","creatorType":"author"},{"firstName":"Mark","lastName":"Chen","creatorType":"author"},{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:09:17Z","dateModified":"2023-12-06T23:09:17Z","uri":"http://zotero.org/users/11367251/items/5P99R7VL","itemID":1694,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400324,"version":0},"$loki":35},{"itemID":1505,"item":{"key":"5W9ID4UD","version":1693,"itemType":"blogPost","title":"How Google uses location information","date":"2023","url":"https://policies.google.com/technologies/location-data","accessDate":"2023-11-08","blogTitle":"How Google uses location information","creators":[{"name":"Google","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:26:14Z","dateModified":"2023-11-09T21:26:49Z","uri":"http://zotero.org/users/11367251/items/5W9ID4UD","itemID":1505,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400325,"version":0},"$loki":36},{"itemID":596,"item":{"key":"63TWFUVV","version":798,"itemType":"journalArticle","title":"A Brief Survey of Deep Reinforcement Learning","abstractNote":"Deep reinforcement learning is poised to revolutionise the ﬁeld of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general ﬁeld of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the ﬁeld.","date":"11/2017","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1708.05866","accessDate":"2023-09-05T13:43:02Z","extra":"arXiv:1708.05866 [cs, stat]","volume":"34","pages":"26-38","publicationTitle":"IEEE Signal Processing Magazine","DOI":"10.1109/MSP.2017.2743240","issue":"6","journalAbbreviation":"IEEE Signal Process. Mag.","ISSN":"1053-5888","creators":[{"firstName":"Kai","lastName":"Arulkumaran","creatorType":"author"},{"firstName":"Marc Peter","lastName":"Deisenroth","creatorType":"author"},{"firstName":"Miles","lastName":"Brundage","creatorType":"author"},{"firstName":"Anil Anthony","lastName":"Bharath","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:43:02Z","dateModified":"2023-09-05T13:43:02Z","uri":"http://zotero.org/users/11367251/items/63TWFUVV","itemID":596,"attachments":[{"key":"U6TJ2N5C","version":817,"itemType":"attachment","title":"arulkumaranBriefSurveyDeep2017.pdf","parentItem":"63TWFUVV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/arulkumaranBriefSurveyDeep2017.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_U6TJ2N5C/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/2\">II Reward-driven Behaviour</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/2\">II-A Markov Decision Processes</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/3\">II-B Challenges in RL</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/4\">III Reinforcement Learning Algorithms</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/4\">III-A Value Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/4\">III-B Sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/4\">III-C Policy Search</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/5\">III-D Planning and Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/6\">III-E The Rise of DRL</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/6\">IV Value Functions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/6\">IV-A Function Approximation and the DQN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/7\">IV-B Q-Function Modifications</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/8\">V Policy Search</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/8\">V-A Backpropagation through Stochastic Functions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/8\">V-B Compounding Errors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/9\">V-C Actor-Critic Methods</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/10\">VI Current Research and Challenges</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/10\">VI-A Model-based RL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/10\">VI-B Exploration vs. Exploitation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/11\">VI-C Hierarchical RL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/11\">VI-D Imitation Learning and Inverse RL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/11\">VI-E Multi-agent RL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/11\">VI-F Memory and Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/12\">VI-G Transfer Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/12\">VI-H Benchmarks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/13\">VII Conclusion: Beyond Pattern Recognition</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/16\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/16\">Kai Arulkumaran</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/16\">Marc Peter Deisenroth</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/16\">Miles Brundage</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_U6TJ2N5C/16\">Anil Anthony Bharath</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:24Z","dateModified":"2023-09-05T13:47:25Z","uri":"http://zotero.org/users/11367251/items/U6TJ2N5C","localPath":"/Users/reyvababtista/Projects/Papers/arulkumaranBriefSurveyDeep2017.pdf","defaultPath":"files/612/arulkumaranBriefSurveyDeep2017.pdf"}],"notes":[{"key":"QNZKKUJD","version":798,"itemType":"note","parentItem":"63TWFUVV","note":"Comment: IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding (arXiv extended version)","tags":[],"relations":{},"dateAdded":"2023-09-05T13:43:02Z","dateModified":"2023-09-05T13:43:02Z","uri":"http://zotero.org/users/11367251/items/QNZKKUJD"}]},"meta":{"revision":0,"created":1709832400325,"version":0},"$loki":37},{"itemID":1475,"item":{"key":"6DFQPRDD","version":1625,"itemType":"journalArticle","title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions","abstractNote":"We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.","date":"12/2014","language":"en","shortTitle":"From image descriptions to visual denotations","libraryCatalog":"DOI.org (Crossref)","url":"https://direct.mit.edu/tacl/article/43313","accessDate":"2023-11-08T23:01:07Z","volume":"2","pages":"67-78","publicationTitle":"Transactions of the Association for Computational Linguistics","DOI":"10.1162/tacl_a_00166","journalAbbreviation":"TACL","ISSN":"2307-387X","creators":[{"firstName":"Peter","lastName":"Young","creatorType":"author"},{"firstName":"Alice","lastName":"Lai","creatorType":"author"},{"firstName":"Micah","lastName":"Hodosh","creatorType":"author"},{"firstName":"Julia","lastName":"Hockenmaier","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:01:07Z","dateModified":"2023-11-08T23:01:07Z","uri":"http://zotero.org/users/11367251/items/6DFQPRDD","itemID":1475,"attachments":[{"key":"GT2I43W9","version":1625,"itemType":"attachment","title":"youngimagedescriptionsvisual2014.pdf","parentItem":"6DFQPRDD","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/youngimagedescriptionsvisual2014.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:01:09Z","dateModified":"2023-11-08T23:01:09Z","uri":"http://zotero.org/users/11367251/items/GT2I43W9","localPath":"/Users/reyvababtista/Projects/Papers/youngimagedescriptionsvisual2014.pdf","defaultPath":"files/1477/youngimagedescriptionsvisual2014.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400326,"version":0},"$loki":38},{"itemID":50,"item":{"key":"6FAJT9C8","version":201,"itemType":"journalArticle","title":"Deep Learning for Large-Scale Traffic-Sign Detection and Recognition","date":"4/2020","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8709983/","accessDate":"2023-03-20T16:29:21Z","volume":"21","pages":"1427-1440","publicationTitle":"IEEE Transactions on Intelligent Transportation Systems","DOI":"10.1109/TITS.2019.2913588","issue":"4","journalAbbreviation":"IEEE Trans. Intell. Transport. Syst.","ISSN":"1524-9050, 1558-0016","creators":[{"firstName":"Domen","lastName":"Tabernik","creatorType":"author"},{"firstName":"Danijel","lastName":"Skocaj","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/6S3BF2JN"]},"dateAdded":"2023-03-20T16:29:21Z","dateModified":"2023-03-22T17:03:37Z","uri":"http://zotero.org/users/11367251/items/6FAJT9C8","itemID":50,"attachments":[{"key":"M2BRCYR5","version":240,"itemType":"attachment","title":"tabernikDeepLearningLargeScale2020.pdf","parentItem":"6FAJT9C8","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/tabernikDeepLearningLargeScale2020.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:02Z","dateModified":"2023-03-22T18:52:22Z","uri":"http://zotero.org/users/11367251/items/M2BRCYR5","localPath":"/Users/reyvababtista/Projects/Papers/tabernikDeepLearningLargeScale2020.pdf","defaultPath":"files/246/tabernikDeepLearningLargeScale2020.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400327,"version":0},"$loki":39},{"itemID":1846,"item":{"key":"6JN8DTJ8","version":2326,"itemType":"journalArticle","title":"Developing Facebook Chatbot Based on Deep Learning Using RASA Framework for University Enquiries","abstractNote":"Abstract\n            Smart systems for universities powered by artificial intelligence have been massively developed to help humans in various tasks. The chatbot concept is not something new in today’s society which is developing with recent technology. College students or candidates of college students often need actual information like asking for something to customer service, especially during this pandemic, when it is difficult to have an immediate face-to-face meeting. Chatbots are functionally helping in several things such as curriculum information, admission for new students, schedule info for any lecture courses, students grade information, and some adding features for Muslim worships schedule, also weather forecast information. This Chatbot is developed by deep learning models, which was adopted by an artificial intelligence model that replicates human intelligence with some specific training schemes. This kind of deep learning is based on RNN which has some specific memory savings scheme for the deep learning model, specifically this chatbot using LSTM which already integrates by RASA framework. LSTM is also known as Long Short Term Memory which efficiently saves some required memory but will remove some memory that is not needed. This Chatbot uses the Facebook platform because the Facebook users have already reached up to 60.8% of its entire population in Indonesia.","date":"2021-02-01","libraryCatalog":"DOI.org (Crossref)","url":"https://iopscience.iop.org/article/10.1088/1757-899X/1077/1/012060","accessDate":"2024-01-19T15:38:48Z","volume":"1077","pages":"012060","publicationTitle":"IOP Conference Series: Materials Science and Engineering","DOI":"10.1088/1757-899X/1077/1/012060","issue":"1","journalAbbreviation":"IOP Conf. Ser.: Mater. Sci. Eng.","ISSN":"1757-8981, 1757-899X","creators":[{"firstName":"Yurio","lastName":"Windiatmoko","creatorType":"author"},{"firstName":"Ridho","lastName":"Rahmadi","creatorType":"author"},{"firstName":"Ahmad Fathan","lastName":"Hidayatullah","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:38:48Z","dateModified":"2024-01-19T15:38:48Z","uri":"http://zotero.org/users/11367251/items/6JN8DTJ8","itemID":1846,"attachments":[{"key":"HNHY6MCG","version":2327,"itemType":"attachment","title":"windiatmokoDevelopingFacebookChatbot2021.pdf","parentItem":"6JN8DTJ8","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/windiatmokoDevelopingFacebookChatbot2021.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:38:55Z","dateModified":"2024-01-19T15:38:55Z","uri":"http://zotero.org/users/11367251/items/HNHY6MCG","localPath":"/Users/reyvababtista/Projects/Papers/windiatmokoDevelopingFacebookChatbot2021.pdf","defaultPath":"files/1848/windiatmokoDevelopingFacebookChatbot2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400327,"version":0},"$loki":40},{"itemID":1004,"item":{"key":"6LTCGXQ9","version":1391,"itemType":"preprint","title":"Unsupervised Cross-lingual Representation Learning at Scale","abstractNote":"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.","date":"2020-04-07","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1911.02116","accessDate":"2023-11-07T23:47:14Z","extra":"arXiv:1911.02116 [cs]","repository":"arXiv","archiveID":"arXiv:1911.02116","creators":[{"firstName":"Alexis","lastName":"Conneau","creatorType":"author"},{"firstName":"Kartikay","lastName":"Khandelwal","creatorType":"author"},{"firstName":"Naman","lastName":"Goyal","creatorType":"author"},{"firstName":"Vishrav","lastName":"Chaudhary","creatorType":"author"},{"firstName":"Guillaume","lastName":"Wenzek","creatorType":"author"},{"firstName":"Francisco","lastName":"Guzmán","creatorType":"author"},{"firstName":"Edouard","lastName":"Grave","creatorType":"author"},{"firstName":"Myle","lastName":"Ott","creatorType":"author"},{"firstName":"Luke","lastName":"Zettlemoyer","creatorType":"author"},{"firstName":"Veselin","lastName":"Stoyanov","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T23:47:14Z","dateModified":"2023-11-07T23:47:14Z","uri":"http://zotero.org/users/11367251/items/6LTCGXQ9","itemID":1004,"attachments":[{"key":"BJHN4FLH","version":1394,"itemType":"attachment","title":"conneauUnsupervisedCrosslingualRepresentation2020.pdf","parentItem":"6LTCGXQ9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/conneauUnsupervisedCrosslingualRepresentation2020.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T23:47:24Z","dateModified":"2023-11-07T23:47:24Z","uri":"http://zotero.org/users/11367251/items/BJHN4FLH","localPath":"/Users/reyvababtista/Projects/Papers/conneauUnsupervisedCrosslingualRepresentation2020.pdf","defaultPath":"files/1006/conneauUnsupervisedCrosslingualRepresentation2020.pdf"}],"notes":[{"key":"YSDTR9RN","version":1391,"itemType":"note","parentItem":"6LTCGXQ9","note":"Comment: ACL 2020 (+ updated results)","tags":[],"relations":{},"dateAdded":"2023-11-07T23:47:14Z","dateModified":"2023-11-07T23:47:14Z","uri":"http://zotero.org/users/11367251/items/YSDTR9RN"}]},"meta":{"revision":0,"created":1709832400328,"version":0},"$loki":41},{"itemID":1131,"item":{"key":"6MSWA35Z","version":1442,"itemType":"preprint","title":"Flamingo: a Visual Language Model for Few-Shot Learning","abstractNote":"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their ﬂexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-speciﬁc examples. On numerous benchmarks, Flamingo outperforms models ﬁne-tuned on thousands of times more task-speciﬁc data.","date":"2022-11-15","language":"en","shortTitle":"Flamingo","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2204.14198","accessDate":"2023-11-08T15:39:28Z","extra":"arXiv:2204.14198 [cs]","repository":"arXiv","archiveID":"arXiv:2204.14198","creators":[{"firstName":"Jean-Baptiste","lastName":"Alayrac","creatorType":"author"},{"firstName":"Jeff","lastName":"Donahue","creatorType":"author"},{"firstName":"Pauline","lastName":"Luc","creatorType":"author"},{"firstName":"Antoine","lastName":"Miech","creatorType":"author"},{"firstName":"Iain","lastName":"Barr","creatorType":"author"},{"firstName":"Yana","lastName":"Hasson","creatorType":"author"},{"firstName":"Karel","lastName":"Lenc","creatorType":"author"},{"firstName":"Arthur","lastName":"Mensch","creatorType":"author"},{"firstName":"Katie","lastName":"Millican","creatorType":"author"},{"firstName":"Malcolm","lastName":"Reynolds","creatorType":"author"},{"firstName":"Roman","lastName":"Ring","creatorType":"author"},{"firstName":"Eliza","lastName":"Rutherford","creatorType":"author"},{"firstName":"Serkan","lastName":"Cabi","creatorType":"author"},{"firstName":"Tengda","lastName":"Han","creatorType":"author"},{"firstName":"Zhitao","lastName":"Gong","creatorType":"author"},{"firstName":"Sina","lastName":"Samangooei","creatorType":"author"},{"firstName":"Marianne","lastName":"Monteiro","creatorType":"author"},{"firstName":"Jacob","lastName":"Menick","creatorType":"author"},{"firstName":"Sebastian","lastName":"Borgeaud","creatorType":"author"},{"firstName":"Andrew","lastName":"Brock","creatorType":"author"},{"firstName":"Aida","lastName":"Nematzadeh","creatorType":"author"},{"firstName":"Sahand","lastName":"Sharifzadeh","creatorType":"author"},{"firstName":"Mikolaj","lastName":"Binkowski","creatorType":"author"},{"firstName":"Ricardo","lastName":"Barreira","creatorType":"author"},{"firstName":"Oriol","lastName":"Vinyals","creatorType":"author"},{"firstName":"Andrew","lastName":"Zisserman","creatorType":"author"},{"firstName":"Karen","lastName":"Simonyan","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T15:39:28Z","dateModified":"2023-11-08T15:39:28Z","uri":"http://zotero.org/users/11367251/items/6MSWA35Z","itemID":1131,"attachments":[{"key":"BBWD876L","version":1443,"itemType":"attachment","title":"alayracFlamingoVisualLanguage2022.pdf","parentItem":"6MSWA35Z","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/alayracFlamingoVisualLanguage2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_BBWD876L/3\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/4\">2 Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/5\">2.1 Visual processing and the Perceiver Resampler</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/5\">2.2 Conditioning frozen language models on visual representations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/6\">2.3 Multi-visual input support: per-image/video attention masking</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/6\">2.4 Training on a mixture of vision and language datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/6\">2.5 Task adaptation with few-shot in-context learning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/7\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/7\">3.1 Few-shot learning on vision-language tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/8\">3.2 Fine-tuning Flamingo as a pretrained vision-language model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/8\">3.3 Ablation studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/9\">4 Related work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/10\">5 Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/23\">A Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/23\">A.1 Model details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/23\">A.1.1 Perceiver Resampler</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/23\">A.1.2 gated xattn-dense details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/24\">A.1.3 Multi-visual input support</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/24\">A.1.4 Transformer architecture</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/25\">A.2 In-context few-shot evaluation details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/26\">A.3 Training dataset details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/27\">A.3.1 M3W collection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/27\">A.3.2 M3W image-placement augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/27\">A.3.3 LTIP and VTP: Visual data paired with text</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/28\">A.3.4 Dataset deduplication against evaluation tasks</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/28\">B Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/28\">B.1 Training and evaluation details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/28\">B.1.1 Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/29\">B.1.2 Training details for the Flamingo models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/30\">B.1.3 Contrastive model details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/30\">B.1.4 Evaluation benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/32\">B.1.5 Few-shot learning evaluation hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/32\">B.1.6 Dialogue prompt</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/33\">B.2 Additional performance results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/33\">B.2.1 Few-shot learning on classification tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/33\">B.2.2 Fine-tuning Flamingo as a pretrained vision-language model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/34\">B.2.3 Zero-shot performance of the pretrained contrastive model</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/35\">B.3 Extended ablation studies</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/35\">B.3.1 Flamingo</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/36\">B.3.2 Dataset mixing strategies for the contrastive pretraining</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/37\">C Qualitative results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/38\">D Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/38\">D.1 Limitations, failure cases and opportunities</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/43\">D.2 Benefits, risks and mitigation strategies</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/43\">D.2.1 Benefits</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/43\">D.2.2 Risks and mitigation strategies</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/45\">E Flamingo Model Card</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/47\">F Datasheets</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/47\">F.1 M3W dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_BBWD876L/49\">F.2 Image and video text pair datasets</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/50\">F.2.1 Datasheet for LTIP</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/52\">F.2.2 Datasheet for VTP</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_BBWD876L/53\">G Credit for visual content</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-08T15:39:32Z","dateModified":"2023-11-08T15:39:32Z","uri":"http://zotero.org/users/11367251/items/BBWD876L","localPath":"/Users/reyvababtista/Projects/Papers/alayracFlamingoVisualLanguage2022.pdf","defaultPath":"files/1133/alayracFlamingoVisualLanguage2022.pdf"}],"notes":[{"key":"QZSHFTBS","version":1442,"itemType":"note","parentItem":"6MSWA35Z","note":"Comment: 54 pages. In Proceedings of Neural Information Processing Systems (NeurIPS) 2022","tags":[],"relations":{},"dateAdded":"2023-11-08T15:39:28Z","dateModified":"2023-11-08T15:39:28Z","uri":"http://zotero.org/users/11367251/items/QZSHFTBS"}]},"meta":{"revision":0,"created":1709832400336,"version":0},"$loki":42},{"itemID":299,"item":{"key":"6SHLBW37","version":307,"itemType":"journalArticle","title":"Field Evaluation of the Smartphone-based Travel Behaviour Data Collection App “SmartMo”","abstractNote":"This paper outlines an innovative approach to the evaluation of a self-administered smartphone-based survey for the collection of travel behaviour data. For this approach, a traditional travel survey is modified to match mobile devices. The smartphone application “SmartMo” is designed in a multi-stage iterative development process. It is then implemented and evaluated through a number of field tests involving 97 participants. Results of the field evaluation will be discussed including the technical performance (e.g. secure data transfer and data management, energy consumption, map-matching), usability (e.g. comprehensibility, handling, joy of use) as well as user acceptance (e.g. willingness to participate, data protection and privacy). A brief overview of the SmartMo data collection system will also be provided.","date":"2015","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S2352146515003142","accessDate":"2023-03-26T21:26:15Z","volume":"11","pages":"263-279","publicationTitle":"Transportation Research Procedia","DOI":"10.1016/j.trpro.2015.12.023","journalAbbreviation":"Transportation Research Procedia","ISSN":"23521465","creators":[{"firstName":"Martin","lastName":"Berger","creatorType":"author"},{"firstName":"Mario","lastName":"Platzer","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:26:15Z","dateModified":"2023-03-26T21:26:15Z","uri":"http://zotero.org/users/11367251/items/6SHLBW37","itemID":299,"attachments":[{"key":"H6HXAQ6I","version":315,"itemType":"attachment","title":"bergerFieldEvaluationSmartphonebased2015.pdf","parentItem":"6SHLBW37","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bergerFieldEvaluationSmartphonebased2015.pdf","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:27Z","dateModified":"2023-03-26T21:29:27Z","uri":"http://zotero.org/users/11367251/items/H6HXAQ6I","localPath":"/Users/reyvababtista/Projects/Papers/bergerFieldEvaluationSmartphonebased2015.pdf","defaultPath":"files/313/bergerFieldEvaluationSmartphonebased2015.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400336,"version":0},"$loki":43},{"itemID":923,"item":{"key":"76B56T6C","version":1178,"itemType":"blogPost","title":"How Google autocomplete predictions work","date":"2023-11-04","url":"https://support.google.com/websearch/answer/7368877","accessDate":"2023-11-04","blogTitle":"How Google autocomplete predictions work","creators":[{"name":"Google Search Help","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T22:22:59Z","dateModified":"2023-11-04T22:24:15Z","uri":"http://zotero.org/users/11367251/items/76B56T6C","itemID":923,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400337,"version":0},"$loki":44},{"itemID":1128,"item":{"key":"779GNEXK","version":1437,"itemType":"preprint","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering","abstractNote":"Problems at the intersection of vision and language are of signiﬁcant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inﬂated sense of their capability.","date":"2017-05-15","language":"en","shortTitle":"Making the V in VQA Matter","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1612.00837","accessDate":"2023-11-08T15:37:52Z","extra":"arXiv:1612.00837 [cs]","repository":"arXiv","archiveID":"arXiv:1612.00837","creators":[{"firstName":"Yash","lastName":"Goyal","creatorType":"author"},{"firstName":"Tejas","lastName":"Khot","creatorType":"author"},{"firstName":"Douglas","lastName":"Summers-Stay","creatorType":"author"},{"firstName":"Dhruv","lastName":"Batra","creatorType":"author"},{"firstName":"Devi","lastName":"Parikh","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T15:37:52Z","dateModified":"2023-11-08T15:37:52Z","uri":"http://zotero.org/users/11367251/items/779GNEXK","itemID":1128,"attachments":[{"key":"7PC5ZIUY","version":1440,"itemType":"attachment","title":"goyalMakingVQAMatter2017.pdf","parentItem":"779GNEXK","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/goyalMakingVQAMatter2017.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T15:38:06Z","dateModified":"2023-11-08T15:38:06Z","uri":"http://zotero.org/users/11367251/items/7PC5ZIUY","localPath":"/Users/reyvababtista/Projects/Papers/goyalMakingVQAMatter2017.pdf","defaultPath":"files/1129/goyalMakingVQAMatter2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400338,"version":0},"$loki":45},{"itemID":536,"item":{"key":"792XQQP3","version":709,"itemType":"preprint","title":"ReAct: Synergizing Reasoning and Acting in Language Models","abstractNote":"While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.","date":"2023-03-09","language":"en","shortTitle":"ReAct","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2210.03629","accessDate":"2023-06-29T19:49:29Z","extra":"arXiv:2210.03629 [cs]","repository":"arXiv","archiveID":"arXiv:2210.03629","creators":[{"firstName":"Shunyu","lastName":"Yao","creatorType":"author"},{"firstName":"Jeffrey","lastName":"Zhao","creatorType":"author"},{"firstName":"Dian","lastName":"Yu","creatorType":"author"},{"firstName":"Nan","lastName":"Du","creatorType":"author"},{"firstName":"Izhak","lastName":"Shafran","creatorType":"author"},{"firstName":"Karthik","lastName":"Narasimhan","creatorType":"author"},{"firstName":"Yuan","lastName":"Cao","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-29T19:49:29Z","dateModified":"2023-06-29T19:49:30Z","uri":"http://zotero.org/users/11367251/items/792XQQP3","itemID":536,"attachments":[{"key":"6BTBYT2V","version":709,"itemType":"attachment","title":"yaoReActSynergizingReasoning2023.pdf","parentItem":"792XQQP3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/yaoReActSynergizingReasoning2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_6BTBYT2V/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/3\">2 ReAct: Synergizing Reasoning + Acting</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/4\">3 Knowledge-Intensive Reasoning Tasks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/4\">3.1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/4\">3.2 Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/5\">3.3 Results and Observations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/7\">4 Decision Making Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/9\">5 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/9\">6 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/14\">A Additional Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/14\">A.1 GPT-3 Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/14\">A.2 ReAct obtains up-to-date knowledge on HotpotQA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/14\">A.3 Human-in-the-loop behavior correction on AlfWorld</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/15\">B Experiment Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/15\">B.1 HotpotQA Finetuning Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/15\">B.2 AlfWorld IM-Style Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/16\">C Prompts</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/16\">C.1 HotpotQA</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/20\">C.2 Fever</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/22\">C.3 Webshop</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/23\">C.4 ALFWorld</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/25\">D Trajectories</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/25\">D.1 FEVER Trajectories</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/27\">D.2 ALFWorld Trajectories</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/27\">D.2.1 ReAct trajectory</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/28\">D.2.2 Act trajectory</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/29\">D.2.3 ReAct-IM trajectory</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/31\">D.3 Webshop Trajectories</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/32\">E More Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6BTBYT2V/32\">E.1 Success and Failure Modes Analysis</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-29T19:49:30Z","dateModified":"2023-06-29T19:49:30Z","uri":"http://zotero.org/users/11367251/items/6BTBYT2V","localPath":"/Users/reyvababtista/Projects/Papers/yaoReActSynergizingReasoning2023.pdf","defaultPath":"files/538/yaoReActSynergizingReasoning2023.pdf"}],"notes":[{"key":"VE2D7GBT","version":709,"itemType":"note","parentItem":"792XQQP3","note":"Comment: v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io","tags":[],"relations":{},"dateAdded":"2023-06-29T19:49:29Z","dateModified":"2023-06-29T19:49:29Z","uri":"http://zotero.org/users/11367251/items/VE2D7GBT"}]},"meta":{"revision":0,"created":1709832400340,"version":0},"$loki":46},{"itemID":444,"item":{"key":"7HDYVE3X","version":559,"itemType":"blogPost","title":"Train Custom Data","date":"5/9/2023","url":"https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/","accessDate":"2023-05-09","blogTitle":"Train Custom Data","creators":[{"name":"Ultralytics","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-05-10T00:32:58Z","dateModified":"2023-05-10T00:33:36Z","uri":"http://zotero.org/users/11367251/items/7HDYVE3X","itemID":444,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400341,"version":0},"$loki":47},{"itemID":922,"item":{"key":"7LAB2QEM","version":1171,"itemType":"blogPost","title":"How Google autocomplete works in Search","date":"04/20/2018","url":"https://blog.google/products/search/how-google-autocomplete-works-search/","accessDate":"2023-11-04","blogTitle":"How Google autocomplete works in Search","creators":[{"name":"Danny Sullivan","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T22:16:39Z","dateModified":"2023-11-04T22:17:37Z","uri":"http://zotero.org/users/11367251/items/7LAB2QEM","itemID":922,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400342,"version":0},"$loki":48},{"itemID":1452,"item":{"key":"7LTA2X48","version":1593,"itemType":"preprint","title":"Do ImageNet Classifiers Generalize to ImageNet?","abstractNote":"We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets.","date":"2019-06-12","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1902.10811","accessDate":"2023-11-08T22:53:15Z","extra":"arXiv:1902.10811 [cs, stat]","repository":"arXiv","archiveID":"arXiv:1902.10811","creators":[{"firstName":"Benjamin","lastName":"Recht","creatorType":"author"},{"firstName":"Rebecca","lastName":"Roelofs","creatorType":"author"},{"firstName":"Ludwig","lastName":"Schmidt","creatorType":"author"},{"firstName":"Vaishaal","lastName":"Shankar","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:53:15Z","dateModified":"2023-11-08T22:53:15Z","uri":"http://zotero.org/users/11367251/items/7LTA2X48","itemID":1452,"attachments":[{"key":"7MCNDWYB","version":1596,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1902.10811","accessDate":"2023-11-08T22:53:28Z","parentItem":"7LTA2X48","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1902.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:53:28Z","dateModified":"2023-11-08T22:53:28Z","uri":"http://zotero.org/users/11367251/items/7MCNDWYB","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/7MCNDWYB/1902.html","defaultPath":"files/1455/1902.html"},{"key":"849KEEPI","version":1594,"itemType":"attachment","title":"rechtImageNetClassifiersGeneralize2019.pdf","parentItem":"7LTA2X48","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/rechtImageNetClassifiersGeneralize2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:53:22Z","dateModified":"2023-11-08T22:53:22Z","uri":"http://zotero.org/users/11367251/items/849KEEPI","localPath":"/Users/reyvababtista/Projects/Papers/rechtImageNetClassifiersGeneralize2019.pdf","defaultPath":"files/1454/rechtImageNetClassifiersGeneralize2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400342,"version":0},"$loki":49},{"itemID":1507,"item":{"key":"7NTHPKWI","version":1706,"itemType":"blogPost","title":"Media in the age of algorithms","date":"2016/11/16","url":"https://www.oreilly.com/radar/media-in-the-age-of-algorithms/","accessDate":"2023-11-09","blogTitle":"Media in the age of algorithms","creators":[{"name":"Tim O'Reilly","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:28:32Z","dateModified":"2023-11-09T21:29:09Z","uri":"http://zotero.org/users/11367251/items/7NTHPKWI","itemID":1507,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400343,"version":0},"$loki":50},{"itemID":421,"item":{"key":"7RZ5E8YD","version":495,"itemType":"webpage","title":"nixon-diary-extraction","date":"2023-05-04","url":"https://github.com/ChristianCme/nixon-diary-extraction","accessDate":"2023-05-04","websiteTitle":"nixon-diary-extraction","creators":[{"name":"Christian Cmehil-Warn","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T01:39:39Z","dateModified":"2023-05-05T01:51:23Z","uri":"http://zotero.org/users/11367251/items/7RZ5E8YD","itemID":421,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400343,"version":0},"$loki":51},{"itemID":927,"item":{"key":"7SAEAWC7","version":1203,"itemType":"blogPost","title":"Mobile-first indexing","date":"11/04/2016","url":"https://developers.google.com/search/blog/2016/11/mobile-first-indexing","accessDate":"2023-11-04","blogTitle":"Mobile-first indexing","creators":[{"name":"Doantam Phan","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-05T14:40:38Z","dateModified":"2023-11-05T14:41:18Z","uri":"http://zotero.org/users/11367251/items/7SAEAWC7","itemID":927,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400343,"version":0},"$loki":52},{"itemID":1789,"item":{"key":"7SMDZVYE","version":2222,"itemType":"report","title":"Evolutionary-scale prediction of atomic level protein structure with a language model","abstractNote":"Abstract\n          Artificial intelligence has the potential to open insight into the structure of proteins at the scale of evolution. It has only recently been possible to extend protein structure prediction to two hundred million cataloged proteins. Characterizing the structures of the exponentially growing billions of protein sequences revealed by large scale gene sequencing experiments would necessitate a break-through in the speed of folding. Here we show that direct inference of structure from primary sequence using a large language model enables an order of magnitude speed-up in high resolution structure prediction. Leveraging the insight that language models learn evolutionary patterns across millions of sequences, we train models up to 15B parameters, the largest language model of proteins to date. As the language models are scaled they learn information that enables prediction of the three-dimensional structure of a protein at the resolution of individual atoms. This results in prediction that is up to 60x faster than state-of-the-art while maintaining resolution and accuracy. Building on this, we present the ESM Metage-nomic Atlas. This is the first large-scale structural characterization of metagenomic proteins, with more than 617 million structures. The atlas reveals more than 225 million high confidence predictions, including millions whose structures are novel in comparison with experimentally determined structures, giving an unprecedented view into the vast breadth and diversity of the structures of some of the least understood proteins on earth.","date":"2022-07-21","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://biorxiv.org/lookup/doi/10.1101/2022.07.20.500902","accessDate":"2023-12-11T19:10:03Z","extra":"DOI: 10.1101/2022.07.20.500902","reportType":"preprint","institution":"Synthetic Biology","creators":[{"firstName":"Zeming","lastName":"Lin","creatorType":"author"},{"firstName":"Halil","lastName":"Akin","creatorType":"author"},{"firstName":"Roshan","lastName":"Rao","creatorType":"author"},{"firstName":"Brian","lastName":"Hie","creatorType":"author"},{"firstName":"Zhongkai","lastName":"Zhu","creatorType":"author"},{"firstName":"Wenting","lastName":"Lu","creatorType":"author"},{"firstName":"Nikita","lastName":"Smetanin","creatorType":"author"},{"firstName":"Robert","lastName":"Verkuil","creatorType":"author"},{"firstName":"Ori","lastName":"Kabeli","creatorType":"author"},{"firstName":"Yaniv","lastName":"Shmueli","creatorType":"author"},{"firstName":"Allan","lastName":"Dos Santos Costa","creatorType":"author"},{"firstName":"Maryam","lastName":"Fazel-Zarandi","creatorType":"author"},{"firstName":"Tom","lastName":"Sercu","creatorType":"author"},{"firstName":"Salvatore","lastName":"Candido","creatorType":"author"},{"firstName":"Alexander","lastName":"Rives","creatorType":"author"}],"tags":[],"collections":["6HQNAPTR"],"relations":{},"dateAdded":"2023-12-11T19:10:03Z","dateModified":"2023-12-11T19:10:03Z","uri":"http://zotero.org/users/11367251/items/7SMDZVYE","itemID":1789,"attachments":[{"key":"K9PQ6ZZV","version":2224,"itemType":"attachment","title":"linEvolutionaryscalepredictionatomic2022.pdf","parentItem":"7SMDZVYE","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/linEvolutionaryscalepredictionatomic2022.pdf","tags":[],"relations":{},"dateAdded":"2023-12-11T19:10:08Z","dateModified":"2023-12-11T19:10:08Z","uri":"http://zotero.org/users/11367251/items/K9PQ6ZZV","localPath":"/Users/reyvababtista/Projects/Papers/linEvolutionaryscalepredictionatomic2022.pdf","defaultPath":"files/1791/linEvolutionaryscalepredictionatomic2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400344,"version":0},"$loki":53},{"itemID":1681,"item":{"key":"7SYAY99V","version":2130,"itemType":"blogPost","title":"Principal component analysis","date":"2023","url":"https://en.wikipedia.org/w/index.php?title=Principal_component_analysis&oldid=1188411307","accessDate":"2023-12-05","blogTitle":"Principal component analysis","creators":[{"name":"Wikipedia contributors","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:07:48Z","dateModified":"2023-12-05T21:08:18Z","uri":"http://zotero.org/users/11367251/items/7SYAY99V","itemID":1681,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400344,"version":0},"$loki":54},{"itemID":1403,"item":{"key":"7TSEQB7V","version":1539,"itemType":"preprint","title":"Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements","abstractNote":"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.","date":"2020-10-08","shortTitle":"Widget Captioning","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2010.04295","accessDate":"2023-11-08T22:41:06Z","extra":"arXiv:2010.04295 [cs]","repository":"arXiv","archiveID":"arXiv:2010.04295","creators":[{"firstName":"Yang","lastName":"Li","creatorType":"author"},{"firstName":"Gang","lastName":"Li","creatorType":"author"},{"firstName":"Luheng","lastName":"He","creatorType":"author"},{"firstName":"Jingjie","lastName":"Zheng","creatorType":"author"},{"firstName":"Hong","lastName":"Li","creatorType":"author"},{"firstName":"Zhiwei","lastName":"Guan","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Human-Computer Interaction","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:41:06Z","dateModified":"2023-11-08T22:41:06Z","uri":"http://zotero.org/users/11367251/items/7TSEQB7V","itemID":1403,"attachments":[{"key":"PYRK57F6","version":1543,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2010.04295","accessDate":"2023-11-08T22:41:23Z","parentItem":"7TSEQB7V","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2010.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:41:23Z","dateModified":"2023-11-08T22:41:23Z","uri":"http://zotero.org/users/11367251/items/PYRK57F6","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/PYRK57F6/2010.html","defaultPath":"files/1407/2010.html"},{"key":"U2D4LE3Q","version":1540,"itemType":"attachment","title":"liWidgetCaptioningGenerating2020.pdf","parentItem":"7TSEQB7V","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liWidgetCaptioningGenerating2020.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:41:17Z","dateModified":"2023-11-08T22:41:17Z","uri":"http://zotero.org/users/11367251/items/U2D4LE3Q","localPath":"/Users/reyvababtista/Projects/Papers/liWidgetCaptioningGenerating2020.pdf","defaultPath":"files/1406/liWidgetCaptioningGenerating2020.pdf"}],"notes":[{"key":"4ENM3M72","version":1539,"itemType":"note","parentItem":"7TSEQB7V","note":"Comment: 16 pages, EMNLP 2020","tags":[],"relations":{},"dateAdded":"2023-11-08T22:41:06Z","dateModified":"2023-11-08T22:41:06Z","uri":"http://zotero.org/users/11367251/items/4ENM3M72"}]},"meta":{"revision":0,"created":1709832400345,"version":0},"$loki":55},{"itemID":1770,"item":{"key":"7WS4UBFZ","version":2211,"itemType":"journalArticle","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences","abstractNote":"Significance\n            Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue–residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction.\n          , \n            In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.","date":"2021-04-13","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://pnas.org/doi/full/10.1073/pnas.2016239118","accessDate":"2023-12-11T16:40:41Z","volume":"118","pages":"e2016239118","publicationTitle":"Proceedings of the National Academy of Sciences","DOI":"10.1073/pnas.2016239118","issue":"15","journalAbbreviation":"Proc. Natl. Acad. Sci. U.S.A.","ISSN":"0027-8424, 1091-6490","creators":[{"firstName":"Alexander","lastName":"Rives","creatorType":"author"},{"firstName":"Joshua","lastName":"Meier","creatorType":"author"},{"firstName":"Tom","lastName":"Sercu","creatorType":"author"},{"firstName":"Siddharth","lastName":"Goyal","creatorType":"author"},{"firstName":"Zeming","lastName":"Lin","creatorType":"author"},{"firstName":"Jason","lastName":"Liu","creatorType":"author"},{"firstName":"Demi","lastName":"Guo","creatorType":"author"},{"firstName":"Myle","lastName":"Ott","creatorType":"author"},{"firstName":"C. Lawrence","lastName":"Zitnick","creatorType":"author"},{"firstName":"Jerry","lastName":"Ma","creatorType":"author"},{"firstName":"Rob","lastName":"Fergus","creatorType":"author"}],"tags":[],"collections":["6HQNAPTR"],"relations":{},"dateAdded":"2023-12-11T16:40:41Z","dateModified":"2023-12-11T16:40:41Z","uri":"http://zotero.org/users/11367251/items/7WS4UBFZ","itemID":1770,"attachments":[{"key":"5VUYCPF4","version":2212,"itemType":"attachment","title":"rivesBiologicalstructurefunction2021.pdf","parentItem":"7WS4UBFZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/rivesBiologicalstructurefunction2021.pdf","tags":[],"relations":{},"dateAdded":"2023-12-11T16:40:46Z","dateModified":"2023-12-11T16:40:46Z","uri":"http://zotero.org/users/11367251/items/5VUYCPF4","localPath":"/Users/reyvababtista/Projects/Papers/rivesBiologicalstructurefunction2021.pdf","defaultPath":"files/1772/rivesBiologicalstructurefunction2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400348,"version":0},"$loki":56},{"itemID":1466,"item":{"key":"7ZFLL2CY","version":1613,"itemType":"preprint","title":"Microsoft COCO: Common Objects in Context","abstractNote":"We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.","date":"2015-02-20","shortTitle":"Microsoft COCO","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1405.0312","accessDate":"2023-11-08T22:59:39Z","extra":"arXiv:1405.0312 [cs]","repository":"arXiv","archiveID":"arXiv:1405.0312","creators":[{"firstName":"Tsung-Yi","lastName":"Lin","creatorType":"author"},{"firstName":"Michael","lastName":"Maire","creatorType":"author"},{"firstName":"Serge","lastName":"Belongie","creatorType":"author"},{"firstName":"Lubomir","lastName":"Bourdev","creatorType":"author"},{"firstName":"Ross","lastName":"Girshick","creatorType":"author"},{"firstName":"James","lastName":"Hays","creatorType":"author"},{"firstName":"Pietro","lastName":"Perona","creatorType":"author"},{"firstName":"Deva","lastName":"Ramanan","creatorType":"author"},{"firstName":"C. Lawrence","lastName":"Zitnick","creatorType":"author"},{"firstName":"Piotr","lastName":"Dollár","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:59:39Z","dateModified":"2023-11-08T22:59:39Z","uri":"http://zotero.org/users/11367251/items/7ZFLL2CY","itemID":1466,"attachments":[{"key":"TQRLZGC6","version":1617,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1405.0312","accessDate":"2023-11-08T22:59:51Z","parentItem":"7ZFLL2CY","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1405.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:59:51Z","dateModified":"2023-11-08T22:59:51Z","uri":"http://zotero.org/users/11367251/items/TQRLZGC6","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/TQRLZGC6/1405.html","defaultPath":"files/1470/1405.html"},{"key":"5Y2XP7NW","version":1614,"itemType":"attachment","title":"linMicrosoftCOCOCommon2015.pdf","parentItem":"7ZFLL2CY","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/linMicrosoftCOCOCommon2015.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:59:45Z","dateModified":"2023-11-08T22:59:45Z","uri":"http://zotero.org/users/11367251/items/5Y2XP7NW","localPath":"/Users/reyvababtista/Projects/Papers/linMicrosoftCOCOCommon2015.pdf","defaultPath":"files/1469/linMicrosoftCOCOCommon2015.pdf"}],"notes":[{"key":"5D2AV937","version":1613,"itemType":"note","parentItem":"7ZFLL2CY","note":"Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list","tags":[],"relations":{},"dateAdded":"2023-11-08T22:59:39Z","dateModified":"2023-11-08T22:59:39Z","uri":"http://zotero.org/users/11367251/items/5D2AV937"}]},"meta":{"revision":0,"created":1709832400349,"version":0},"$loki":57},{"itemID":710,"item":{"key":"7ZIPE3JM","version":922,"itemType":"preprint","title":"AutoML-GPT: Automatic Machine Learning with GPT","abstractNote":"AI tasks encompass a wide range of domains and ﬁelds. While numerous AI models have been designed for speciﬁc tasks and applications, they often require considerable human efforts in ﬁnding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging AutoML-GPT’s robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneﬁcial for many AI tasks.","date":"2023-05-03","language":"en","shortTitle":"AutoML-GPT","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2305.02499","accessDate":"2023-09-21T21:29:52Z","extra":"arXiv:2305.02499 [cs, stat]","repository":"arXiv","archiveID":"arXiv:2305.02499","creators":[{"firstName":"Shujian","lastName":"Zhang","creatorType":"author"},{"firstName":"Chengyue","lastName":"Gong","creatorType":"author"},{"firstName":"Lemeng","lastName":"Wu","creatorType":"author"},{"firstName":"Xingchao","lastName":"Liu","creatorType":"author"},{"firstName":"Mingyuan","lastName":"Zhou","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-21T21:29:52Z","dateModified":"2023-09-21T21:29:52Z","uri":"http://zotero.org/users/11367251/items/7ZIPE3JM","itemID":710,"attachments":[{"key":"FEAKYXBM","version":922,"itemType":"attachment","title":"zhangAutoMLGPTAutomaticMachine2023.pdf","parentItem":"7ZIPE3JM","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhangAutoMLGPTAutomaticMachine2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_FEAKYXBM/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/2\">2 AutoML-GPT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/3\">2.1 Input Decomposition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/4\">2.2 Data Processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/4\">2.3 Model Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/4\">2.4 Hyperparameter Tuning with Predicted Training Log</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/5\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/5\">3.1 Unseen Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/6\">3.2 Object Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/7\">3.3 Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/7\">3.4 Classification</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/8\">4 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FEAKYXBM/9\">5 Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-21T21:29:56Z","dateModified":"2023-09-21T21:29:57Z","uri":"http://zotero.org/users/11367251/items/FEAKYXBM","localPath":"/Users/reyvababtista/Projects/Papers/zhangAutoMLGPTAutomaticMachine2023.pdf","defaultPath":"files/711/zhangAutoMLGPTAutomaticMachine2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400350,"version":0},"$loki":58},{"itemID":634,"item":{"key":"8356ND53","version":825,"itemType":"journalArticle","title":"ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope","abstractNote":"In recent years, artiﬁcial intelligence (AI) and machine learning have been transforming the landscape of scientiﬁc research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientiﬁc research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time.","date":"2023","language":"en","shortTitle":"ChatGPT","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S266734522300024X","accessDate":"2023-09-07T21:41:39Z","volume":"3","pages":"121-154","publicationTitle":"Internet of Things and Cyber-Physical Systems","DOI":"10.1016/j.iotcps.2023.04.003","journalAbbreviation":"Internet of Things and Cyber-Physical Systems","ISSN":"26673452","creators":[{"firstName":"Partha Pratim","lastName":"Ray","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-07T21:41:39Z","dateModified":"2023-09-07T21:41:39Z","uri":"http://zotero.org/users/11367251/items/8356ND53","itemID":634,"attachments":[{"key":"PMVW8ZAC","version":828,"itemType":"attachment","title":"rayChatGPTcomprehensivereview2023.pdf","parentItem":"8356ND53","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/rayChatGPTcomprehensivereview2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PMVW8ZAC/1\">1. Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/2\">1.1. Key milestones in the development of ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/2\">1.2. Improvements and innovations in ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/2\">1.3. Existing issues that ChatGPT can resolve</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/2\">1.4. Growth of ChatGPT in scientific community</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/2\">1.5. Key contributions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/3\">1.6. Organization of paper</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/3\">2. Background of ChatGPT</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/6\">3. Related large language models and tools</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/6\">3.1. GPT-2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/7\">3.2. GPT-3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/8\">3.3. Some key features and aspects of BERT include</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/13\">4. Summary on evaluation of LLM</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/14\">5. Smartness of ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/14\">6. Applications across various domains</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/20\">7. Challenges, ethics, controversies, and future scope</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/23\">8. Ethics in computer science and ChatGPT’s challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/27\">9. Biases and limitations of ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/29\">10. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/29\">Declaration of competing interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/29\">Acknowledgement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PMVW8ZAC/29\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-07T21:44:43Z","dateModified":"2023-09-07T21:44:44Z","uri":"http://zotero.org/users/11367251/items/PMVW8ZAC","localPath":"/Users/reyvababtista/Projects/Papers/rayChatGPTcomprehensivereview2023.pdf","defaultPath":"files/635/rayChatGPTcomprehensivereview2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400351,"version":0},"$loki":59},{"itemID":29,"item":{"key":"84SNSU4X","version":201,"itemType":"journalArticle","title":"Automated Visual Recognizability Evaluation of Traffic Sign Based on 3D LiDAR Point Clouds","abstractNote":"Maintaining the high visual recognizability of trafﬁc signs for trafﬁc safety is a key matter for road network management. Mobile Laser Scanning (MLS) systems provide efﬁcient way of 3D measurement over large-scale trafﬁc environment. This paper presents a quantitative visual recognizability evaluation method for trafﬁc signs in large-scale trafﬁc environment based on trafﬁc recognition theory and MLS 3D point clouds. We ﬁrst propose the Visibility Evaluation Model (VEM) to quantitatively describe the visibility of trafﬁc sign from any given viewpoint, then we proposed the concept of visual recognizability ﬁeld and Trafﬁc Sign Visual Recognizability Evaluation Model (TSVREM) to measure the visual recognizability of a trafﬁc sign. Finally, we present an automatic TSVREM calculation algorithm for MLS 3D point clouds. Experimental results on real MLS 3D point clouds show that the proposed method is feasible and efﬁcient.","date":"2019-06-19","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.mdpi.com/2072-4292/11/12/1453","accessDate":"2023-03-20T14:25:15Z","volume":"11","pages":"1453","publicationTitle":"Remote Sensing","DOI":"10.3390/rs11121453","issue":"12","journalAbbreviation":"Remote Sensing","ISSN":"2072-4292","creators":[{"firstName":"Shanxin","lastName":"Zhang","creatorType":"author"},{"firstName":"Cheng","lastName":"Wang","creatorType":"author"},{"firstName":"Lili","lastName":"Lin","creatorType":"author"},{"firstName":"Chenglu","lastName":"Wen","creatorType":"author"},{"firstName":"Chenhui","lastName":"Yang","creatorType":"author"},{"firstName":"Zhemin","lastName":"Zhang","creatorType":"author"},{"firstName":"Jonathan","lastName":"Li","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/353Q5HQB"]},"dateAdded":"2023-03-20T14:25:15Z","dateModified":"2023-03-22T17:03:38Z","uri":"http://zotero.org/users/11367251/items/84SNSU4X","itemID":29,"attachments":[{"key":"CM38CX8B","version":240,"itemType":"attachment","title":"zhangAutomatedVisualRecognizability2019.pdf","parentItem":"84SNSU4X","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhangAutomatedVisualRecognizability2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CM38CX8B/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/3\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/3\">Visibility and Recognizability Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/4\">Traffic Sign Detection and Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/4\">Road Marking Detection and Classification</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/5\">Definition of Models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/5\">Vem Model</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/7\">Tsvrem Model</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/7\">Viewpoint Recognizability and Definition of Visual Recognizability Field</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/8\">Traffic Sign Visual Recognizability</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/8\">Tsvrem Model Implementation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/9\">Viewpoints Selection and Segment Traffic Sign Surrounding Point Clouds</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/9\">Viewpoints Selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/9\">Segment Traffic Sign Surrounding Point Clouds</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/11\">Traffic Sign Visual Recognizability Computing</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/11\">Traffic Sign Retina Imaging Area Computing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/11\">Occlusion Point Clouds Retina Imaging Area Computing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/12\">Sight Line Deviation Computing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/12\">Standard Traffic Surrounding Setting</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/12\">Experiments And Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/12\">Parameter Sensitivity Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/13\">Datasets Acquisition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/14\">Verification Experiment and Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_CM38CX8B/16\">Accuracy Analysis and Reliability Analysis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/16\">Accuracy Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/19\">Reliability Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/20\">Large-Scale Application Experiment and Discussion</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/22\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CM38CX8B/22\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:02Z","dateModified":"2023-03-22T18:52:22Z","uri":"http://zotero.org/users/11367251/items/CM38CX8B","localPath":"/Users/reyvababtista/Projects/Papers/zhangAutomatedVisualRecognizability2019.pdf","defaultPath":"files/247/zhangAutomatedVisualRecognizability2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400351,"version":0},"$loki":60},{"itemID":1851,"item":{"key":"859XPF43","version":2348,"itemType":"conferencePaper","title":"Design and Implementation of a New Serverless Conversational Survey System","date":"2021-10-29","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9650203/","accessDate":"2024-01-26T03:06:56Z","place":"Dalian, China","publisher":"IEEE","ISBN":"978-1-66544-054-7","pages":"358-363","proceedingsTitle":"2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)","conferenceName":"2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)","DOI":"10.1109/ICDSCA53499.2021.9650203","creators":[{"firstName":"Wenbin","lastName":"Guo","creatorType":"author"},{"firstName":"Shaoyi","lastName":"Zong","creatorType":"author"},{"firstName":"Songxi","lastName":"Chen","creatorType":"author"},{"firstName":"Fengxiang","lastName":"Zhao","creatorType":"author"},{"firstName":"Yi","lastName":"Shang","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-26T03:06:56Z","dateModified":"2024-01-26T03:06:56Z","uri":"http://zotero.org/users/11367251/items/859XPF43","itemID":1851,"attachments":[{"key":"B5S8MIDG","version":2349,"itemType":"attachment","title":"guoDesignImplementationNew2021.pdf","parentItem":"859XPF43","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/guoDesignImplementationNew2021.pdf","tags":[],"relations":{},"dateAdded":"2024-01-26T03:07:17Z","dateModified":"2024-01-26T03:07:17Z","uri":"http://zotero.org/users/11367251/items/B5S8MIDG","localPath":"/Users/reyvababtista/Projects/Papers/guoDesignImplementationNew2021.pdf","defaultPath":"files/1853/guoDesignImplementationNew2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400352,"version":0},"$loki":61},{"itemID":270,"item":{"key":"87EWQQ2Y","version":263,"itemType":"journalArticle","title":"Mental Health and Behavior of College Students During the Early Phases of the COVID-19 Pandemic: Longitudinal Smartphone and Ecological Momentary Assessment Study","abstractNote":"Background: The vast majority of people worldwide have been impacted by coronavirus disease (COVID-19). In addition to the millions of individuals who have been infected with the disease, billions of individuals have been asked or required by local and national governments to change their behavioral patterns. Previous research on epidemics or traumatic events suggests that this can lead to profound behavioral and mental health changes; however, researchers are rarely able to track these changes with frequent, near-real-time sampling or compare their findings to previous years of data for the same individuals.","date":"2020-6-17","language":"en","shortTitle":"Mental Health and Behavior of College Students During the Early Phases of the COVID-19 Pandemic","libraryCatalog":"DOI.org (Crossref)","url":"http://www.jmir.org/2020/6/e20185/","accessDate":"2023-03-23T22:47:01Z","volume":"22","pages":"e20185","publicationTitle":"Journal of Medical Internet Research","DOI":"10.2196/20185","issue":"6","journalAbbreviation":"J Med Internet Res","ISSN":"1438-8871","creators":[{"firstName":"Jeremy F","lastName":"Huckins","creatorType":"author"},{"firstName":"Alex W","lastName":"daSilva","creatorType":"author"},{"firstName":"Weichen","lastName":"Wang","creatorType":"author"},{"firstName":"Elin","lastName":"Hedlund","creatorType":"author"},{"firstName":"Courtney","lastName":"Rogers","creatorType":"author"},{"firstName":"Subigya K","lastName":"Nepal","creatorType":"author"},{"firstName":"Jialing","lastName":"Wu","creatorType":"author"},{"firstName":"Mikio","lastName":"Obuchi","creatorType":"author"},{"firstName":"Eilis I","lastName":"Murphy","creatorType":"author"},{"firstName":"Meghan L","lastName":"Meyer","creatorType":"author"},{"firstName":"Dylan D","lastName":"Wagner","creatorType":"author"},{"firstName":"Paul E","lastName":"Holtzheimer","creatorType":"author"},{"firstName":"Andrew T","lastName":"Campbell","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-23T22:47:01Z","dateModified":"2023-03-23T22:47:01Z","uri":"http://zotero.org/users/11367251/items/87EWQQ2Y","itemID":270,"attachments":[{"key":"PDK4MKPX","version":265,"itemType":"attachment","title":"huckinsMentalHealthBehavior2020.pdf","parentItem":"87EWQQ2Y","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/huckinsMentalHealthBehavior2020.pdf","tags":[],"relations":{},"dateAdded":"2023-03-23T22:47:06Z","dateModified":"2023-03-23T22:47:06Z","uri":"http://zotero.org/users/11367251/items/PDK4MKPX","localPath":"/Users/reyvababtista/Projects/Papers/huckinsMentalHealthBehavior2020.pdf","defaultPath":"files/271/huckinsMentalHealthBehavior2020.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400353,"version":0},"$loki":62},{"itemID":1893,"item":{"key":"8AA2QYL6","version":2446,"itemType":"journalArticle","title":"The Use of Wearable ECG Devices in the Clinical Setting: a Review","date":"09/2022","language":"en","shortTitle":"The Use of Wearable ECG Devices in the Clinical Setting","libraryCatalog":"DOI.org (Crossref)","url":"https://link.springer.com/10.1007/s40138-022-00248-x","accessDate":"2024-02-06T22:30:23Z","volume":"10","pages":"67-72","publicationTitle":"Current Emergency and Hospital Medicine Reports","DOI":"10.1007/s40138-022-00248-x","issue":"3","journalAbbreviation":"Curr Emerg Hosp Med Rep","ISSN":"2167-4884","creators":[{"firstName":"Paola","lastName":"Kamga","creatorType":"author"},{"firstName":"Rasik","lastName":"Mostafa","creatorType":"author"},{"firstName":"Saba","lastName":"Zafar","creatorType":"author"}],"tags":[],"collections":["TAUT9NML"],"relations":{},"dateAdded":"2024-02-06T22:30:23Z","dateModified":"2024-02-06T22:30:23Z","uri":"http://zotero.org/users/11367251/items/8AA2QYL6","itemID":1893,"attachments":[{"key":"D9VUDHHW","version":2446,"itemType":"attachment","title":"kamgaUseWearableECG2022.pdf","parentItem":"8AA2QYL6","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kamgaUseWearableECG2022.pdf","tags":[],"relations":{},"dateAdded":"2024-02-06T22:30:26Z","dateModified":"2024-02-06T22:30:26Z","uri":"http://zotero.org/users/11367251/items/D9VUDHHW","localPath":"/Users/reyvababtista/Projects/Papers/kamgaUseWearableECG2022.pdf","defaultPath":"files/1895/kamgaUseWearableECG2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400354,"version":0},"$loki":63},{"itemID":1634,"item":{"key":"8AEK4ULM","version":2185,"itemType":"webpage","title":"Introducing Claude 2.1","date":"11/21/2023","url":"https://www.anthropic.com/index/claude-2-1","accessDate":"2023-11-29","websiteTitle":"Introducing Claude 2.1","creators":[{"name":"Anthropic","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T07:50:29Z","dateModified":"2023-12-08T17:18:33Z","uri":"http://zotero.org/users/11367251/items/8AEK4ULM","itemID":1634,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400354,"version":0},"$loki":64},{"itemID":406,"item":{"key":"8AIHQ57B","version":453,"itemType":"journalArticle","title":"An Introduction to Multisensor Data Fusion","date":"1997","language":"en","libraryCatalog":"Zotero","volume":"85","publicationTitle":"PROCEEDINGS OF THE IEEE","issue":"1","creators":[{"firstName":"David L","lastName":"Hall","creatorType":"author"},{"firstName":"James","lastName":"Llinas","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-04-18T03:16:06Z","dateModified":"2023-04-18T03:16:06Z","uri":"http://zotero.org/users/11367251/items/8AIHQ57B","itemID":406,"attachments":[{"key":"6LSPHD3U","version":456,"itemType":"attachment","title":"hallIntroductionMultisensorData1997.pdf","parentItem":"8AIHQ57B","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hallIntroductionMultisensorData1997.pdf","tags":[],"relations":{},"dateAdded":"2023-04-18T03:16:14Z","dateModified":"2023-04-18T03:16:14Z","uri":"http://zotero.org/users/11367251/items/6LSPHD3U","localPath":"/Users/reyvababtista/Projects/Papers/hallIntroductionMultisensorData1997.pdf","defaultPath":"files/407/hallIntroductionMultisensorData1997.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400357,"version":0},"$loki":65},{"itemID":553,"item":{"key":"8EZ3Z3PM","version":740,"itemType":"attachment","title":"6_25_23 Cannabis Machine Learning Manuscript-tim.pdf","linkMode":"imported_file","contentType":"application/pdf","charset":"","filename":"6_25_23 Cannabis Machine Learning Manuscript-tim.pdf","tags":[],"collections":[],"relations":{},"dateAdded":"2023-06-30T20:14:12Z","dateModified":"2023-06-30T20:14:12Z","uri":"http://zotero.org/users/11367251/items/8EZ3Z3PM","itemID":553,"localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/8EZ3Z3PM/6_25_23 Cannabis Machine Learning Manuscript-tim.pdf","defaultPath":"files/553/6_25_23 Cannabis Machine Learning Manuscript-tim.pdf"},"meta":{"revision":0,"created":1709832400358,"version":0},"$loki":66},{"itemID":936,"item":{"key":"8H9JQ7WQ","version":1258,"itemType":"blogPost","title":"How Google's Knowledge Graph works","date":"11/05/2023","url":"https://support.google.com/knowledgepanel/answer/9787176?hl=en","accessDate":"2023-11-05","blogTitle":"How Google's Knowledge Graph works","creators":[{"name":"Knowledge Panel Help","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T05:08:25Z","dateModified":"2023-11-06T05:08:58Z","uri":"http://zotero.org/users/11367251/items/8H9JQ7WQ","itemID":936,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400358,"version":0},"$loki":67},{"itemID":924,"item":{"key":"8LSDUCBB","version":1183,"itemType":"blogPost","title":"How AI powers great search results","date":"02/03/2022","url":"https://blog.google/products/search/how-ai-powers-great-search-results/","accessDate":"2023-11-04","blogTitle":"How AI powers great search results","creators":[{"name":"Pandu Nayak","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T23:07:46Z","dateModified":"2023-11-04T23:08:39Z","uri":"http://zotero.org/users/11367251/items/8LSDUCBB","itemID":924,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400358,"version":0},"$loki":68},{"itemID":1509,"item":{"key":"8MMHG6EL","version":1718,"itemType":"blogPost","title":"Google Turning Its Lucrative Web Search Over to AI Machines","date":"2015/10/26","url":"https://www.bloomberg.com/news/articles/2015-10-26/google-turning-its-lucrative-web-search-over-to-ai-machines","accessDate":"2023-11-09","blogTitle":"Google Turning Its Lucrative Web Search Over to AI Machines","creators":[{"name":"Jack Clark","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:30:36Z","dateModified":"2023-11-09T21:31:49Z","uri":"http://zotero.org/users/11367251/items/8MMHG6EL","itemID":1509,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400359,"version":0},"$loki":69},{"itemID":430,"item":{"key":"8PNAZPFH","version":530,"itemType":"webpage","title":"Optical Character Recognition (OCR)","date":"2023-05-04","url":"https://paperswithcode.com/task/optical-character-recognition","accessDate":"2023-05-04","websiteTitle":"Optical Character Recognition (OCR)","creators":[{"name":"Papers with Code","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T03:51:13Z","dateModified":"2023-05-05T03:51:56Z","uri":"http://zotero.org/users/11367251/items/8PNAZPFH","itemID":430,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400359,"version":0},"$loki":70},{"itemID":61,"item":{"key":"8QGCNSVS","version":201,"itemType":"journalArticle","title":"Context-Aware Block Net for Small Object Detection","abstractNote":"State-of-the-art object detectors usually progressively downsample the input image until it is represented by small feature maps, which loses the spatial information and compromises the representation of small objects. In this article, we propose a context-aware block net (CAB Net) to improve small object detection by building high-resolution and strong semantic feature maps. To internally enhance the representation capacity of feature maps with high spatial resolution, we delicately design the context-aware block (CAB). CAB exploits pyramidal dilated convolutions to incorporate multilevel contextual information without losing the original resolution of feature maps. Then, we assemble CAB to the end of the truncated backbone network (e.g., VGG16) with a relatively small downsampling factor (e.g., 8) and cast off all following layers. CAB Net can capture both basic visual patterns as well as semantical information of small objects, thus improving the performance of small object detection. Experiments conducted on the benchmark Tsinghua-Tencent 100K and the Airport dataset show that CAB Net outperforms other top-performing detectors by a large margin while keeping real-time speed, which demonstrates the effectiveness of CAB Net for small object detection.","date":"4/2022","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9151360/","accessDate":"2023-03-21T22:25:03Z","volume":"52","pages":"2300-2313","publicationTitle":"IEEE Transactions on Cybernetics","DOI":"10.1109/TCYB.2020.3004636","issue":"4","journalAbbreviation":"IEEE Trans. Cybern.","ISSN":"2168-2267, 2168-2275","creators":[{"firstName":"Lisha","lastName":"Cui","creatorType":"author"},{"firstName":"Pei","lastName":"Lv","creatorType":"author"},{"firstName":"Xiaoheng","lastName":"Jiang","creatorType":"author"},{"firstName":"Zhimin","lastName":"Gao","creatorType":"author"},{"firstName":"Bing","lastName":"Zhou","creatorType":"author"},{"firstName":"Luming","lastName":"Zhang","creatorType":"author"},{"firstName":"Ling","lastName":"Shao","creatorType":"author"},{"firstName":"Mingliang","lastName":"Xu","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/8JA7HWPZ"]},"dateAdded":"2023-03-21T22:25:03Z","dateModified":"2023-03-22T17:03:31Z","uri":"http://zotero.org/users/11367251/items/8QGCNSVS","itemID":61,"attachments":[{"key":"72ISGZPT","version":240,"itemType":"attachment","title":"cuiContextAwareBlockNet2022.pdf","parentItem":"8QGCNSVS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/cuiContextAwareBlockNet2022.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:51:59Z","dateModified":"2023-03-22T18:52:21Z","uri":"http://zotero.org/users/11367251/items/72ISGZPT","localPath":"/Users/reyvababtista/Projects/Papers/cuiContextAwareBlockNet2022.pdf","defaultPath":"files/240/cuiContextAwareBlockNet2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400359,"version":0},"$loki":71},{"itemID":422,"item":{"key":"8ZA8N7WS","version":504,"itemType":"webpage","title":"Python","date":"2023-05-04","url":"https://www.python.org","accessDate":"2023-05-04","websiteTitle":"Python","creators":[{"name":"Python Software Foundation","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T01:52:20Z","dateModified":"2023-05-05T01:53:29Z","uri":"http://zotero.org/users/11367251/items/8ZA8N7WS","itemID":422,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400359,"version":0},"$loki":72},{"itemID":1609,"item":{"key":"97L9R8MS","version":2191,"itemType":"webpage","title":"Apache Airflow","date":"11/28/2023","url":"https://airflow.apache.org/","accessDate":"2023-11-28","websiteTitle":"Apache Airflow","creators":[{"name":"The Apache Software Foundation","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T04:08:20Z","dateModified":"2023-12-08T17:19:11Z","uri":"http://zotero.org/users/11367251/items/97L9R8MS","itemID":1609,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400360,"version":0},"$loki":73},{"itemID":1145,"item":{"key":"99HJU6A6","version":1458,"itemType":"preprint","title":"TextCaps: a Dataset for Image Captioning with Reading Comprehension","abstractNote":"Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.","date":"2020-08-04","shortTitle":"TextCaps","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2003.12462","accessDate":"2023-11-08T16:03:32Z","extra":"arXiv:2003.12462 [cs]","repository":"arXiv","archiveID":"arXiv:2003.12462","creators":[{"firstName":"Oleksii","lastName":"Sidorov","creatorType":"author"},{"firstName":"Ronghang","lastName":"Hu","creatorType":"author"},{"firstName":"Marcus","lastName":"Rohrbach","creatorType":"author"},{"firstName":"Amanpreet","lastName":"Singh","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T16:03:32Z","dateModified":"2023-11-08T16:03:32Z","uri":"http://zotero.org/users/11367251/items/99HJU6A6","itemID":1145,"attachments":[{"key":"42P59LWU","version":1462,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2003.12462","accessDate":"2023-11-08T16:03:45Z","parentItem":"99HJU6A6","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2003.html","tags":[],"relations":{},"dateAdded":"2023-11-08T16:03:45Z","dateModified":"2023-11-08T16:03:45Z","uri":"http://zotero.org/users/11367251/items/42P59LWU","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/42P59LWU/2003.html","defaultPath":"files/1149/2003.html"},{"key":"34BJ5YD8","version":1459,"itemType":"attachment","title":"sidorovTextCapsDatasetImage2020.pdf","parentItem":"99HJU6A6","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/sidorovTextCapsDatasetImage2020.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T16:03:40Z","dateModified":"2023-11-08T16:03:40Z","uri":"http://zotero.org/users/11367251/items/34BJ5YD8","localPath":"/Users/reyvababtista/Projects/Papers/sidorovTextCapsDatasetImage2020.pdf","defaultPath":"files/1148/sidorovTextCapsDatasetImage2020.pdf"}],"notes":[{"key":"AYA2W8SY","version":1458,"itemType":"note","parentItem":"99HJU6A6","note":"Comment: To appear in ECCV 2020 (oral) Project page: https://textvqa.org/textcaps","tags":[],"relations":{},"dateAdded":"2023-11-08T16:03:32Z","dateModified":"2023-11-08T16:03:32Z","uri":"http://zotero.org/users/11367251/items/AYA2W8SY"}]},"meta":{"revision":0,"created":1709832400360,"version":0},"$loki":74},{"itemID":1118,"item":{"key":"9AVHRWXU","version":1424,"itemType":"preprint","title":"Deep Visual-Semantic Alignments for Generating Image Descriptions","abstractNote":"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions signiﬁcantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.","date":"2015-04-14","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1412.2306","accessDate":"2023-11-08T15:33:45Z","extra":"arXiv:1412.2306 [cs]","repository":"arXiv","archiveID":"arXiv:1412.2306","creators":[{"firstName":"Andrej","lastName":"Karpathy","creatorType":"author"},{"firstName":"Li","lastName":"Fei-Fei","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T15:33:45Z","dateModified":"2023-11-08T15:33:46Z","uri":"http://zotero.org/users/11367251/items/9AVHRWXU","itemID":1118,"attachments":[{"key":"H8RJCU3U","version":1424,"itemType":"attachment","title":"karpathyDeepVisualSemanticAlignments2015.pdf","parentItem":"9AVHRWXU","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/karpathyDeepVisualSemanticAlignments2015.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T15:33:47Z","dateModified":"2023-11-08T15:33:47Z","uri":"http://zotero.org/users/11367251/items/H8RJCU3U","localPath":"/Users/reyvababtista/Projects/Papers/karpathyDeepVisualSemanticAlignments2015.pdf","defaultPath":"files/1119/karpathyDeepVisualSemanticAlignments2015.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400361,"version":0},"$loki":75},{"itemID":360,"item":{"key":"9CJ3NQ5U","version":399,"itemType":"preprint","title":"A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond","abstractNote":"YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO’s evolution, examining the innovations and contributions in each iteration from the original YOLO to YOLOv8. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO’s development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.","date":"2023-04-02","language":"en","shortTitle":"A Comprehensive Review of YOLO","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2304.00501","accessDate":"2023-04-10T22:30:59Z","extra":"arXiv:2304.00501 [cs]","repository":"arXiv","archiveID":"arXiv:2304.00501","creators":[{"firstName":"Juan","lastName":"Terven","creatorType":"author"},{"firstName":"Diana","lastName":"Cordova-Esparza","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-10T22:30:59Z","dateModified":"2023-04-10T22:30:59Z","uri":"http://zotero.org/users/11367251/items/9CJ3NQ5U","itemID":360,"attachments":[{"key":"C6KLIPFT","version":401,"itemType":"attachment","title":"tervenComprehensiveReviewYOLO2023.pdf","parentItem":"9CJ3NQ5U","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/tervenComprehensiveReviewYOLO2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_C6KLIPFT/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/2\">2 YOLO Applications Across Diverse Fields</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/2\">3 Object Detection Metrics and Non-Maximum Suppression (NMS)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/3\">3.1 How AP works?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/3\">3.2 Computing AP</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/4\">3.3 Non-Maximum Suppression (NMS)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/4\">4 YOLO: You Only Look Once</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/5\">4.1 How YOLOv1 works?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/5\">4.2 YOLOv1 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/5\">4.3 YOLOv1 Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/6\">4.4 YOLOv1 Strengths and Limitations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/7\">5 YOLOv2: Better, Faster, and Stronger</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/8\">5.1 YOLOv2 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/9\">5.2 YOLO9000 is a stronger YOLOv2</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/9\">6 YOLOv3</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/10\">6.1 YOLOv3 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/11\">6.2 YOLOv3 Multi-Scale Predictions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/11\">6.3 YOLOv3 Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/12\">7 Backbone, Neck, and Head</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/12\">8 YOLOv4</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/13\">8.1 YOLOv4 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/14\">9 YOLOv5</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/14\">9.1 YOLOv5 Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/15\">10 Scaled-YOLOv4</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/15\">11 YOLOR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/15\">12 YOLOX</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/16\">13 YOLOv6</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/17\">13.1 YOLOv6 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/17\">14 YOLOv7</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/17\">14.1 Comparison with YOLOv4 and YOLOR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/18\">14.2 YOLOv7 Results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/18\">15 DAMO-YOLO</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/18\">16 YOLOv8</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/18\">16.1 YOLOv8 Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/18\">17 PP-YOLO, PP-YOLOv2, and PP-YOLOE</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/19\">17.1 PP-YOLO augmentations and preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/19\">17.2 PP-YOLO results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/19\">17.3 PP-YOLOv2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/20\">17.4 PP-YOLOE</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/20\">18 Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/21\">18.1 Tradeoff between speed and accuracy</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/21\">19 The future of YOLO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_C6KLIPFT/22\">20 Acknowledgments</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-10T22:31:20Z","dateModified":"2023-04-10T22:31:20Z","uri":"http://zotero.org/users/11367251/items/C6KLIPFT","localPath":"/Users/reyvababtista/Projects/Papers/tervenComprehensiveReviewYOLO2023.pdf","defaultPath":"files/362/tervenComprehensiveReviewYOLO2023.pdf"}],"notes":[{"key":"9S7C5PMN","version":399,"itemType":"note","parentItem":"9CJ3NQ5U","note":"Comment: 27 pages, 12 figures, 4 tables, submitted to ACM Computing Surveys","tags":[],"relations":{},"dateAdded":"2023-04-10T22:30:59Z","dateModified":"2023-04-10T22:30:59Z","uri":"http://zotero.org/users/11367251/items/9S7C5PMN"}]},"meta":{"revision":0,"created":1709832400361,"version":0},"$loki":76},{"itemID":1418,"item":{"key":"9EXSKL5I","version":1555,"itemType":"conferencePaper","title":"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language","date":"6/2016","shortTitle":"MSR-VTT","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/7780940/","accessDate":"2023-11-08T22:43:47Z","place":"Las Vegas, NV, USA","publisher":"IEEE","ISBN":"978-1-4673-8851-1","pages":"5288-5296","proceedingsTitle":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2016.571","creators":[{"firstName":"Jun","lastName":"Xu","creatorType":"author"},{"firstName":"Tao","lastName":"Mei","creatorType":"author"},{"firstName":"Ting","lastName":"Yao","creatorType":"author"},{"firstName":"Yong","lastName":"Rui","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:43:47Z","dateModified":"2023-11-08T22:43:47Z","uri":"http://zotero.org/users/11367251/items/9EXSKL5I","itemID":1418,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400362,"version":0},"$loki":77},{"itemID":1627,"item":{"key":"9KIDRVWK","version":1989,"itemType":"journalArticle","title":"Visualizing data using t-SNE","date":"2008","url":"http://jmlr.org/papers/v9/vandermaaten08a.html","extra":"Citation Key: JMLR:v9:vandermaaten08a","volume":"9","pages":"2579–2605","publicationTitle":"Journal of Machine Learning Research","issue":"86","creators":[{"firstName":"Laurens","lastName":"van der Maaten","creatorType":"author"},{"firstName":"Geoffrey","lastName":"Hinton","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T05:07:42Z","dateModified":"2023-11-29T05:07:42Z","uri":"http://zotero.org/users/11367251/items/9KIDRVWK","itemID":1627,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400363,"version":0},"$loki":78},{"itemID":917,"item":{"key":"9M899L3M","version":1161,"itemType":"blogPost","title":"RankBrain, BERT, MUM — Evolution of Google`s Core Algorithm","date":"07/07/2022","url":"https://huskyhamster.com/blog/13/rankbrain-bert-mum-evolution-of-googles-core-algorithm","accessDate":"2023-11-04","blogTitle":"RankBrain, BERT, MUM — Evolution of Google`s Core Algorithm","creators":[{"name":"Przemyslaw Puternicki","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T21:47:02Z","dateModified":"2023-11-04T21:59:19Z","uri":"http://zotero.org/users/11367251/items/9M899L3M","itemID":917,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400363,"version":0},"$loki":79},{"itemID":505,"item":{"key":"9N4KNYUW","version":655,"itemType":"journalArticle","title":"Incorporating ChatGPT into a Financial Data Science Course with Python Programming","abstractNote":"ChatGPT’s artificial intelligence (AI) has generated considerable debate and even panic in higher education. However, AI is an unavoidable trend in education that has the potential to subvert many courses’ pedagogical structure and methods. One such course is financial data science with Python programming, a course that is in high demand among finance majors. Incorporating ChatGPT into such a course can solve longstanding challenges and difficulties that instructors and students are facing, and it can shift the focus of the course from tedious Python coding back to financial data interpretations and real-world applications. This paper represents a first attempt at welcoming ChatGPT into financial education, by focusing on the pedagogical innovation of a financial data science course with Python programming. In particular, the paper provides specific examples of potential projects, with other insights and suggestions for using ChatGPT in data science courses for finance majors.","date":"2023","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.ssrn.com/abstract=4412371","accessDate":"2023-06-16T13:29:50Z","publicationTitle":"SSRN Electronic Journal","DOI":"10.2139/ssrn.4412371","journalAbbreviation":"SSRN Journal","ISSN":"1556-5068","creators":[{"firstName":"Yang","lastName":"Liu","creatorType":"author"},{"firstName":"Laura","lastName":"Miller","creatorType":"author"},{"firstName":"Xu","lastName":"Niu","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-16T13:29:50Z","dateModified":"2023-06-16T13:29:50Z","uri":"http://zotero.org/users/11367251/items/9N4KNYUW","itemID":505,"attachments":[{"key":"MZHY9X25","version":658,"itemType":"attachment","title":"liuIncorporatingChatGPTFinancial2023.pdf","parentItem":"9N4KNYUW","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liuIncorporatingChatGPTFinancial2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MZHY9X25/3\">1. Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/6\">2. Literature Review</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/8\">3. Examples of Python Projects with ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/9\">3.1 Example Project 1: Monte Carlo Simulation and Optimal Portfolio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/17\">3.2 Example Project 2: Regression Analysis and Fama-French Three-Factor Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/23\">3.3 Other Potential Projects for Financial Data Science with ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/27\">4. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/28\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MZHY9X25/30\">Appendix A.  Python Codes Generated by ChatGPT</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-16T13:30:00Z","dateModified":"2023-06-16T13:30:00Z","uri":"http://zotero.org/users/11367251/items/MZHY9X25","localPath":"/Users/reyvababtista/Projects/Papers/liuIncorporatingChatGPTFinancial2023.pdf","defaultPath":"files/506/liuIncorporatingChatGPTFinancial2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400366,"version":0},"$loki":80},{"itemID":951,"item":{"key":"9V3IG9ZR","version":1315,"itemType":"blogPost","title":"Where do DuckDuckGo search results come from?","date":"2023","url":"https://duckduckgo.com/duckduckgo-help-pages/results/sources/","accessDate":"2023-11-06","blogTitle":"Where do DuckDuckGo search results come from?","creators":[{"name":"DuckDuckGo","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:15:16Z","dateModified":"2023-11-07T02:15:50Z","uri":"http://zotero.org/users/11367251/items/9V3IG9ZR","itemID":951,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400366,"version":0},"$loki":81},{"itemID":1792,"item":{"key":"9WS6H9WW","version":2232,"itemType":"preprint","title":"Precise Zero-Shot Dense Retrieval without Relevance Labels","abstractNote":"While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder~(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages~(e.g. sw, ko, ja).","date":"2022-12-20","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2212.10496","accessDate":"2024-01-18T20:20:54Z","extra":"arXiv:2212.10496 [cs]","repository":"arXiv","archiveID":"arXiv:2212.10496","creators":[{"firstName":"Luyu","lastName":"Gao","creatorType":"author"},{"firstName":"Xueguang","lastName":"Ma","creatorType":"author"},{"firstName":"Jimmy","lastName":"Lin","creatorType":"author"},{"firstName":"Jamie","lastName":"Callan","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Information Retrieval","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2024-01-18T20:20:54Z","dateModified":"2024-01-18T20:20:54Z","uri":"http://zotero.org/users/11367251/items/9WS6H9WW","itemID":1792,"attachments":[{"key":"HE9R6IQS","version":2235,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2212.10496","accessDate":"2024-01-18T20:21:02Z","parentItem":"9WS6H9WW","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2212.html","tags":[],"relations":{},"dateAdded":"2024-01-18T20:21:02Z","dateModified":"2024-01-18T20:21:02Z","uri":"http://zotero.org/users/11367251/items/HE9R6IQS","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/HE9R6IQS/2212.html","defaultPath":"files/1795/2212.html"},{"key":"6QSQC5FA","version":2232,"itemType":"attachment","title":"gaoPreciseZeroShotDense2022.pdf","parentItem":"9WS6H9WW","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gaoPreciseZeroShotDense2022.pdf","tags":[],"relations":{},"dateAdded":"2024-01-18T20:20:56Z","dateModified":"2024-01-18T20:20:56Z","uri":"http://zotero.org/users/11367251/items/6QSQC5FA","localPath":"/Users/reyvababtista/Projects/Papers/gaoPreciseZeroShotDense2022.pdf","defaultPath":"files/1794/gaoPreciseZeroShotDense2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400367,"version":0},"$loki":82},{"itemID":274,"item":{"key":"9XRBBMCH","version":268,"itemType":"journalArticle","title":"Ecological Momentary Assessment","abstractNote":"Assessment in clinical psychology typically relies on global retrospective self-reports collected at research or clinic visits, which are limited by recall bias and are not well suited to address how behavior changes over time and across contexts. Ecological momentary assessment (EMA) involves repeated sampling of subjects’ current behaviors and experiences in real time, in subjects’ natural environments. EMA aims to minimize recall bias, maximize ecological validity, and allow study of microprocesses that inﬂuence behavior in real-world contexts. EMA studies assess particular events in subjects’ lives or assess subjects at periodic intervals, often by random time sampling, using technologies ranging from written diaries and telephones to electronic diaries and physiological sensors. We discuss the rationale for EMA, EMA designs, methodological and practical issues, and comparisons of EMA and recall data. EMA holds unique promise to advance the science and practice of clinical psychology by shedding light on the dynamics of behavior in real-world settings.","date":"2008-04-01","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.annualreviews.org/doi/10.1146/annurev.clinpsy.3.022806.091415","accessDate":"2023-03-24T01:11:51Z","volume":"4","pages":"1-32","publicationTitle":"Annual Review of Clinical Psychology","DOI":"10.1146/annurev.clinpsy.3.022806.091415","issue":"1","journalAbbreviation":"Annu. Rev. Clin. Psychol.","ISSN":"1548-5943, 1548-5951","creators":[{"firstName":"Saul","lastName":"Shiffman","creatorType":"author"},{"firstName":"Arthur A.","lastName":"Stone","creatorType":"author"},{"firstName":"Michael R.","lastName":"Hufford","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-24T01:11:51Z","dateModified":"2023-03-24T01:11:51Z","uri":"http://zotero.org/users/11367251/items/9XRBBMCH","itemID":274,"attachments":[{"key":"HRIGA5NG","version":271,"itemType":"attachment","title":"shiffmanEcologicalMomentaryAssessment2008.pdf","parentItem":"9XRBBMCH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/shiffmanEcologicalMomentaryAssessment2008.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:8px\"><ul style=\"list-style-type: none; padding-left:0px\"><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li><li style=\"padding-top:4px\"></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-24T01:16:45Z","dateModified":"2023-03-24T01:16:45Z","uri":"http://zotero.org/users/11367251/items/HRIGA5NG","localPath":"/Users/reyvababtista/Projects/Papers/shiffmanEcologicalMomentaryAssessment2008.pdf","defaultPath":"files/275/shiffmanEcologicalMomentaryAssessment2008.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400368,"version":0},"$loki":83},{"itemID":1461,"item":{"key":"9YR3IYRE","version":1609,"itemType":"preprint","title":"Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset","abstractNote":"Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (~0.1 ms to ~100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (~3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.","date":"2019-01-17","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1810.12247","accessDate":"2023-11-08T22:58:38Z","extra":"arXiv:1810.12247 [cs, eess, stat]","repository":"arXiv","archiveID":"arXiv:1810.12247","creators":[{"firstName":"Curtis","lastName":"Hawthorne","creatorType":"author"},{"firstName":"Andriy","lastName":"Stasyuk","creatorType":"author"},{"firstName":"Adam","lastName":"Roberts","creatorType":"author"},{"firstName":"Ian","lastName":"Simon","creatorType":"author"},{"firstName":"Cheng-Zhi Anna","lastName":"Huang","creatorType":"author"},{"firstName":"Sander","lastName":"Dieleman","creatorType":"author"},{"firstName":"Erich","lastName":"Elsen","creatorType":"author"},{"firstName":"Jesse","lastName":"Engel","creatorType":"author"},{"firstName":"Douglas","lastName":"Eck","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:58:38Z","dateModified":"2023-11-08T22:58:38Z","uri":"http://zotero.org/users/11367251/items/9YR3IYRE","itemID":1461,"attachments":[{"key":"DB8TCKAV","version":1611,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1810.12247","accessDate":"2023-11-08T22:58:46Z","parentItem":"9YR3IYRE","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1810.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:58:46Z","dateModified":"2023-11-08T22:58:46Z","uri":"http://zotero.org/users/11367251/items/DB8TCKAV","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/DB8TCKAV/1810.html","defaultPath":"files/1465/1810.html"},{"key":"M8QIUEYP","version":1609,"itemType":"attachment","title":"hawthorneEnablingFactorizedPiano2019.pdf","parentItem":"9YR3IYRE","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hawthorneEnablingFactorizedPiano2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_M8QIUEYP/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/3\">2 Contributions of this paper</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/3\">3 Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/4\">3.1 Alignment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/4\">3.2 Dataset splitting</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/5\">4 Piano Transcription</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/6\">5 Music Transformer Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/6\">6 Piano Synthesis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/7\">7 Listening Tests</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_M8QIUEYP/8\">8 Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-08T22:58:40Z","dateModified":"2023-11-08T22:58:42Z","uri":"http://zotero.org/users/11367251/items/M8QIUEYP","localPath":"/Users/reyvababtista/Projects/Papers/hawthorneEnablingFactorizedPiano2019.pdf","defaultPath":"files/1464/hawthorneEnablingFactorizedPiano2019.pdf"}],"notes":[{"key":"NUCSYKHL","version":1609,"itemType":"note","parentItem":"9YR3IYRE","note":"Comment: Examples available at https://goo.gl/magenta/maestro-examples","tags":[],"relations":{},"dateAdded":"2023-11-08T22:58:38Z","dateModified":"2023-11-08T22:58:38Z","uri":"http://zotero.org/users/11367251/items/NUCSYKHL"}]},"meta":{"revision":0,"created":1709832400384,"version":0},"$loki":84},{"itemID":540,"item":{"key":"9ZJH6J4Z","version":711,"itemType":"preprint","title":"Augmented Language Models: a Survey","abstractNote":"This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is deﬁned as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.","date":"2023-02-15","language":"en","shortTitle":"Augmented Language Models","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2302.07842","accessDate":"2023-06-29T21:59:53Z","extra":"arXiv:2302.07842 [cs]","repository":"arXiv","archiveID":"arXiv:2302.07842","creators":[{"firstName":"Grégoire","lastName":"Mialon","creatorType":"author"},{"firstName":"Roberto","lastName":"Dessì","creatorType":"author"},{"firstName":"Maria","lastName":"Lomeli","creatorType":"author"},{"firstName":"Christoforos","lastName":"Nalmpantis","creatorType":"author"},{"firstName":"Ram","lastName":"Pasunuru","creatorType":"author"},{"firstName":"Roberta","lastName":"Raileanu","creatorType":"author"},{"firstName":"Baptiste","lastName":"Rozière","creatorType":"author"},{"firstName":"Timo","lastName":"Schick","creatorType":"author"},{"firstName":"Jane","lastName":"Dwivedi-Yu","creatorType":"author"},{"firstName":"Asli","lastName":"Celikyilmaz","creatorType":"author"},{"firstName":"Edouard","lastName":"Grave","creatorType":"author"},{"firstName":"Yann","lastName":"LeCun","creatorType":"author"},{"firstName":"Thomas","lastName":"Scialom","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-29T21:59:53Z","dateModified":"2023-06-29T21:59:53Z","uri":"http://zotero.org/users/11367251/items/9ZJH6J4Z","itemID":540,"attachments":[{"key":"HWZLIU3X","version":712,"itemType":"attachment","title":"mialonAugmentedLanguageModels2023.pdf","parentItem":"9ZJH6J4Z","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/mialonAugmentedLanguageModels2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_HWZLIU3X/2\">1 Introduction: motivation for the survey and definitions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/2\">1.1 Motivation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/4\">1.2 Our classification</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/4\">2 Reasoning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/4\">2.1 Eliciting reasoning with prompting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/6\">2.2 Recursive prompting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/6\">2.3 Explicitly teaching language models to reason</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/8\">2.4 Comparison and limitations of abstract reasoning</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/9\">3 Using Tools and Act</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/9\">3.1 Calling another model</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/11\">3.2 Information retrieval</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/11\">3.2.1 Retrieval-augmented language models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/12\">3.2.2 Querying search engines</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/12\">3.2.3 Searching and navigating the web</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/13\">3.3 Computing via Symbolic Modules and Code Interpreters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/13\">3.4 Acting on the virtual and physical world</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/15\">4 Learning to reason, use tools, and act</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/15\">4.1 Supervision</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/16\">4.2 Reinforcement learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/18\">4.3 Limitations and future directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/19\">5 Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HWZLIU3X/21\">6 Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-29T21:59:57Z","dateModified":"2023-06-29T21:59:57Z","uri":"http://zotero.org/users/11367251/items/HWZLIU3X","localPath":"/Users/reyvababtista/Projects/Papers/mialonAugmentedLanguageModels2023.pdf","defaultPath":"files/541/mialonAugmentedLanguageModels2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400385,"version":0},"$loki":85},{"itemID":717,"item":{"key":"A3BR5CFM","version":953,"itemType":"conferencePaper","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning","abstractNote":"The limited availability of annotated data often hinders real-world applications of machine learning. To efficiently learn from small quantities of multimodal data, we leverage the linguistic knowledge from a large pre-trained language model (PLM) and quickly adapt it to new domains of image captioning. To effectively utilize a pretrained model, it is critical to balance the visual input and prior linguistic knowledge from pretraining. We propose VisualGPT, which employs a novel self-resurrecting encoderdecoder attention mechanism to quickly adapt the PLM with a small amount of in-domain image-text data. The proposed self-resurrecting activation unit produces sparse activations that prevent accidental overwriting of linguistic knowledge. When trained on 0.1%, 0.5% and 1% of the respective training sets, VisualGPT surpasses the best baseline by up to 10.0% CIDEr on MS COCO [45] and 17.9% CIDEr on Conceptual Captions [69]. Furthermore, VisualGPT achieves the state-of-the-art result on IU X-ray [15], a medical report generation dataset. Our code is available at https://github.com/Vision-CAIR/ VisualGPT.","date":"6/2022","language":"en","shortTitle":"VisualGPT","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9879874/","accessDate":"2023-09-21T21:52:28Z","place":"New Orleans, LA, USA","publisher":"IEEE","ISBN":"978-1-66546-946-3","pages":"18009-18019","proceedingsTitle":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52688.2022.01750","creators":[{"firstName":"Jun","lastName":"Chen","creatorType":"author"},{"firstName":"Han","lastName":"Guo","creatorType":"author"},{"firstName":"Kai","lastName":"Yi","creatorType":"author"},{"firstName":"Boyang","lastName":"Li","creatorType":"author"},{"firstName":"Mohamed","lastName":"Elhoseiny","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-21T21:52:28Z","dateModified":"2023-09-21T21:52:28Z","uri":"http://zotero.org/users/11367251/items/A3BR5CFM","itemID":717,"attachments":[{"key":"I2W3RUE3","version":956,"itemType":"attachment","title":"chenVisualGPTDataefficientAdaptation2022.pdf","parentItem":"A3BR5CFM","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenVisualGPTDataefficientAdaptation2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_I2W3RUE3/1\">1 . Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/2\">2 . Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/3\">3 . Preliminaries: Transformer for Captioning</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/3\">4 . VisualGPT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/3\">4.1 . Self-Resurrecting Activation Unit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/4\">4.2 . The Architecture and Training of VisualGPT</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/4\">5 . Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/4\">5.1 . Datasets and Evaluation Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/5\">5.2 . Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/6\">5.3 . Quantitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/6\">5.4 . Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/7\">5.5 . Human Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/8\">5.6 . Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/8\">5.7 . Limitation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/8\">6 . Conclusions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/12\">A . Supplementary material</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/12\">A.1 . Additional implementation details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/12\">A.2 . Train VisualGPT with more COCO and Conceptual Caption Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/13\">A.3 . Attention over Different types of words</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I2W3RUE3/13\">A.4 . More Qualitative Examples</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-21T21:52:35Z","dateModified":"2023-09-21T21:52:35Z","uri":"http://zotero.org/users/11367251/items/I2W3RUE3","localPath":"/Users/reyvababtista/Projects/Papers/chenVisualGPTDataefficientAdaptation2022.pdf","defaultPath":"files/718/chenVisualGPTDataefficientAdaptation2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400386,"version":0},"$loki":86},{"itemID":28,"item":{"key":"ANCSXKDX","version":200,"itemType":"journalArticle","title":"Real-Time Embedded Traffic Sign Recognition Using Efficient Convolutional Neural Network","abstractNote":"Trafﬁc sign recognition(TSR) based on deep learning is rapidly developing. Speciﬁcally, TSR contains two technologies, namely, trafﬁc sign classiﬁcation (TSC) and trafﬁc sign detection (TSD). However, the challenge of TSR is to ensure its efﬁciency, which means adequate accuracy, generalization, and speed in real-time by a computationally limited platform. In this paper, we will introduce a new efﬁcient TSC network called ENet (efﬁcient network) and a TSD network called EmdNet (efﬁcient network using multiscale operation and depthwise separable convolution). We used data mining and multiscale operation to improve the accuracy and generalization ability and used depthwise separable convolution (DSC) to improve the speed. The resulting ENet possesses 0.9 M parameters (1/15 the parameters of the start-of-the-art method) while still achieving an accuracy of 98.6 % on the German Trafﬁc Sign Recognition benchmark (GTSRB). In addition, we design EmdNet’ s backbone network according to the principles of ENet. The EmdNet with the SDD Framework possesses only 6.3 M parameters, which is similar to MobileNet’s scale.","date":"2019","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8698449/","accessDate":"2023-03-20T14:25:14Z","volume":"7","pages":"53330-53346","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2019.2912311","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Xie","lastName":"Bangquan","creatorType":"author"},{"firstName":"Weng","lastName":"Xiao Xiong","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/WPNC3DG6"]},"dateAdded":"2023-03-20T14:25:14Z","dateModified":"2023-03-22T17:03:43Z","uri":"http://zotero.org/users/11367251/items/ANCSXKDX","itemID":28,"attachments":[{"key":"UQRZIY5C","version":241,"itemType":"attachment","title":"bangquanRealTimeEmbeddedTraffic2019.pdf","parentItem":"ANCSXKDX","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bangquanRealTimeEmbeddedTraffic2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UQRZIY5C/1\">INTRODUCTION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/2\">RELATED WORD</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/2\">OBJECT CLASSIFICATION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/3\">OBJECT DETECTION</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/4\">DATASETS AND SETUP</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/4\">DATASETS TO TRAIN TSC</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/4\">DATASETS TO TRAIN TSD</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/4\">SETUP</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/5\">METHODOLOGY</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/5\">METHODOLOGY OF TSC</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/5\">DATA MINING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/7\">THE CONSTRUCTION PROCESS OF ENET</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/13\">METHODOLOGY OF TSD</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/13\">DESIGN PRINCIPLE OF EMDNET</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/13\">THE STRUCTURE OF EMDNET</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/15\">DISCUSSIONS AND EVALUATION</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/15\">DISCUSSIONS AND EVALUATION OF TSC</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/15\">DISCUSSION OF ENET</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/15\">TESTING USING NEW DATASET</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/15\">DISCUSSIONS OF TSD</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/15\">THE RESULT OF EMDNET</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/16\">CONCLUSIONS AND FUTURE WORKS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/16\">REFERENCES</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/17\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/17\">XIE BANGQUAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UQRZIY5C/17\">WENG XIAO XIONG</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:06Z","dateModified":"2023-03-22T18:52:24Z","uri":"http://zotero.org/users/11367251/items/UQRZIY5C","localPath":"/Users/reyvababtista/Projects/Papers/bangquanRealTimeEmbeddedTraffic2019.pdf","defaultPath":"files/253/bangquanRealTimeEmbeddedTraffic2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400387,"version":0},"$loki":87},{"itemID":758,"item":{"key":"ATJ8W27C","version":1021,"itemType":"journalArticle","title":"Predicting quantity of cannabis smoked in daily life: An exploratory study using machine learning","abstractNote":"Background: Cannabis use is prevalent in the United States and is associated with a host of negative consequences. Importantly, a robust indicator of negative consequences is the amount of cannabis consumed.\nMethods: Data were obtained from fifty-two adult, regular cannabis flower users (3+ times per week) recruited from the community; participants completed multiple ecological momentary assessment (EMA) surveys each day for 14 days. In this exploratory study, we used various machine learning algorithms to build models to predict the amount of cannabis smoked since participants’ last report including forty-three EMA measures of mood, impulsivity, pain, alcohol use, cigarette use, craving, cannabis potency, cannabis use motivation, subjective effects of cannabis, social context, and location in daily life.\nResults: Our best-fitting model (Gradient Boosted Trees; 71.15% accuracy, 72.46% precision) found that affects, subjective effects of cannabis, and cannabis use motives were among the best predictors of cannabis use amount in daily life. The social context of being with others, and particularly with a partner or friend, was moderately weighted in the final prediction model, but contextual items reflecting location were not strongly weighted in the final prediction model, the one exception being not at work.\nConclusions: Machine learning approaches can help identify additional environmental and psychological phe­ nomena that may be clinically-relevant to cannabis use.","date":"11/2023","language":"en","shortTitle":"Predicting quantity of cannabis smoked in daily life","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S0376871623012024","accessDate":"2023-10-06T01:38:30Z","volume":"252","pages":"110964","publicationTitle":"Drug and Alcohol Dependence","DOI":"10.1016/j.drugalcdep.2023.110964","journalAbbreviation":"Drug and Alcohol Dependence","ISSN":"03768716","creators":[{"firstName":"Ching-Yun","lastName":"Yu","creatorType":"author"},{"firstName":"Yi","lastName":"Shang","creatorType":"author"},{"firstName":"Tionna M.","lastName":"Hough","creatorType":"author"},{"firstName":"Anthony L.","lastName":"Bokshan","creatorType":"author"},{"firstName":"Megan N.","lastName":"Fleming","creatorType":"author"},{"firstName":"Alison M.","lastName":"Haney","creatorType":"author"},{"firstName":"Timothy J.","lastName":"Trull","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-10-06T01:38:30Z","dateModified":"2023-10-06T01:38:30Z","uri":"http://zotero.org/users/11367251/items/ATJ8W27C","itemID":758,"attachments":[{"key":"YHDKSCGT","version":1023,"itemType":"attachment","title":"yuPredictingquantitycannabis2023.pdf","parentItem":"ATJ8W27C","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/yuPredictingquantitycannabis2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_YHDKSCGT/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.1 Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.2 Procedure</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.3 EMA measures</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.3.1 Affect</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.3.2 Impulsivity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.3.3 Alcohol and tobacco use</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/2\">2.3.4 Cannabis use</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">2.3.5 Motives for cannabis use</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">2.3.6 Cravings for cannabis use</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">2.3.7 Subjective effects of cannabis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">2.3.8 Pain</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">2.3.9 Location</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">2.3.10 Social contexts</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">3 Analytic plan</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/3\">3.1 Model training steps</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/4\">4 Results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/4\">5 Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/5\">5.1 Limitations and implications</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/6\">6 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/6\">Role of funding source</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/6\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/6\">Declaration of Competing Interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_YHDKSCGT/6\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-10-06T01:38:36Z","dateModified":"2023-10-06T01:38:37Z","uri":"http://zotero.org/users/11367251/items/YHDKSCGT","localPath":"/Users/reyvababtista/Projects/Papers/yuPredictingquantitycannabis2023.pdf","defaultPath":"files/759/yuPredictingquantitycannabis2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400389,"version":0},"$loki":88},{"itemID":409,"item":{"key":"AVINHQP3","version":460,"itemType":"preprint","title":"Squeeze-and-Excitation Networks","abstractNote":"The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive ﬁelds at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring signiﬁcant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classiﬁcation submission which won ﬁrst place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%. Models and code are available at https://github.com/hujie-frank/SENet.","date":"2019-05-16","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1709.01507","accessDate":"2023-04-25T21:59:17Z","extra":"arXiv:1709.01507 [cs]","repository":"arXiv","archiveID":"arXiv:1709.01507","creators":[{"firstName":"Jie","lastName":"Hu","creatorType":"author"},{"firstName":"Li","lastName":"Shen","creatorType":"author"},{"firstName":"Samuel","lastName":"Albanie","creatorType":"author"},{"firstName":"Gang","lastName":"Sun","creatorType":"author"},{"firstName":"Enhua","lastName":"Wu","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-25T21:59:17Z","dateModified":"2023-04-25T21:59:17Z","uri":"http://zotero.org/users/11367251/items/AVINHQP3","itemID":409,"attachments":[{"key":"KA3ZN74A","version":463,"itemType":"attachment","title":"huSqueezeandExcitationNetworks2019.pdf","parentItem":"AVINHQP3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/huSqueezeandExcitationNetworks2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_KA3ZN74A/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/3\">3 Squeeze-and-Excitation Blocks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/3\">3.1 Squeeze: Global Information Embedding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/3\">3.2 Excitation: Adaptive Recalibration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/4\">3.3 Instantiations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/4\">4 Model and Computational Complexity</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/5\">5 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/5\">5.1 Image Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/7\">5.2 Scene Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/7\">5.3 Object Detection on COCO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/7\">5.4 ILSVRC 2017 Classification Competition</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/8\">6 Ablation Study</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/8\">6.1 Reduction ratio</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/8\">6.2 Squeeze Operator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/8\">6.3 Excitation Operator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/9\">6.4 Different stages</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/9\">6.5 Integration strategy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/9\">7 Role of SE blocks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/9\">7.1 Effect of Squeeze</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/10\">7.2 Role of Excitation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/11\">8 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_KA3ZN74A/12\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-25T21:59:32Z","dateModified":"2023-04-25T21:59:33Z","uri":"http://zotero.org/users/11367251/items/KA3ZN74A","localPath":"/Users/reyvababtista/Projects/Papers/huSqueezeandExcitationNetworks2019.pdf","defaultPath":"files/411/huSqueezeandExcitationNetworks2019.pdf"}],"notes":[{"key":"CS84BFPP","version":460,"itemType":"note","parentItem":"AVINHQP3","note":"Comment: journal version of the CVPR 2018 paper, accepted by TPAMI","tags":[],"relations":{},"dateAdded":"2023-04-25T21:59:17Z","dateModified":"2023-04-25T21:59:17Z","uri":"http://zotero.org/users/11367251/items/CS84BFPP"}]},"meta":{"revision":0,"created":1709832400390,"version":0},"$loki":89},{"itemID":333,"item":{"key":"B99NXQY7","version":343,"itemType":"journalArticle","title":"Center of Excellence for Mobile Sensor Data-to-Knowledge (MD2K)","date":"4/2017","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/7891193/","accessDate":"2023-03-27T03:26:46Z","volume":"16","pages":"18-22","publicationTitle":"IEEE Pervasive Computing","DOI":"10.1109/MPRV.2017.29","issue":"2","journalAbbreviation":"IEEE Pervasive Comput.","ISSN":"1536-1268","creators":[{"firstName":"Santosh","lastName":"Kumar","creatorType":"author"},{"firstName":"Gregory","lastName":"Abowd","creatorType":"author"},{"firstName":"William T.","lastName":"Abraham","creatorType":"author"},{"firstName":"Mustafa","lastName":"al'Absi","creatorType":"author"},{"firstName":"Duen Horng","lastName":"Chau","creatorType":"author"},{"firstName":"Emre","lastName":"Ertin","creatorType":"author"},{"firstName":"Deborah","lastName":"Estrin","creatorType":"author"},{"firstName":"Deepak","lastName":"Ganesan","creatorType":"author"},{"firstName":"Timothy","lastName":"Hnat","creatorType":"author"},{"firstName":"Syed Monowar","lastName":"Hossain","creatorType":"author"},{"firstName":"Zachary","lastName":"Ives","creatorType":"author"},{"firstName":"Jacqueline","lastName":"Kerr","creatorType":"author"},{"firstName":"Benjamin M.","lastName":"Marlin","creatorType":"author"},{"firstName":"Susan","lastName":"Murphy","creatorType":"author"},{"firstName":"James M.","lastName":"Rehg","creatorType":"author"},{"firstName":"Inbal","lastName":"Nahum-Shani","creatorType":"author"},{"firstName":"Vivek","lastName":"Shetty","creatorType":"author"},{"firstName":"Ida","lastName":"Sim","creatorType":"author"},{"firstName":"Bonnie","lastName":"Spring","creatorType":"author"},{"firstName":"Mani","lastName":"Srivastava","creatorType":"author"},{"firstName":"Dave","lastName":"Wetter","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-27T03:26:46Z","dateModified":"2023-03-27T03:26:46Z","uri":"http://zotero.org/users/11367251/items/B99NXQY7","itemID":333,"attachments":[{"key":"HPWECA59","version":346,"itemType":"attachment","title":"kumarCenterExcellenceMobile2017.pdf","parentItem":"B99NXQY7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kumarCenterExcellenceMobile2017.pdf","tags":[],"relations":{},"dateAdded":"2023-03-27T03:26:53Z","dateModified":"2023-03-27T03:26:53Z","uri":"http://zotero.org/users/11367251/items/HPWECA59","localPath":"/Users/reyvababtista/Projects/Papers/kumarCenterExcellenceMobile2017.pdf","defaultPath":"files/334/kumarCenterExcellenceMobile2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400391,"version":0},"$loki":90},{"itemID":1680,"item":{"key":"BHRQQQUQ","version":2125,"itemType":"blogPost","title":"K-means clustering","date":"2023","url":"https://en.wikipedia.org/w/index.php?title=K-means_clustering&oldid=1186084028","accessDate":"2023-12-05","blogTitle":"K-means clustering","creators":[{"name":"Wikipedia contributors","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:06:40Z","dateModified":"2023-12-05T21:06:40Z","uri":"http://zotero.org/users/11367251/items/BHRQQQUQ","itemID":1680,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400391,"version":0},"$loki":91},{"itemID":325,"item":{"key":"BI6E4X2H","version":331,"itemType":"journalArticle","title":"HOPES: An Integrative Digital Phenotyping Platform for Data Collection, Monitoring, and Machine Learning","abstractNote":"The collection of data from a personal digital device to characterize current health conditions and behaviors that determine how an individual’s health will evolve has been called digital phenotyping. In this paper, we describe the development of and early experiences with a comprehensive digital phenotyping platform: Health Outcomes through Positive Engagement and Self-Empowerment (HOPES). HOPES is based on the open-source Beiwe platform but adds a wider range of data collection, including the integration of wearable devices and further sensor collection from smartphones. Requirements were partly derived from a concurrent clinical trial for schizophrenia that required the development of significant capabilities in HOPES for security, privacy, ease of use, and scalability, based on a careful combination of public cloud and on-premises operation. We describe new data pipelines to clean, process, present, and analyze data. This includes a set of dashboards customized to the needs of research study operations and clinical care. A test use case for HOPES was described by analyzing the digital behavior of 22 participants during the SARS-CoV-2 pandemic.","date":"2021-3-15","language":"en","shortTitle":"HOPES","libraryCatalog":"DOI.org (Crossref)","url":"https://www.jmir.org/2021/3/e23984","accessDate":"2023-03-27T02:17:29Z","volume":"23","pages":"e23984","publicationTitle":"Journal of Medical Internet Research","DOI":"10.2196/23984","issue":"3","journalAbbreviation":"J Med Internet Res","ISSN":"1438-8871","creators":[{"firstName":"Xuancong","lastName":"Wang","creatorType":"author"},{"firstName":"Nikola","lastName":"Vouk","creatorType":"author"},{"firstName":"Creighton","lastName":"Heaukulani","creatorType":"author"},{"firstName":"Thisum","lastName":"Buddhika","creatorType":"author"},{"firstName":"Wijaya","lastName":"Martanto","creatorType":"author"},{"firstName":"Jimmy","lastName":"Lee","creatorType":"author"},{"firstName":"Robert JT","lastName":"Morris","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-27T02:17:29Z","dateModified":"2023-03-27T02:17:29Z","uri":"http://zotero.org/users/11367251/items/BI6E4X2H","itemID":325,"attachments":[{"key":"BDV7KE4N","version":332,"itemType":"attachment","title":"wangHOPESIntegrativeDigital2021.pdf","parentItem":"BI6E4X2H","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/wangHOPESIntegrativeDigital2021.pdf","tags":[],"relations":{},"dateAdded":"2023-03-27T02:16:23Z","dateModified":"2023-03-27T02:17:36Z","uri":"http://zotero.org/users/11367251/items/BDV7KE4N","localPath":"/Users/reyvababtista/Projects/Papers/wangHOPESIntegrativeDigital2021.pdf","defaultPath":"files/324/wangHOPESIntegrativeDigital2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400392,"version":0},"$loki":92},{"itemID":1837,"item":{"key":"BJ3WIVFS","version":2305,"itemType":"conferencePaper","title":"University Chatbot using Artificial Intelligence Markup Language","date":"2/2020","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9022814/","accessDate":"2024-01-19T15:23:55Z","place":"Yangon, Myanmar","publisher":"IEEE","ISBN":"978-1-72815-925-6","pages":"1-5","proceedingsTitle":"2020 IEEE Conference on Computer Applications(ICCA)","conferenceName":"2020 IEEE Conference on Computer Applications (ICCA)","DOI":"10.1109/ICCA49400.2020.9022814","creators":[{"firstName":"Naing Naing","lastName":"Khin","creatorType":"author"},{"firstName":"Khin Mar","lastName":"Soe","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:23:55Z","dateModified":"2024-01-19T15:23:55Z","uri":"http://zotero.org/users/11367251/items/BJ3WIVFS","itemID":1837,"attachments":[{"key":"XFTR8SLX","version":2306,"itemType":"attachment","title":"khinUniversityChatbotusing2020.pdf","parentItem":"BJ3WIVFS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/khinUniversityChatbotusing2020.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:24:14Z","dateModified":"2024-01-19T15:24:14Z","uri":"http://zotero.org/users/11367251/items/XFTR8SLX","localPath":"/Users/reyvababtista/Projects/Papers/khinUniversityChatbotusing2020.pdf","defaultPath":"files/1839/khinUniversityChatbotusing2020.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400393,"version":0},"$loki":93},{"itemID":752,"item":{"key":"BT8XNJL6","version":1013,"itemType":"blogPost","title":"Introducing ChatGPT","date":"11/30/2022","url":"https://openai.com/blog/chatgpt","accessDate":"2023-10-04","blogTitle":"Introducing ChatGPT","creators":[{"name":"OpenAI","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-10-04T15:54:31Z","dateModified":"2023-10-04T15:55:27Z","uri":"http://zotero.org/users/11367251/items/BT8XNJL6","itemID":752,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400394,"version":0},"$loki":94},{"itemID":582,"item":{"key":"BU6KYXJS","version":778,"itemType":"preprint","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation","abstractNote":"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.","date":"2015-05-18","language":"en","shortTitle":"U-Net","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1505.04597","accessDate":"2023-09-05T13:37:26Z","extra":"arXiv:1505.04597 [cs]","repository":"arXiv","archiveID":"arXiv:1505.04597","creators":[{"firstName":"Olaf","lastName":"Ronneberger","creatorType":"author"},{"firstName":"Philipp","lastName":"Fischer","creatorType":"author"},{"firstName":"Thomas","lastName":"Brox","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:37:26Z","dateModified":"2023-09-05T13:37:26Z","uri":"http://zotero.org/users/11367251/items/BU6KYXJS","itemID":582,"attachments":[{"key":"5UG547VS","version":818,"itemType":"attachment","title":"ronnebergerUNetConvolutionalNetworks2015.pdf","parentItem":"BU6KYXJS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/ronnebergerUNetConvolutionalNetworks2015.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5UG547VS/1\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:35Z","dateModified":"2023-09-05T13:47:35Z","uri":"http://zotero.org/users/11367251/items/5UG547VS","localPath":"/Users/reyvababtista/Projects/Papers/ronnebergerUNetConvolutionalNetworks2015.pdf","defaultPath":"files/628/ronnebergerUNetConvolutionalNetworks2015.pdf"}],"notes":[{"key":"GCM8PN7J","version":778,"itemType":"note","parentItem":"BU6KYXJS","note":"Comment: conditionally accepted at MICCAI 2015","tags":[],"relations":{},"dateAdded":"2023-09-05T13:37:26Z","dateModified":"2023-09-05T13:37:26Z","uri":"http://zotero.org/users/11367251/items/GCM8PN7J"}]},"meta":{"revision":0,"created":1709832400395,"version":0},"$loki":95},{"itemID":1631,"item":{"key":"BVR2JSR4","version":2186,"itemType":"webpage","title":"Statistical functions","date":"2023","url":"https://docs.scipy.org/doc/scipy/reference/stats.html","accessDate":"2023-11-28","websiteTitle":"Statistical functions","creators":[{"name":"SciPy","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T05:36:45Z","dateModified":"2023-12-08T17:18:43Z","uri":"http://zotero.org/users/11367251/items/BVR2JSR4","itemID":1631,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400395,"version":0},"$loki":96},{"itemID":942,"item":{"key":"C9PSKTI6","version":1290,"itemType":"blogPost","title":"Confirmed: the new Bing runs on OpenAI’s GPT-4","date":"04/14/2023","url":"https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI’s-GPT-4","accessDate":"2023-11-06","blogTitle":"Confirmed: the new Bing runs on OpenAI’s GPT-4","creators":[{"name":"Yusuf Mehdi","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:00:27Z","dateModified":"2023-11-07T02:01:21Z","uri":"http://zotero.org/users/11367251/items/C9PSKTI6","itemID":942,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400395,"version":0},"$loki":97},{"itemID":761,"item":{"key":"CADMY92M","version":1029,"itemType":"preprint","title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model","abstractNote":"We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.","date":"2023-05-29","language":"en","shortTitle":"PaLI-X","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2305.18565","accessDate":"2023-10-11T16:36:57Z","extra":"arXiv:2305.18565 [cs]","repository":"arXiv","archiveID":"arXiv:2305.18565","creators":[{"firstName":"Xi","lastName":"Chen","creatorType":"author"},{"firstName":"Josip","lastName":"Djolonga","creatorType":"author"},{"firstName":"Piotr","lastName":"Padlewski","creatorType":"author"},{"firstName":"Basil","lastName":"Mustafa","creatorType":"author"},{"firstName":"Soravit","lastName":"Changpinyo","creatorType":"author"},{"firstName":"Jialin","lastName":"Wu","creatorType":"author"},{"firstName":"Carlos Riquelme","lastName":"Ruiz","creatorType":"author"},{"firstName":"Sebastian","lastName":"Goodman","creatorType":"author"},{"firstName":"Xiao","lastName":"Wang","creatorType":"author"},{"firstName":"Yi","lastName":"Tay","creatorType":"author"},{"firstName":"Siamak","lastName":"Shakeri","creatorType":"author"},{"firstName":"Mostafa","lastName":"Dehghani","creatorType":"author"},{"firstName":"Daniel","lastName":"Salz","creatorType":"author"},{"firstName":"Mario","lastName":"Lucic","creatorType":"author"},{"firstName":"Michael","lastName":"Tschannen","creatorType":"author"},{"firstName":"Arsha","lastName":"Nagrani","creatorType":"author"},{"firstName":"Hexiang","lastName":"Hu","creatorType":"author"},{"firstName":"Mandar","lastName":"Joshi","creatorType":"author"},{"firstName":"Bo","lastName":"Pang","creatorType":"author"},{"firstName":"Ceslee","lastName":"Montgomery","creatorType":"author"},{"firstName":"Paulina","lastName":"Pietrzyk","creatorType":"author"},{"firstName":"Marvin","lastName":"Ritter","creatorType":"author"},{"firstName":"A. J.","lastName":"Piergiovanni","creatorType":"author"},{"firstName":"Matthias","lastName":"Minderer","creatorType":"author"},{"firstName":"Filip","lastName":"Pavetic","creatorType":"author"},{"firstName":"Austin","lastName":"Waters","creatorType":"author"},{"firstName":"Gang","lastName":"Li","creatorType":"author"},{"firstName":"Ibrahim","lastName":"Alabdulmohsin","creatorType":"author"},{"firstName":"Lucas","lastName":"Beyer","creatorType":"author"},{"firstName":"Julien","lastName":"Amelot","creatorType":"author"},{"firstName":"Kenton","lastName":"Lee","creatorType":"author"},{"firstName":"Andreas Peter","lastName":"Steiner","creatorType":"author"},{"firstName":"Yang","lastName":"Li","creatorType":"author"},{"firstName":"Daniel","lastName":"Keysers","creatorType":"author"},{"firstName":"Anurag","lastName":"Arnab","creatorType":"author"},{"firstName":"Yuanzhong","lastName":"Xu","creatorType":"author"},{"firstName":"Keran","lastName":"Rong","creatorType":"author"},{"firstName":"Alexander","lastName":"Kolesnikov","creatorType":"author"},{"firstName":"Mojtaba","lastName":"Seyedhosseini","creatorType":"author"},{"firstName":"Anelia","lastName":"Angelova","creatorType":"author"},{"firstName":"Xiaohua","lastName":"Zhai","creatorType":"author"},{"firstName":"Neil","lastName":"Houlsby","creatorType":"author"},{"firstName":"Radu","lastName":"Soricut","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-11T16:36:57Z","dateModified":"2023-10-11T16:36:58Z","uri":"http://zotero.org/users/11367251/items/CADMY92M","itemID":761,"attachments":[{"key":"ETBQF4PG","version":1032,"itemType":"attachment","title":"chenPaLIXScalingMultilingual2023.pdf","parentItem":"CADMY92M","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenPaLIXScalingMultilingual2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ETBQF4PG/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/3\">Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/3\">Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/4\">Pretraining Data and Mixture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/4\">Training Stages</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/5\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/5\">Image Captioning and Visual Question Answering</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/5\">Per-task fine-tuning results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/6\">Multitask Fine-tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/6\">Few-shot Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/7\">Video Captioning and Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/7\">Image classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/8\">Object Detection</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/9\">Model Fairness, Biases, and Other Potential Issues</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/11\">Conclusions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/13\">Additional Model Details and Examples</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/13\">PaLI-X Architecture Illustration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/13\">Tuning ViT-22B for better OCR capabilities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/13\">Illustrative PaLI-X Examples</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/14\">Additional results: Image Captioning and VQA</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/14\">Information of Downstream Image Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/14\">Extended Tables of Image Benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/15\">Multi-lingual Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/16\">TallyQA and the emergence of complex counting capability</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/16\">Details on Few-shot Modeling</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/16\">Few-shot Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/17\">Additional Few-shot Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/18\">Few-shot ablation results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/19\">Finetuning hyperparameters</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/19\">Multi-task finetuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/19\">Ablation studies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/21\">Additional results: Video Captioning and QA</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/21\">Datasets &amp;amp; Benchmarks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/22\">Additional results: Image Classification</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/23\">Object Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/23\">Object detection as a VLM task</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/24\">Preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ETBQF4PG/24\">Licenses and attribution for images used in Main Text Figure 2</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-10-11T16:37:03Z","dateModified":"2023-10-11T16:37:03Z","uri":"http://zotero.org/users/11367251/items/ETBQF4PG","localPath":"/Users/reyvababtista/Projects/Papers/chenPaLIXScalingMultilingual2023.pdf","defaultPath":"files/762/chenPaLIXScalingMultilingual2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400398,"version":0},"$loki":98},{"itemID":938,"item":{"key":"CVKB6RI7","version":1268,"itemType":"blogPost","title":"Google’s approach to fighting misinformation online","date":"11/05/2023","url":"https://safety.google/intl/en_uk/stories/fighting-misinformation-online/","accessDate":"2023-11-05","blogTitle":"Google’s approach to fighting misinformation online","creators":[{"name":"Google Safety Centre","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T05:10:29Z","dateModified":"2023-11-06T05:11:14Z","uri":"http://zotero.org/users/11367251/items/CVKB6RI7","itemID":938,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400399,"version":0},"$loki":99},{"itemID":426,"item":{"key":"CYQJSN52","version":517,"itemType":"webpage","title":"pdftools","date":"2023-05-04","url":"https://docs.ropensci.org/pdftools/","accessDate":"2023-05-04","websiteTitle":"pdftools","creators":[{"name":"rOpenSci","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T02:02:30Z","dateModified":"2023-05-05T02:03:26Z","uri":"http://zotero.org/users/11367251/items/CYQJSN52","itemID":426,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400399,"version":0},"$loki":100},{"itemID":571,"item":{"key":"CZ7N8TWC","version":766,"itemType":"preprint","title":"Attention Is All You Need","abstractNote":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","date":"2023-08-01","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1706.03762","accessDate":"2023-09-05T13:33:12Z","extra":"arXiv:1706.03762 [cs]","repository":"arXiv","archiveID":"arXiv:1706.03762","creators":[{"firstName":"Ashish","lastName":"Vaswani","creatorType":"author"},{"firstName":"Noam","lastName":"Shazeer","creatorType":"author"},{"firstName":"Niki","lastName":"Parmar","creatorType":"author"},{"firstName":"Jakob","lastName":"Uszkoreit","creatorType":"author"},{"firstName":"Llion","lastName":"Jones","creatorType":"author"},{"firstName":"Aidan N.","lastName":"Gomez","creatorType":"author"},{"firstName":"Lukasz","lastName":"Kaiser","creatorType":"author"},{"firstName":"Illia","lastName":"Polosukhin","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:33:12Z","dateModified":"2023-09-05T13:33:13Z","uri":"http://zotero.org/users/11367251/items/CZ7N8TWC","itemID":571,"attachments":[{"key":"X5HGLCPK","version":817,"itemType":"attachment","title":"vaswaniAttentionAllYou2023.pdf","parentItem":"CZ7N8TWC","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/vaswaniAttentionAllYou2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_X5HGLCPK/2\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/2\">Background</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/2\">Model Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/3\">Encoder and Decoder Stacks</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/3\">Attention</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/4\">Scaled Dot-Product Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/4\">Multi-Head Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/5\">Applications of Attention in our Model</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/5\">Position-wise Feed-Forward Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/5\">Embeddings and Softmax</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/6\">Positional Encoding</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/6\">Why Self-Attention</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/7\">Training</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/7\">Training Data and Batching</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/7\">Hardware and Schedule</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/7\">Optimizer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/7\">Regularization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/8\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/8\">Machine Translation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/8\">Model Variations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/9\">English Constituency Parsing</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_X5HGLCPK/10\">Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:25Z","dateModified":"2023-09-05T13:47:25Z","uri":"http://zotero.org/users/11367251/items/X5HGLCPK","localPath":"/Users/reyvababtista/Projects/Papers/vaswaniAttentionAllYou2023.pdf","defaultPath":"files/613/vaswaniAttentionAllYou2023.pdf"}],"notes":[{"key":"J2IB6X63","version":766,"itemType":"note","parentItem":"CZ7N8TWC","note":"Comment: 15 pages, 5 figures","tags":[],"relations":{},"dateAdded":"2023-09-05T13:33:12Z","dateModified":"2023-09-05T13:33:12Z","uri":"http://zotero.org/users/11367251/items/J2IB6X63"}]},"meta":{"revision":0,"created":1709832400400,"version":0},"$loki":101},{"itemID":345,"item":{"key":"D2CVLPGV","version":373,"itemType":"journalArticle","title":"A mobile data collection platform for mental health research","abstractNote":"Ubiquitous computing technologies offer exciting new possibilities for monitoring and analyzing user’s experience in real time. In this paper, we describe the design and development of Psychlog, a mobile phone platform designed to collect users’ psychological, physiological, and activity information for mental health research. The tool allows administering self-report questionnaires at speciﬁc times or randomly within a day. The system also permits to collect heart rate and activity information from a wireless electrocardiogram equipped with a three-axial accelerometer. By combining self-reports with heart rate and activity data, the application makes it possible to investigate the relationship between psychological, physiological, and behavioral variables, as well as to monitor their ﬂuctuations over time. The software runs on Windows mobile operative system and is available as open source (http://sourceforge. net/projects/psychlog/).","date":"2/2013","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://link.springer.com/10.1007/s00779-011-0465-2","accessDate":"2023-03-29T07:19:03Z","volume":"17","pages":"241-251","publicationTitle":"Personal and Ubiquitous Computing","DOI":"10.1007/s00779-011-0465-2","issue":"2","journalAbbreviation":"Pers Ubiquit Comput","ISSN":"1617-4909, 1617-4917","creators":[{"firstName":"Andrea","lastName":"Gaggioli","creatorType":"author"},{"firstName":"Giovanni","lastName":"Pioggia","creatorType":"author"},{"firstName":"Gennaro","lastName":"Tartarisco","creatorType":"author"},{"firstName":"Giovanni","lastName":"Baldus","creatorType":"author"},{"firstName":"Daniele","lastName":"Corda","creatorType":"author"},{"firstName":"Pietro","lastName":"Cipresso","creatorType":"author"},{"firstName":"Giuseppe","lastName":"Riva","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-29T07:19:03Z","dateModified":"2023-03-29T07:19:03Z","uri":"http://zotero.org/users/11367251/items/D2CVLPGV","itemID":345,"attachments":[{"key":"UKFY835M","version":377,"itemType":"attachment","title":"gaggiolimobiledatacollection2013.pdf","parentItem":"D2CVLPGV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gaggiolimobiledatacollection22.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UKFY835M/1\">Abstract</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UKFY835M/1\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/2\">Experience sampling method</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/2\">Computerized experience sampling procedures</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UKFY835M/3\">Design requirements</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/3\">Activity and physiological sensing</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UKFY835M/4\">System’s overview</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/4\">Survey manager module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/5\">Sensing/computing module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/6\">Visualization module</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/6\">Data management</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/6\">Implementation of the prototype</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/6\">System’s performance evaluation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UKFY835M/6\">Pilot study</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/7\">Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/7\">Procedure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/7\">Psychological measures of stress</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/7\">Physiological measures of stress</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/7\">Data analysis and results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/9\">Conclusions and future developments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/10\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UKFY835M/10\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-29T07:19:09Z","dateModified":"2023-03-29T07:19:10Z","uri":"http://zotero.org/users/11367251/items/UKFY835M","localPath":"/Users/reyvababtista/Projects/Papers/gaggiolimobiledatacollection22.pdf","defaultPath":"files/346/gaggiolimobiledatacollection22.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400400,"version":0},"$loki":102},{"itemID":952,"item":{"key":"D2FWWTZR","version":1320,"itemType":"blogPost","title":"Privacy concerns regarding Google","date":"2023","url":"https://en.wikipedia.org/wiki/Privacy_concerns_regarding_Google","accessDate":"2023-11-06","blogTitle":"Privacy concerns regarding Google","creators":[{"name":"Wikipedia","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:16:33Z","dateModified":"2023-11-07T02:17:06Z","uri":"http://zotero.org/users/11367251/items/D2FWWTZR","itemID":952,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400401,"version":0},"$loki":103},{"itemID":1695,"item":{"key":"D3EWVSBQ","version":2153,"itemType":"journalArticle","title":"Language Models are Few-Shot Learners","abstractNote":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.","date":"2020","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2005.14165","accessDate":"2023-12-06T23:09:58Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 4","DOI":"10.48550/ARXIV.2005.14165","creators":[{"firstName":"Tom B.","lastName":"Brown","creatorType":"author"},{"firstName":"Benjamin","lastName":"Mann","creatorType":"author"},{"firstName":"Nick","lastName":"Ryder","creatorType":"author"},{"firstName":"Melanie","lastName":"Subbiah","creatorType":"author"},{"firstName":"Jared","lastName":"Kaplan","creatorType":"author"},{"firstName":"Prafulla","lastName":"Dhariwal","creatorType":"author"},{"firstName":"Arvind","lastName":"Neelakantan","creatorType":"author"},{"firstName":"Pranav","lastName":"Shyam","creatorType":"author"},{"firstName":"Girish","lastName":"Sastry","creatorType":"author"},{"firstName":"Amanda","lastName":"Askell","creatorType":"author"},{"firstName":"Sandhini","lastName":"Agarwal","creatorType":"author"},{"firstName":"Ariel","lastName":"Herbert-Voss","creatorType":"author"},{"firstName":"Gretchen","lastName":"Krueger","creatorType":"author"},{"firstName":"Tom","lastName":"Henighan","creatorType":"author"},{"firstName":"Rewon","lastName":"Child","creatorType":"author"},{"firstName":"Aditya","lastName":"Ramesh","creatorType":"author"},{"firstName":"Daniel M.","lastName":"Ziegler","creatorType":"author"},{"firstName":"Jeffrey","lastName":"Wu","creatorType":"author"},{"firstName":"Clemens","lastName":"Winter","creatorType":"author"},{"firstName":"Christopher","lastName":"Hesse","creatorType":"author"},{"firstName":"Mark","lastName":"Chen","creatorType":"author"},{"firstName":"Eric","lastName":"Sigler","creatorType":"author"},{"firstName":"Mateusz","lastName":"Litwin","creatorType":"author"},{"firstName":"Scott","lastName":"Gray","creatorType":"author"},{"firstName":"Benjamin","lastName":"Chess","creatorType":"author"},{"firstName":"Jack","lastName":"Clark","creatorType":"author"},{"firstName":"Christopher","lastName":"Berner","creatorType":"author"},{"firstName":"Sam","lastName":"McCandlish","creatorType":"author"},{"firstName":"Alec","lastName":"Radford","creatorType":"author"},{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"},{"firstName":"Dario","lastName":"Amodei","creatorType":"author"}],"tags":[{"tag":"Computation and Language (cs.CL)","type":1},{"tag":"FOS: Computer and information sciences","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:09:58Z","dateModified":"2023-12-06T23:09:58Z","uri":"http://zotero.org/users/11367251/items/D3EWVSBQ","itemID":1695,"attachments":[],"notes":[{"key":"26LT2LS8","version":2153,"itemType":"note","parentItem":"D3EWVSBQ","note":"<h2>Other</h2>\n40+32 pages","tags":[],"relations":{},"dateAdded":"2023-12-06T23:09:58Z","dateModified":"2023-12-06T23:09:58Z","uri":"http://zotero.org/users/11367251/items/26LT2LS8"}]},"meta":{"revision":0,"created":1709832400401,"version":0},"$loki":104},{"itemID":425,"item":{"key":"D6ZIPS33","version":513,"itemType":"webpage","title":"tesseract","date":"2023-05-04","url":"https://docs.ropensci.org/tesseract/","accessDate":"2023-05-04","websiteTitle":"tesseract","creators":[{"name":"rOpenSci","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T01:54:59Z","dateModified":"2023-05-05T01:55:21Z","uri":"http://zotero.org/users/11367251/items/D6ZIPS33","itemID":425,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400402,"version":0},"$loki":105},{"itemID":1677,"item":{"key":"D93ACNUT","version":2121,"itemType":"blogPost","title":"Support vector machine","date":"2023","url":"https://en.wikipedia.org/w/index.php?title=Support_vector_machine&oldid=1183475870","accessDate":"2023-12-05","blogTitle":"Support vector machine","creators":[{"name":"Wikipedia contributors","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:02:34Z","dateModified":"2023-12-05T21:04:28Z","uri":"http://zotero.org/users/11367251/items/D93ACNUT","itemID":1677,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400402,"version":0},"$loki":106},{"itemID":1689,"item":{"key":"DA4BYWTA","version":2149,"itemType":"journalArticle","title":"High-Resolution Image Synthesis with Latent Diffusion Models","abstractNote":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .","date":"2021","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2112.10752","accessDate":"2023-12-06T23:04:43Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 2","DOI":"10.48550/ARXIV.2112.10752","creators":[{"firstName":"Robin","lastName":"Rombach","creatorType":"author"},{"firstName":"Andreas","lastName":"Blattmann","creatorType":"author"},{"firstName":"Dominik","lastName":"Lorenz","creatorType":"author"},{"firstName":"Patrick","lastName":"Esser","creatorType":"author"},{"firstName":"Björn","lastName":"Ommer","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:04:43Z","dateModified":"2023-12-06T23:04:43Z","uri":"http://zotero.org/users/11367251/items/DA4BYWTA","itemID":1689,"attachments":[],"notes":[{"key":"7TUZ6VDD","version":2149,"itemType":"note","parentItem":"DA4BYWTA","note":"<h2>Other</h2>\nCVPR 2022","tags":[],"relations":{},"dateAdded":"2023-12-06T23:04:43Z","dateModified":"2023-12-06T23:04:43Z","uri":"http://zotero.org/users/11367251/items/7TUZ6VDD"}]},"meta":{"revision":0,"created":1709832400403,"version":0},"$loki":107},{"itemID":376,"item":{"key":"DDQ9AHWX","version":418,"itemType":"journalArticle","title":"Data fusion of mobile and environmental sensing devices to understand the effect of the indoor environment on measured and self-reported sleep quality","abstractNote":"The Indoor Air Quality (IAQ) of the bedroom environment has recently garnered attention since air pollution can affect sleep. Previous studies investigated IAQ and sleep quality in controlled environments which impacts both self-reported and measured sleep quality. Studies within a participant’s home environment are ecologically valid and reduce participant bias. Here, we study 20 participants over 77 days in Austin, TX. We monitored five components of IAQ using the BEVO Beacon, a calibrated purpose-built environmental monitor, and measured participant sleep quality through wearable activity trackers and 4-question surveys sent four times a week. We found significant decreases in sleep quality during nights with elevated CO, CO2, and temperature. Elevated CO was associated with a mean increase in 0.9 self-reported awakenings and decreases in device-measured sleep time of 21.6 min and sleep efficiency of 0.6%. Increased CO2 and temperature were associated with decreases in device-measured sleep time of 17.5 and 15.2 min, respectively. Elevated PM2.5 and TVOCs concentrations were associated with overall improvements in sleep quality. Participants reported a mean of 4.4 fewer awakenings and had a 1.1% increased in measured sleep efficiency for nights with elevated PM2.5. Elevated TVOCs were associated with an increase in sleep time of 14.5 min. These findings indicate a need to study the relationship between these aggregate IAQ measures and sleep quality more closely. Our results also indicate that pollutants can independently affect sleep quality regardless of the CO2 measurements. Compared to literature, our study is the longest and includes the most IAQ parameters.","date":"04/2022","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S0360132322000828","accessDate":"2023-04-11T22:10:29Z","volume":"214","pages":"108835","publicationTitle":"Building and Environment","DOI":"10.1016/j.buildenv.2022.108835","journalAbbreviation":"Building and Environment","ISSN":"03601323","creators":[{"firstName":"Hagen","lastName":"Fritz","creatorType":"author"},{"firstName":"Kerry A.","lastName":"Kinney","creatorType":"author"},{"firstName":"Congyu","lastName":"Wu","creatorType":"author"},{"firstName":"David M.","lastName":"Schnyer","creatorType":"author"},{"firstName":"Zoltan","lastName":"Nagy","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-04-11T22:10:29Z","dateModified":"2023-04-11T22:10:29Z","uri":"http://zotero.org/users/11367251/items/DDQ9AHWX","itemID":376,"attachments":[{"key":"I7KSQ497","version":422,"itemType":"attachment","title":"fritzDatafusionmobile2022.pdf","parentItem":"DDQ9AHWX","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/fritzDatafusionmobile2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_I7KSQ497/1\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/2\">Related works</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/2\">Contributions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/2\">Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/2\">Study design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/2\">Self-reported mood and sleep</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/3\">Device-monitored sleep quality</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/3\">Indoor environmental quality monitoring</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/4\">Data fusion for occupancy detection</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/4\">Smartphone GPS and address comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/4\">Carbon Dioxide monitoring</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/4\">Analysis of IAQ parameters during sleep</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/5\">Analysis of non-IAQ parameters during sleep</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/5\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/5\">Data availability and summary by modality</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/5\">Self-report sleep metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/5\">Device-monitored sleep metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/6\">Indoor air quality</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/6\">IAQ and sleep quality</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/6\">Self-report sleep</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/8\">Fitbit-monitored sleep</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/8\">Relationships between sleep quality and other study variables</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/8\">Fitbit-measured versus self-reported sleep metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/9\">Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/9\">Comparison to related studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/10\">Limitations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">Declaration of competing interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">Acknowledgment</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">Appendix. Additional Figures and Tables</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">Participant demographics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/11\">Ecological momentary assessment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/12\">Sensors on the Building EnVironment and Occupancy Beacon</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/12\">Calibration</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/13\">Distributions of nightly IAQ measurements</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_I7KSQ497/13\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-11T22:10:45Z","dateModified":"2023-04-11T22:10:45Z","uri":"http://zotero.org/users/11367251/items/I7KSQ497","localPath":"/Users/reyvababtista/Projects/Papers/fritzDatafusionmobile2022.pdf","defaultPath":"files/379/fritzDatafusionmobile2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400403,"version":0},"$loki":108},{"itemID":1408,"item":{"key":"DHVFGBGX","version":1545,"itemType":"preprint","title":"Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities","abstractNote":"Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit strong generalization on various visual domains and tasks. However, existing image classification benchmarks often evaluate recognition on a specific domain (e.g., outdoor images) or a specific task (e.g., classifying plant species), which falls short of evaluating whether pre-trained foundational models are universal visual recognizers. To address this, we formally present the task of Open-domain Visual Entity recognitioN (OVEN), where a model need to link an image onto a Wikipedia entity with respect to a text query. We construct OVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto one single label space: Wikipedia entities. OVEN challenges models to select among six million possible Wikipedia entities, making it a general visual recognition benchmark with the largest number of labels. Our study on state-of-the-art pre-trained models reveals large headroom in generalizing to the massive-scale label space. We show that a PaLI-based auto-regressive visual recognition model performs surprisingly well, even on Wikipedia entities that have never been seen during fine-tuning. We also find existing pretrained models yield different strengths: while PaLI-based models obtain higher overall performance, CLIP-based models are better at recognizing tail entities.","date":"2023-02-23","shortTitle":"Open-domain Visual Entity Recognition","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2302.11154","accessDate":"2023-11-08T22:42:00Z","extra":"arXiv:2302.11154 [cs]","repository":"arXiv","archiveID":"arXiv:2302.11154","creators":[{"firstName":"Hexiang","lastName":"Hu","creatorType":"author"},{"firstName":"Yi","lastName":"Luan","creatorType":"author"},{"firstName":"Yang","lastName":"Chen","creatorType":"author"},{"firstName":"Urvashi","lastName":"Khandelwal","creatorType":"author"},{"firstName":"Mandar","lastName":"Joshi","creatorType":"author"},{"firstName":"Kenton","lastName":"Lee","creatorType":"author"},{"firstName":"Kristina","lastName":"Toutanova","creatorType":"author"},{"firstName":"Ming-Wei","lastName":"Chang","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:42:00Z","dateModified":"2023-11-08T22:42:00Z","uri":"http://zotero.org/users/11367251/items/DHVFGBGX","itemID":1408,"attachments":[{"key":"ZLT88ILS","version":1548,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2302.11154","accessDate":"2023-11-08T22:42:11Z","parentItem":"DHVFGBGX","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2302.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:42:11Z","dateModified":"2023-11-08T22:42:11Z","uri":"http://zotero.org/users/11367251/items/ZLT88ILS","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/ZLT88ILS/2302.html","defaultPath":"files/1412/2302.html"},{"key":"UVSDYBBQ","version":1546,"itemType":"attachment","title":"huOpendomainVisualEntity2023.pdf","parentItem":"DHVFGBGX","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/huOpendomainVisualEntity2023.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:42:06Z","dateModified":"2023-11-08T22:42:06Z","uri":"http://zotero.org/users/11367251/items/UVSDYBBQ","localPath":"/Users/reyvababtista/Projects/Papers/huOpendomainVisualEntity2023.pdf","defaultPath":"files/1411/huOpendomainVisualEntity2023.pdf"}],"notes":[{"key":"8JL46QVD","version":1545,"itemType":"note","parentItem":"DHVFGBGX","note":"Comment: Dataset available at https://open-vision-language.github.io/oven","tags":[],"relations":{},"dateAdded":"2023-11-08T22:42:00Z","dateModified":"2023-11-08T22:42:00Z","uri":"http://zotero.org/users/11367251/items/8JL46QVD"}]},"meta":{"revision":0,"created":1709832400404,"version":0},"$loki":109},{"itemID":1649,"item":{"key":"DJ8X937U","version":2074,"itemType":"conferencePaper","title":"Random decision forests","date":"1995","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/598994/","accessDate":"2023-12-03T20:37:27Z","volume":"1","place":"Montreal, Que., Canada","publisher":"IEEE Comput. Soc. Press","ISBN":"978-0-8186-7128-9","pages":"278-282","proceedingsTitle":"Proceedings of 3rd International Conference on Document Analysis and Recognition","conferenceName":"3rd International Conference on Document Analysis and Recognition","DOI":"10.1109/ICDAR.1995.598994","creators":[{"name":"Tin Kam Ho","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-03T20:37:27Z","dateModified":"2023-12-03T20:37:27Z","uri":"http://zotero.org/users/11367251/items/DJ8X937U","itemID":1649,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400405,"version":0},"$loki":110},{"itemID":928,"item":{"key":"DL4D9899","version":1209,"itemType":"blogPost","title":"Announcing mobile first indexing for the whole web","date":"03/05/2020","url":"https://developers.google.com/search/blog/2020/03/announcing-mobile-first-indexing-for","accessDate":"2023-12-04","blogTitle":"Announcing mobile first indexing for the whole web","creators":[{"name":"John Mueller","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-05T14:43:21Z","dateModified":"2023-11-05T14:44:08Z","uri":"http://zotero.org/users/11367251/items/DL4D9899","itemID":928,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400405,"version":0},"$loki":111},{"itemID":1471,"item":{"key":"DPPNIYI9","version":1619,"itemType":"preprint","title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models","abstractNote":"The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.","date":"2016-09-19","shortTitle":"Flickr30k Entities","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1505.04870","accessDate":"2023-11-08T23:00:20Z","extra":"arXiv:1505.04870 [cs]","repository":"arXiv","archiveID":"arXiv:1505.04870","creators":[{"firstName":"Bryan A.","lastName":"Plummer","creatorType":"author"},{"firstName":"Liwei","lastName":"Wang","creatorType":"author"},{"firstName":"Chris M.","lastName":"Cervantes","creatorType":"author"},{"firstName":"Juan C.","lastName":"Caicedo","creatorType":"author"},{"firstName":"Julia","lastName":"Hockenmaier","creatorType":"author"},{"firstName":"Svetlana","lastName":"Lazebnik","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:00:20Z","dateModified":"2023-11-08T23:00:20Z","uri":"http://zotero.org/users/11367251/items/DPPNIYI9","itemID":1471,"attachments":[{"key":"99M9Q8VS","version":1623,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1505.04870","accessDate":"2023-11-08T23:00:31Z","parentItem":"DPPNIYI9","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1505.html","tags":[],"relations":{},"dateAdded":"2023-11-08T23:00:31Z","dateModified":"2023-11-08T23:00:31Z","uri":"http://zotero.org/users/11367251/items/99M9Q8VS","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/99M9Q8VS/1505.html","defaultPath":"files/1474/1505.html"},{"key":"G2A9T4SV","version":1620,"itemType":"attachment","title":"plummerFlickr30kEntitiesCollecting2016.pdf","parentItem":"DPPNIYI9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/plummerFlickr30kEntitiesCollecting2016.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:00:26Z","dateModified":"2023-11-08T23:00:26Z","uri":"http://zotero.org/users/11367251/items/G2A9T4SV","localPath":"/Users/reyvababtista/Projects/Papers/plummerFlickr30kEntitiesCollecting2016.pdf","defaultPath":"files/1473/plummerFlickr30kEntitiesCollecting2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400405,"version":0},"$loki":112},{"itemID":550,"item":{"key":"DS5HQUJW","version":733,"itemType":"preprint","title":"Evaluating Large Language Models Trained on Code","abstractNote":"We introduce Codex, a GPT language model ﬁnetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we ﬁnd that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difﬁculty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.","date":"2021-07-14","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2107.03374","accessDate":"2023-06-30T14:53:38Z","extra":"arXiv:2107.03374 [cs]","repository":"arXiv","archiveID":"arXiv:2107.03374","creators":[{"firstName":"Mark","lastName":"Chen","creatorType":"author"},{"firstName":"Jerry","lastName":"Tworek","creatorType":"author"},{"firstName":"Heewoo","lastName":"Jun","creatorType":"author"},{"firstName":"Qiming","lastName":"Yuan","creatorType":"author"},{"firstName":"Henrique Ponde de Oliveira","lastName":"Pinto","creatorType":"author"},{"firstName":"Jared","lastName":"Kaplan","creatorType":"author"},{"firstName":"Harri","lastName":"Edwards","creatorType":"author"},{"firstName":"Yuri","lastName":"Burda","creatorType":"author"},{"firstName":"Nicholas","lastName":"Joseph","creatorType":"author"},{"firstName":"Greg","lastName":"Brockman","creatorType":"author"},{"firstName":"Alex","lastName":"Ray","creatorType":"author"},{"firstName":"Raul","lastName":"Puri","creatorType":"author"},{"firstName":"Gretchen","lastName":"Krueger","creatorType":"author"},{"firstName":"Michael","lastName":"Petrov","creatorType":"author"},{"firstName":"Heidy","lastName":"Khlaaf","creatorType":"author"},{"firstName":"Girish","lastName":"Sastry","creatorType":"author"},{"firstName":"Pamela","lastName":"Mishkin","creatorType":"author"},{"firstName":"Brooke","lastName":"Chan","creatorType":"author"},{"firstName":"Scott","lastName":"Gray","creatorType":"author"},{"firstName":"Nick","lastName":"Ryder","creatorType":"author"},{"firstName":"Mikhail","lastName":"Pavlov","creatorType":"author"},{"firstName":"Alethea","lastName":"Power","creatorType":"author"},{"firstName":"Lukasz","lastName":"Kaiser","creatorType":"author"},{"firstName":"Mohammad","lastName":"Bavarian","creatorType":"author"},{"firstName":"Clemens","lastName":"Winter","creatorType":"author"},{"firstName":"Philippe","lastName":"Tillet","creatorType":"author"},{"firstName":"Felipe Petroski","lastName":"Such","creatorType":"author"},{"firstName":"Dave","lastName":"Cummings","creatorType":"author"},{"firstName":"Matthias","lastName":"Plappert","creatorType":"author"},{"firstName":"Fotios","lastName":"Chantzis","creatorType":"author"},{"firstName":"Elizabeth","lastName":"Barnes","creatorType":"author"},{"firstName":"Ariel","lastName":"Herbert-Voss","creatorType":"author"},{"firstName":"William Hebgen","lastName":"Guss","creatorType":"author"},{"firstName":"Alex","lastName":"Nichol","creatorType":"author"},{"firstName":"Alex","lastName":"Paino","creatorType":"author"},{"firstName":"Nikolas","lastName":"Tezak","creatorType":"author"},{"firstName":"Jie","lastName":"Tang","creatorType":"author"},{"firstName":"Igor","lastName":"Babuschkin","creatorType":"author"},{"firstName":"Suchir","lastName":"Balaji","creatorType":"author"},{"firstName":"Shantanu","lastName":"Jain","creatorType":"author"},{"firstName":"William","lastName":"Saunders","creatorType":"author"},{"firstName":"Christopher","lastName":"Hesse","creatorType":"author"},{"firstName":"Andrew N.","lastName":"Carr","creatorType":"author"},{"firstName":"Jan","lastName":"Leike","creatorType":"author"},{"firstName":"Josh","lastName":"Achiam","creatorType":"author"},{"firstName":"Vedant","lastName":"Misra","creatorType":"author"},{"firstName":"Evan","lastName":"Morikawa","creatorType":"author"},{"firstName":"Alec","lastName":"Radford","creatorType":"author"},{"firstName":"Matthew","lastName":"Knight","creatorType":"author"},{"firstName":"Miles","lastName":"Brundage","creatorType":"author"},{"firstName":"Mira","lastName":"Murati","creatorType":"author"},{"firstName":"Katie","lastName":"Mayer","creatorType":"author"},{"firstName":"Peter","lastName":"Welinder","creatorType":"author"},{"firstName":"Bob","lastName":"McGrew","creatorType":"author"},{"firstName":"Dario","lastName":"Amodei","creatorType":"author"},{"firstName":"Sam","lastName":"McCandlish","creatorType":"author"},{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"},{"firstName":"Wojciech","lastName":"Zaremba","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-30T14:53:38Z","dateModified":"2023-06-30T14:53:38Z","uri":"http://zotero.org/users/11367251/items/DS5HQUJW","itemID":550,"attachments":[{"key":"ZCBH9MR5","version":733,"itemType":"attachment","title":"chenEvaluatingLargeLanguage2021.pdf","parentItem":"DS5HQUJW","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenEvaluatingLargeLanguage2021.pdf","tags":[],"relations":{},"dateAdded":"2023-06-30T14:53:43Z","dateModified":"2023-06-30T14:53:43Z","uri":"http://zotero.org/users/11367251/items/ZCBH9MR5","localPath":"/Users/reyvababtista/Projects/Papers/chenEvaluatingLargeLanguage2021.pdf","defaultPath":"files/552/chenEvaluatingLargeLanguage2021.pdf"}],"notes":[{"key":"NYSVESFC","version":732,"itemType":"note","parentItem":"DS5HQUJW","note":"Comment: corrected typos, added references, added authors, added acknowledgements","tags":[],"relations":{},"dateAdded":"2023-06-30T14:53:38Z","dateModified":"2023-06-30T14:53:38Z","uri":"http://zotero.org/users/11367251/items/NYSVESFC"}]},"meta":{"revision":0,"created":1709832400406,"version":0},"$loki":113},{"itemID":599,"item":{"key":"DVJ69QQB","version":801,"itemType":"journalArticle","title":"Siamese Neural Networks for One-shot Image Recognition","abstractNote":"The process of learning good features for machine learning applications can be very computationally expensive and may prove difﬁcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiﬁcation tasks.","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Gregory","lastName":"Koch","creatorType":"author"},{"firstName":"Richard","lastName":"Zemel","creatorType":"author"},{"firstName":"Ruslan","lastName":"Salakhutdinov","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:43:50Z","dateModified":"2023-09-05T13:43:50Z","uri":"http://zotero.org/users/11367251/items/DVJ69QQB","itemID":599,"attachments":[{"key":"H6V722LR","version":818,"itemType":"attachment","title":"kochSiameseNeuralNetworks.pdf","parentItem":"DVJ69QQB","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kochSiameseNeuralNetworks.pdf","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:33Z","dateModified":"2023-09-05T13:47:33Z","uri":"http://zotero.org/users/11367251/items/H6V722LR","localPath":"/Users/reyvababtista/Projects/Papers/kochSiameseNeuralNetworks.pdf","defaultPath":"files/625/kochSiameseNeuralNetworks.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400407,"version":0},"$loki":114},{"itemID":1483,"item":{"key":"E3VFZN57","version":1634,"itemType":"preprint","title":"The Kinetics Human Action Video Dataset","abstractNote":"We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.","date":"2017-05-19","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1705.06950","accessDate":"2023-11-08T23:03:50Z","extra":"arXiv:1705.06950 [cs]","repository":"arXiv","archiveID":"arXiv:1705.06950","creators":[{"firstName":"Will","lastName":"Kay","creatorType":"author"},{"firstName":"Joao","lastName":"Carreira","creatorType":"author"},{"firstName":"Karen","lastName":"Simonyan","creatorType":"author"},{"firstName":"Brian","lastName":"Zhang","creatorType":"author"},{"firstName":"Chloe","lastName":"Hillier","creatorType":"author"},{"firstName":"Sudheendra","lastName":"Vijayanarasimhan","creatorType":"author"},{"firstName":"Fabio","lastName":"Viola","creatorType":"author"},{"firstName":"Tim","lastName":"Green","creatorType":"author"},{"firstName":"Trevor","lastName":"Back","creatorType":"author"},{"firstName":"Paul","lastName":"Natsev","creatorType":"author"},{"firstName":"Mustafa","lastName":"Suleyman","creatorType":"author"},{"firstName":"Andrew","lastName":"Zisserman","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:03:50Z","dateModified":"2023-11-08T23:03:50Z","uri":"http://zotero.org/users/11367251/items/E3VFZN57","itemID":1483,"attachments":[{"key":"E2XKWNIY","version":1637,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1705.06950","accessDate":"2023-11-08T23:03:59Z","parentItem":"E3VFZN57","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1705.html","tags":[],"relations":{},"dateAdded":"2023-11-08T23:03:59Z","dateModified":"2023-11-08T23:03:59Z","uri":"http://zotero.org/users/11367251/items/E2XKWNIY","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/E2XKWNIY/1705.html","defaultPath":"files/1486/1705.html"},{"key":"KA33NLR8","version":1634,"itemType":"attachment","title":"kayKineticsHumanAction2017.pdf","parentItem":"E3VFZN57","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kayKineticsHumanAction2017.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:03:54Z","dateModified":"2023-11-08T23:03:54Z","uri":"http://zotero.org/users/11367251/items/KA33NLR8","localPath":"/Users/reyvababtista/Projects/Papers/kayKineticsHumanAction2017.pdf","defaultPath":"files/1485/kayKineticsHumanAction2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400407,"version":0},"$loki":115},{"itemID":441,"item":{"key":"EE77A9JT","version":551,"itemType":"preprint","title":"Path Aggregation Network for Instance Segmentation","abstractNote":"The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information ﬂow in proposal-based instance segmentation framework. Speciﬁcally, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.","date":"2018-09-18","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1803.01534","accessDate":"2023-05-08T22:17:07Z","extra":"arXiv:1803.01534 [cs]","repository":"arXiv","archiveID":"arXiv:1803.01534","creators":[{"firstName":"Shu","lastName":"Liu","creatorType":"author"},{"firstName":"Lu","lastName":"Qi","creatorType":"author"},{"firstName":"Haifang","lastName":"Qin","creatorType":"author"},{"firstName":"Jianping","lastName":"Shi","creatorType":"author"},{"firstName":"Jiaya","lastName":"Jia","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-05-08T22:17:07Z","dateModified":"2023-05-08T22:17:07Z","uri":"http://zotero.org/users/11367251/items/EE77A9JT","itemID":441,"attachments":[{"key":"V7P325YU","version":553,"itemType":"attachment","title":"liuPathAggregationNetwork2018.pdf","parentItem":"EE77A9JT","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liuPathAggregationNetwork2018.pdf","tags":[],"relations":{},"dateAdded":"2023-05-08T22:17:16Z","dateModified":"2023-05-08T22:17:16Z","uri":"http://zotero.org/users/11367251/items/V7P325YU","localPath":"/Users/reyvababtista/Projects/Papers/liuPathAggregationNetwork2018.pdf","defaultPath":"files/443/liuPathAggregationNetwork2018.pdf"}],"notes":[{"key":"Z9WYBJ7I","version":551,"itemType":"note","parentItem":"EE77A9JT","note":"Comment: Accepted to CVPR 2018","tags":[],"relations":{},"dateAdded":"2023-05-08T22:17:07Z","dateModified":"2023-05-08T22:17:07Z","uri":"http://zotero.org/users/11367251/items/Z9WYBJ7I"}]},"meta":{"revision":0,"created":1709832400408,"version":0},"$loki":116},{"itemID":318,"item":{"key":"EECBVIVR","version":318,"itemType":"conferencePaper","title":"Data Processing Pipeline of Short-Term Depression Detection with Large-Scale Dataset","abstractNote":"Depression is a common, recurring mental disorder that causes signiﬁcant impairment in people’s lives. In recent years, ubiquitous computing using mobile phones can monitor behavioral patterns relevant to depressive symptoms in-the-wild. In this paper, we propose data processing pipeline of short-term depression detection using mobile sensor data. We build a group model classiﬁed by depression severity for capturing depressive mood in a short-period time to handle data quality and data imbalance problem in a large-scale dataset. We expect the group model to identify and characterize digital phenotype representing each depressive group as a middle step toward personalization.","date":"2/2023","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/10066563/","accessDate":"2023-03-26T21:33:11Z","place":"Jeju, Korea, Republic of","publisher":"IEEE","ISBN":"978-1-66547-578-5","pages":"391-392","proceedingsTitle":"2023 IEEE International Conference on Big Data and Smart Computing (BigComp)","conferenceName":"2023 IEEE International Conference on Big Data and Smart Computing (BigComp)","DOI":"10.1109/BigComp57234.2023.00095","creators":[{"firstName":"Yonggeon","lastName":"Lee","creatorType":"author"},{"firstName":"Youngtae","lastName":"Noh","creatorType":"author"},{"firstName":"Uichin","lastName":"Lee","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:33:11Z","dateModified":"2023-03-26T21:33:12Z","uri":"http://zotero.org/users/11367251/items/EECBVIVR","itemID":318,"attachments":[{"key":"XX4ITPFR","version":321,"itemType":"attachment","title":"leeDataProcessingPipeline2023.pdf","parentItem":"EECBVIVR","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/leeDataProcessingPipeline2023.pdf","tags":[],"relations":{},"dateAdded":"2023-03-26T21:34:08Z","dateModified":"2023-03-26T21:34:08Z","uri":"http://zotero.org/users/11367251/items/XX4ITPFR","localPath":"/Users/reyvababtista/Projects/Papers/leeDataProcessingPipeline2023.pdf","defaultPath":"files/319/leeDataProcessingPipeline2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400408,"version":0},"$loki":117},{"itemID":21,"item":{"key":"EFMV3ZVD","version":199,"itemType":"journalArticle","title":"TSingNet: Scale-aware and context-rich feature learning for traffic sign detection and recognition in the wild","abstractNote":"Trafﬁc sign detection and recognition in the wild is a challenging task. Existing techniques are often incapable of detecting small or occluded trafﬁc signs because of the scale variation and context loss, which causes semantic gaps between multiple scales. We propose a new trafﬁc sign detection network (TSingNet), which learns scale-aware and context-rich features to effectively detect and recognize small and occluded trafﬁc signs in the wild. Speciﬁcally, TSingNet ﬁrst constructs an attention-driven bilateral feature pyramid network, which draws on both bottom-up and top-down subnets to dually circulate low-, mid-, and high-level foreground semantics in scale self-attention learning. This is to learn scaleaware foreground features and thus narrow down the semantic gaps between multiple scales. An adaptive receptive ﬁeld fusion block with variable dilation rates is then introduced to exploit context-rich representation and suppress the inﬂuence of occlusion at each scale. TSingNet is end-to-end trainable by joint minimization of the scale-aware loss and multi-branch fusion losses, this adds a few parameters but signiﬁcantly improves the detection performance. In extensive experiments with three challenging trafﬁc sign datasets (TT100K, STSD and DFG), TSingNet outperformed state-of-the-art methods for trafﬁc sign detection and recognition in the wild.","date":"08/2021","language":"en","shortTitle":"TSingNet","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S0925231221004239","accessDate":"2023-03-20T14:25:03Z","volume":"447","pages":"10-22","publicationTitle":"Neurocomputing","DOI":"10.1016/j.neucom.2021.03.049","journalAbbreviation":"Neurocomputing","ISSN":"09252312","creators":[{"firstName":"Yuanyuan","lastName":"Liu","creatorType":"author"},{"firstName":"Jiyao","lastName":"Peng","creatorType":"author"},{"firstName":"Jing-Hao","lastName":"Xue","creatorType":"author"},{"firstName":"Yongquan","lastName":"Chen","creatorType":"author"},{"firstName":"Zhang-Hua","lastName":"Fu","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/DENKCLUP"]},"dateAdded":"2023-03-20T14:25:03Z","dateModified":"2023-03-22T17:03:35Z","uri":"http://zotero.org/users/11367251/items/EFMV3ZVD","itemID":21,"attachments":[{"key":"E8U6YPAM","version":240,"itemType":"attachment","title":"liuTSingNetScaleawarecontextrich2021.pdf","parentItem":"EFMV3ZVD","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liuTSingNetScaleawarecontextrich2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_E8U6YPAM/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/2\">2 Related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/2\">2.1 Methods based on deep learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/3\">2.2 Multi-scale feature learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/3\">2.3 Context exploitation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/3\">3 Proposed method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/3\">3.1 AbFPN for scale-aware foreground feature learning</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/3\">3.1.1 Bottom-up subnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/3\">3.1.2 Scale-aware top-down subnet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/4\">3.1.3 SAL function</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/5\">3.2 ARFF blocks for context-rich representation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/5\">3.3 Multi-branch classification and regression</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/5\">3.4 Joint multi-loss function for global optimization of TSingNet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/5\">4 Experiments and analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/6\">4.1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/7\">4.2 Experimental setting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/7\">4.3 Results on the TT100K dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/8\">4.4 Results on the STSD dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/9\">4.5 Results on the DFG dataset</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/9\">4.6 Ablation studies</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/9\">4.6.1 Effect of different components</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/9\">4.6.2 Effect of AbFPN on multi-scale traffic signs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">4.6.3 Effect of ARFF on multi-scale traffic signs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">4.6.4 Effect of scale-aware loss on multi-scale traffic signs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">4.6.5 Visualization of feature maps and results</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">CRediT authorship contribution statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">Declaration of Competing Interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_E8U6YPAM/12\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:00Z","dateModified":"2023-03-22T18:52:22Z","uri":"http://zotero.org/users/11367251/items/E8U6YPAM","localPath":"/Users/reyvababtista/Projects/Papers/liuTSingNetScaleawarecontextrich2021.pdf","defaultPath":"files/243/liuTSingNetScaleawarecontextrich2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400410,"version":0},"$loki":118},{"itemID":337,"item":{"key":"ELCD3R8C","version":359,"itemType":"webpage","title":"Forest","date":"2023-03-27","url":"https://forest.beiwe.org/en/latest/","accessDate":"2023-03-27","websiteTitle":"Forest","creators":[{"firstName":"","lastName":"onnela-lab","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-27T22:06:24Z","dateModified":"2023-03-27T22:11:49Z","uri":"http://zotero.org/users/11367251/items/ELCD3R8C","itemID":337,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400411,"version":0},"$loki":119},{"itemID":279,"item":{"key":"ET3L2J63","version":279,"itemType":"journalArticle","title":"Smartphone-Based Ecological Momentary Assessment of Well-Being: A Systematic Review and Recommendations for Future Studies","abstractNote":"Feelings of well-being and happiness fluctuate over time and contexts. Ecological Momentary Assessment (EMA) studies can capture fluctuations in momentary behavior, and experiences by assessing these multiple times per day. Traditionally, EMA was performed using pen and paper. Recently, due to technological advances EMA studies can be conducted more easily with smartphones, a device ubiquitous in our society. The goal of this review was to evaluate the literature on smartphone-based EMA in well-being research in healthy subjects. The systematic review was conducted according to the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines. Searching PubMed and Web of Science, we identified 53 studies using smartphone-based EMA of wellbeing. Studies were heterogeneous in designs, context, and measures. The average study duration was 12.8 days, with well-being assessed 2–12 times per day. Half of the studies included objective data (e.g. location). Only 47.2% reported compliance, indicating a mean of 71.6%. Well-being fluctuated daily and weekly, with higher well-being in evenings and weekends. These fluctuations disappeared when location and activity were accounted for. On average, being in nature and physical activity relates to higher well-being. Working relates to lower well-being, but workplace and company do influence well-being. The important advantages of using smartphones instead of other devices to collect EMAs are the easier data collection and flexible designs. Smartphone-based EMA reach far larger maximum sample sizes and more easily add objective data to their designs than palm-top/ PDA studies. Smartphone-based EMA research is feasible to gain insight in well-being fluctuations and its determinants and offers the opportunity for parallel objective data collection. Most studies currently focus on group comparisons, while studies on individual differences in well-being patterns and fluctuations are lacking. We provide recommendations for future smartphone-based EMA research regarding measures, objective data and analyses.","date":"06/2021","language":"en","shortTitle":"Smartphone-Based Ecological Momentary Assessment of Well-Being","libraryCatalog":"DOI.org (Crossref)","url":"https://link.springer.com/10.1007/s10902-020-00324-7","accessDate":"2023-03-24T01:17:23Z","volume":"22","pages":"2361-2408","publicationTitle":"Journal of Happiness Studies","DOI":"10.1007/s10902-020-00324-7","issue":"5","journalAbbreviation":"J Happiness Stud","ISSN":"1389-4978, 1573-7780","creators":[{"firstName":"Lianne P.","lastName":"de Vries","creatorType":"author"},{"firstName":"Bart M. L.","lastName":"Baselmans","creatorType":"author"},{"firstName":"Meike","lastName":"Bartels","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-24T01:17:23Z","dateModified":"2023-03-24T01:17:24Z","uri":"http://zotero.org/users/11367251/items/ET3L2J63","itemID":279,"attachments":[{"key":"RJS7HHE2","version":281,"itemType":"attachment","title":"devriesSmartphoneBasedEcologicalMomentary2021.pdf","parentItem":"ET3L2J63","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/devriesSmartphoneBasedEcologicalMomentary2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RJS7HHE2/1\">Abstract</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/4\">1 Smartphone-Based EMA Studies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/4\">1.1 Advantages and Difficulties</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/5\">2 Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/5\">2.1 Systematic Review of Smartphone-Based EMA Studies</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/5\">2.1.1 Eligibility Criteria</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/5\">2.1.2 Information Source and Search Strategy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/6\">2.1.3 Study Selection and Data Extraction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/6\">2.1.4 Other EMA Data Collection Devices</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/6\">3 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/6\">3.1 Study Selection and Characteristics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/32\">3.2 Findings of Studies Using Other EMA Data Collection Devices</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/32\">4 Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/33\">4.1 Guidelines for the Use of Smartphone-Based EMA Designs in Well-Being Research</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/34\">4.1.1 Sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/35\">4.1.2 Measures of Well-Being</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/36\">4.1.3 Objective Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/37\">4.1.4 Schedule and Prompting Strategy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/37\">4.1.5 Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/38\">4.1.6 Response and Compliance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/38\">4.1.7 Analyses</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/39\">4.2 The Future of EMA in Well-Being</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/40\">5 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/40\">6 Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RJS7HHE2/41\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-24T01:17:31Z","dateModified":"2023-03-24T01:17:31Z","uri":"http://zotero.org/users/11367251/items/RJS7HHE2","localPath":"/Users/reyvababtista/Projects/Papers/devriesSmartphoneBasedEcologicalMomentary2021.pdf","defaultPath":"files/280/devriesSmartphoneBasedEcologicalMomentary2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400411,"version":0},"$loki":120},{"itemID":381,"item":{"key":"F2D7LNJF","version":424,"itemType":"journalArticle","title":"Improved YOLO v5 with balanced feature pyramid and attention module for traffic sign detection","abstractNote":"With the development of automatic driving technology, traffic sign detection has become a very important task. However, it is a challenging task because of the complex traffic sign scene and the small size of the target. In recent years, a number of convolutional neural network (CNN) based object detection methods have brought great progress to traffic sign detection. Considering the still high false detection rate, as well as the high time overhead and computational overhead, the effect is not satisfactory. Therefore, we employ lightweight network model YOLO v5 (You Only Look Once) as our work foundation. In this paper, we propose an improved YOLO v5 method by using balances feature pyramid structure and global context block to enhance the ability of feature fusion and feature extraction. To verify our proposed method, we have conducted a lot of comparative experiments on the challenging dataset Tsinghua-Tencent-100K (TT100K). The experimental results demonstrate that the mAP@.5 and mAP@.5:0.95 are improved by 1.9% and 2.1%, respectively.","date":"2022","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.matec-conferences.org/10.1051/matecconf/202235503023","accessDate":"2023-04-14T02:43:10Z","volume":"355","pages":"03023","publicationTitle":"MATEC Web of Conferences","DOI":"10.1051/matecconf/202235503023","journalAbbreviation":"MATEC Web Conf.","ISSN":"2261-236X","creators":[{"firstName":"Linfeng","lastName":"Jiang","creatorType":"author"},{"firstName":"Hui","lastName":"Liu","creatorType":"author"},{"firstName":"Hong","lastName":"Zhu","creatorType":"author"},{"firstName":"Guangjian","lastName":"Zhang","creatorType":"author"},{"firstName":"J.","lastName":"Zhu","creatorType":"editor"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-14T02:43:10Z","dateModified":"2023-04-14T02:43:11Z","uri":"http://zotero.org/users/11367251/items/F2D7LNJF","itemID":381,"attachments":[{"key":"VAF7MC4I","version":430,"itemType":"attachment","title":"jiangImprovedYOLOv52022.pdf","parentItem":"F2D7LNJF","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/jiangImprovedYOLOv52022.pdf","tags":[],"relations":{},"dateAdded":"2023-04-14T02:43:53Z","dateModified":"2023-04-14T02:43:53Z","uri":"http://zotero.org/users/11367251/items/VAF7MC4I","localPath":"/Users/reyvababtista/Projects/Papers/jiangImprovedYOLOv52022.pdf","defaultPath":"files/385/jiangImprovedYOLOv52022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400412,"version":0},"$loki":121},{"itemID":1623,"item":{"key":"F47JPWFH","version":1978,"itemType":"journalArticle","title":"Intramuscular FM radio transmitter of muscle potentials","date":"1965-12","language":"eng","libraryCatalog":"PubMed","extra":"PMID: 5855042","volume":"46","pages":"804-808","publicationTitle":"Archives of Physical Medicine and Rehabilitation","issue":"12","journalAbbreviation":"Arch Phys Med Rehabil","ISSN":"0003-9993","creators":[{"firstName":"R. C.","lastName":"Grotz","creatorType":"author"},{"firstName":"E. T.","lastName":"Yon","creatorType":"author"},{"firstName":"C.","lastName":"Long","creatorType":"author"},{"firstName":"W. H.","lastName":"Ko","creatorType":"author"}],"tags":[{"tag":"Electromyography","type":1},{"tag":"Humans","type":1},{"tag":"Radio","type":1},{"tag":"Telemetry","type":1}],"collections":[],"relations":{},"dateAdded":"2023-11-29T05:04:36Z","dateModified":"2023-11-29T05:04:36Z","uri":"http://zotero.org/users/11367251/items/F47JPWFH","itemID":1623,"attachments":[{"key":"4E3AL7M4","version":1977,"itemType":"attachment","title":"PubMed entry","url":"http://www.ncbi.nlm.nih.gov/pubmed/5855042","accessDate":"2023-11-29T05:04:36Z","parentItem":"F47JPWFH","linkMode":"linked_url","contentType":"text/html","charset":"","tags":[],"relations":{},"dateAdded":"2023-11-29T05:04:36Z","dateModified":"2023-11-29T05:04:36Z","uri":"http://zotero.org/users/11367251/items/4E3AL7M4"}],"notes":[]},"meta":{"revision":0,"created":1709832400413,"version":0},"$loki":122},{"itemID":999,"item":{"key":"F64AUV7H","version":1388,"itemType":"preprint","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System","abstractNote":"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.","date":"2020-10-06","language":"en","shortTitle":"UnifiedQA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2005.00700","accessDate":"2023-11-07T23:32:13Z","extra":"arXiv:2005.00700 [cs]","repository":"arXiv","archiveID":"arXiv:2005.00700","creators":[{"firstName":"Daniel","lastName":"Khashabi","creatorType":"author"},{"firstName":"Sewon","lastName":"Min","creatorType":"author"},{"firstName":"Tushar","lastName":"Khot","creatorType":"author"},{"firstName":"Ashish","lastName":"Sabharwal","creatorType":"author"},{"firstName":"Oyvind","lastName":"Tafjord","creatorType":"author"},{"firstName":"Peter","lastName":"Clark","creatorType":"author"},{"firstName":"Hannaneh","lastName":"Hajishirzi","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T23:32:13Z","dateModified":"2023-11-07T23:32:13Z","uri":"http://zotero.org/users/11367251/items/F64AUV7H","itemID":999,"attachments":[{"key":"FSWKCBM5","version":1389,"itemType":"attachment","title":"khashabiUnifiedQACrossingFormat2020.pdf","parentItem":"F64AUV7H","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/khashabiUnifiedQACrossingFormat2020.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T23:32:25Z","dateModified":"2023-11-07T23:32:25Z","uri":"http://zotero.org/users/11367251/items/FSWKCBM5","localPath":"/Users/reyvababtista/Projects/Papers/khashabiUnifiedQACrossingFormat2020.pdf","defaultPath":"files/1002/khashabiUnifiedQACrossingFormat2020.pdf"}],"notes":[{"key":"8Y2TXBJ8","version":1388,"itemType":"note","parentItem":"F64AUV7H","note":"Comment: EMNLP 2020 (Findings)","tags":[],"relations":{},"dateAdded":"2023-11-07T23:32:13Z","dateModified":"2023-11-07T23:32:13Z","uri":"http://zotero.org/users/11367251/items/8Y2TXBJ8"}]},"meta":{"revision":0,"created":1709832400413,"version":0},"$loki":123},{"itemID":483,"item":{"key":"FD84S52V","version":695,"itemType":"journalArticle","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models","abstractNote":"Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique testbeds for exploring the translation challenges of turning literacy into numeracy. Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic. The statistical analysis of four complex datasets described here combines arithmetic manipulations that cannot be memorized or encoded by simple rules. The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding. For example, the work highlights cases for descriptive statistics on in-memory datasets that the LLM initially loads from memory or generates randomly using python libraries. The resulting exploratory data analysis showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression. To extend the model's testable range, the research deletes and appends random rows such that recall alone cannot explain emergent numeracy.","date":"1/2023","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"David","lastName":"Noever","creatorType":"author"},{"firstName":"Forrest","lastName":"McKee","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-16T01:08:52Z","dateModified":"2023-06-22T15:27:14Z","uri":"http://zotero.org/users/11367251/items/FD84S52V","itemID":483,"attachments":[{"key":"KH8CG743","version":646,"itemType":"attachment","title":"noeverNumeracyLiteracyData.pdf","parentItem":"FD84S52V","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/noeverNumeracyLiteracyData.pdf","tags":[],"relations":{},"dateAdded":"2023-06-16T01:09:04Z","dateModified":"2023-06-16T01:09:04Z","uri":"http://zotero.org/users/11367251/items/KH8CG743","localPath":"/Users/reyvababtista/Projects/Papers/noeverNumeracyLiteracyData.pdf","defaultPath":"files/485/noeverNumeracyLiteracyData.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400414,"version":0},"$loki":124},{"itemID":1661,"item":{"key":"FDUR8VAR","version":2083,"itemType":"journalArticle","title":"A logical calculus of the ideas immanent in nervous activity","date":"12/1943","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://link.springer.com/10.1007/BF02478259","accessDate":"2023-12-04T02:45:17Z","volume":"5","pages":"115-133","publicationTitle":"The Bulletin of Mathematical Biophysics","DOI":"10.1007/BF02478259","issue":"4","journalAbbreviation":"Bulletin of Mathematical Biophysics","ISSN":"0007-4985, 1522-9602","creators":[{"firstName":"Warren S.","lastName":"McCulloch","creatorType":"author"},{"firstName":"Walter","lastName":"Pitts","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T02:45:17Z","dateModified":"2023-12-04T02:45:17Z","uri":"http://zotero.org/users/11367251/items/FDUR8VAR","itemID":1661,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400414,"version":0},"$loki":125},{"itemID":607,"item":{"key":"FVA9E4RK","version":811,"itemType":"preprint","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs","abstractNote":"A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a ﬂexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-speciﬁc architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical ﬂow estimation with no explicit mechanisms for multiscale correspondence.","date":"2022-03-15","language":"en","shortTitle":"Perceiver IO","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2107.14795","accessDate":"2023-09-05T13:45:45Z","extra":"arXiv:2107.14795 [cs, eess]","repository":"arXiv","archiveID":"arXiv:2107.14795","creators":[{"firstName":"Andrew","lastName":"Jaegle","creatorType":"author"},{"firstName":"Sebastian","lastName":"Borgeaud","creatorType":"author"},{"firstName":"Jean-Baptiste","lastName":"Alayrac","creatorType":"author"},{"firstName":"Carl","lastName":"Doersch","creatorType":"author"},{"firstName":"Catalin","lastName":"Ionescu","creatorType":"author"},{"firstName":"David","lastName":"Ding","creatorType":"author"},{"firstName":"Skanda","lastName":"Koppula","creatorType":"author"},{"firstName":"Daniel","lastName":"Zoran","creatorType":"author"},{"firstName":"Andrew","lastName":"Brock","creatorType":"author"},{"firstName":"Evan","lastName":"Shelhamer","creatorType":"author"},{"firstName":"Olivier","lastName":"Hénaff","creatorType":"author"},{"firstName":"Matthew M.","lastName":"Botvinick","creatorType":"author"},{"firstName":"Andrew","lastName":"Zisserman","creatorType":"author"},{"firstName":"Oriol","lastName":"Vinyals","creatorType":"author"},{"firstName":"Joāo","lastName":"Carreira","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:45:45Z","dateModified":"2023-09-05T13:45:45Z","uri":"http://zotero.org/users/11367251/items/FVA9E4RK","itemID":607,"attachments":[{"key":"XDATA48W","version":818,"itemType":"attachment","title":"jaeglePerceiverIOGeneral2022.pdf","parentItem":"FVA9E4RK","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/jaeglePerceiverIOGeneral2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XDATA48W/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XDATA48W/3\">3 The Perceiver IO architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/4\">3.1 Encoding, processing, and decoding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/5\">3.2 Decoding the latent representation with a query array</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XDATA48W/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/6\">4.1 Language</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/7\">4.2 Optical flow</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/8\">4.3 Multimodal autoencoding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/9\">4.4 ImageNet, StarCraft II, and AudioSet</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/9\">5 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XDATA48W/17\">A Image classification</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/18\">A.1 Details of ImageNet training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/19\">A.2 Large-scale pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/20\">A.3 2D convolutional preprocessing on ImageNet</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/20\">B StarCraft II </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/21\">C AudioSet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/21\">D FLOPs calculation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XDATA48W/22\">E Architectural details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/23\">E.1 Attention module internals</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/23\">E.2 Computational complexity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/23\">E.3 Using the decoder for classification / regression</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_XDATA48W/24\">F Language: additional details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/24\">F.1 Other Tokenizer-free models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/24\">F.2 Architecture details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/25\">F.3 MLM pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/25\">F.4 GLUE Finetuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/25\">F.5 Ablation on the number of latents</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/26\">G Positional encodings for image and audio experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/26\">H Optical Flow: additional details and results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XDATA48W/29\">I Multimodal autoencoding: additional details</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:31Z","dateModified":"2023-09-05T13:47:32Z","uri":"http://zotero.org/users/11367251/items/XDATA48W","localPath":"/Users/reyvababtista/Projects/Papers/jaeglePerceiverIOGeneral2022.pdf","defaultPath":"files/622/jaeglePerceiverIOGeneral2022.pdf"}],"notes":[{"key":"LDHTJWSA","version":811,"itemType":"note","parentItem":"FVA9E4RK","note":"Comment: ICLR 2022 camera ready. Code: https://dpmd.ai/perceiver-code","tags":[],"relations":{},"dateAdded":"2023-09-05T13:45:45Z","dateModified":"2023-09-05T13:45:45Z","uri":"http://zotero.org/users/11367251/items/LDHTJWSA"}]},"meta":{"revision":0,"created":1709832400415,"version":0},"$loki":126},{"itemID":383,"item":{"key":"FZ7JX4BY","version":427,"itemType":"report","title":"DC-YOLOv8: Small Size Object Detection Algorithm Based on Camera Sensor","abstractNote":"Traditional camera sensors rely on human eyes for observation. However, the human eye is prone to fatigue when observing targets of different sizes for a long time in complex scenes, and human cognition is limited, which often leads to judgment errors and greatly reduces the efﬁciency. Target recognition technology is an important technology to judge the target category in camera sensor. In order to solve this problem, a small size target detection algorithm for special scenarios was proposed by this paper. Its advantage is that this algorithm not only has higher precision for small size target detection, but also can ensure that the detection accuracy of each size is not lower than the existing algorithm. In this paper, a new down-sampling method was proposed, which could better preserve the context feature information. The feature fusion network was improved to effectively combine shallow information and deep information. A new network structure was proposed to effectively improve the detection accuracy of the model. In terms of accuracy, it is better than: YOLOX, YOLOXR, YOLOv3, scaled YOLOv5, YOLOv7-Tiny and YOLOv8.Three authoritative public data sets were used in this experiment: a) On Visdron data sets (small size targets), DC-YOLOv8 is 2.5% more accurate than YOLOv8. b) On Tinyperson data sets (minimal size targets), DC-YOLOv8 is 1% more accurate than YOLOv8. c) On PASCAL VOC2007 data sets (Normal size target), DC-YOLOv8 is 0.5% more accurate than YOLOv8.","date":"2023-04-07","language":"en","shortTitle":"DC-YOLOv8","libraryCatalog":"DOI.org (Crossref)","url":"https://www.preprints.org/manuscript/202304.0124/v1","accessDate":"2023-04-14T02:43:21Z","extra":"DOI: 10.20944/preprints202304.0124.v1","reportType":"preprint","institution":"Engineering","creators":[{"firstName":"Haitong","lastName":"Lou","creatorType":"author"},{"firstName":"Xuehu","lastName":"Duan","creatorType":"author"},{"firstName":"Junmei","lastName":"Guo","creatorType":"author"},{"firstName":"Haiying","lastName":"Liu","creatorType":"author"},{"firstName":"Jason","lastName":"Gu","creatorType":"author"},{"firstName":"Lingyun","lastName":"Bi","creatorType":"author"},{"firstName":"Haonan","lastName":"Chen","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-14T02:43:21Z","dateModified":"2023-04-14T02:43:22Z","uri":"http://zotero.org/users/11367251/items/FZ7JX4BY","itemID":383,"attachments":[{"key":"6EQPATS3","version":430,"itemType":"attachment","title":"louDCYOLOv8SmallSize2023.pdf","parentItem":"FZ7JX4BY","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/louDCYOLOv8SmallSize2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_6EQPATS3/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6EQPATS3/2\">Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/2\">The reason for choosing YOLOv8 as the baseline</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/3\">The network structure of YOLOv8</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6EQPATS3/4\">The proposed DC-YOLOv8 algorithm</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/4\">A modified efficient downsampling method</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/5\">Improved feature fusion method</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/6\">The proposed network structure</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_6EQPATS3/6\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/6\">Experimental platform</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/6\">Valuation index</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/8\">Experimental results analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/8\">Comparison of experiments with different sizes objects</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/11\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_6EQPATS3/11\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-14T02:43:53Z","dateModified":"2023-04-14T02:43:53Z","uri":"http://zotero.org/users/11367251/items/6EQPATS3","localPath":"/Users/reyvababtista/Projects/Papers/louDCYOLOv8SmallSize2023.pdf","defaultPath":"files/384/louDCYOLOv8SmallSize2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400416,"version":0},"$loki":127},{"itemID":929,"item":{"key":"G9N395LL","version":1216,"itemType":"blogPost","title":"Mobile-first indexing has landed - thanks for all your support","date":"10/31/2023","url":"https://developers.google.com/search/blog/2023/10/mobile-first-is-here","accessDate":"2023-12-04","blogTitle":"Mobile-first indexing has landed - thanks for all your support","creators":[{"name":"John Mueller","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-05T14:44:25Z","dateModified":"2023-11-05T14:45:14Z","uri":"http://zotero.org/users/11367251/items/G9N395LL","itemID":929,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400416,"version":0},"$loki":128},{"itemID":321,"item":{"key":"GCDKV6WH","version":325,"itemType":"journalArticle","title":"The digital biomarker discovery pipeline: An open-source software platform for the development of digital biomarkers using mHealth and wearables data","abstractNote":"Introduction: Digital health is rapidly expanding due to surging healthcare costs, deteriorating health outcomes, and the growing prevalence and accessibility of mobile health (mHealth) and wearable technology. Data from Biometric Monitoring Technologies (BioMeTs), including mHealth and wearables, can be transformed into digital biomarkers that act as indicators of health outcomes and can be used to diagnose and monitor a number of chronic diseases and conditions. There are many challenges faced by digital biomarker development, including a lack of regulatory oversight, limited funding opportunities, general mistrust of sharing personal data, and a shortage of open-source data and code. Further, the process of transforming data into digital biomarkers is computationally expensive, and standards and validation methods in digital biomarker research are lacking. Methods: In order to provide a collaborative, standardized space for digital biomarker research and validation, we present the first comprehensive, open-source software platform for end-to-end digital biomarker development: The Digital Biomarker Discovery Pipeline (DBDP). Results: Here, we detail the general DBDP framework as well as three robust modules within the DBDP that have been developed for specific digital biomarker discovery use cases. Conclusions: The clear need for such a platform will accelerate the DBDP’s adoption as the industry standard for digital biomarker development and will support its role as the epicenter of digital biomarker collaboration and exploration.","date":"2021","language":"en","shortTitle":"The digital biomarker discovery pipeline","libraryCatalog":"DOI.org (Crossref)","url":"https://www.cambridge.org/core/product/identifier/S2059866120005117/type/journal_article","accessDate":"2023-03-26T22:14:39Z","volume":"5","pages":"e19","publicationTitle":"Journal of Clinical and Translational Science","DOI":"10.1017/cts.2020.511","issue":"1","journalAbbreviation":"J. Clin. Trans. Sci.","ISSN":"2059-8661","creators":[{"firstName":"Brinnae","lastName":"Bent","creatorType":"author"},{"firstName":"Ke","lastName":"Wang","creatorType":"author"},{"firstName":"Emilia","lastName":"Grzesiak","creatorType":"author"},{"firstName":"Chentian","lastName":"Jiang","creatorType":"author"},{"firstName":"Yuankai","lastName":"Qi","creatorType":"author"},{"firstName":"Yihang","lastName":"Jiang","creatorType":"author"},{"firstName":"Peter","lastName":"Cho","creatorType":"author"},{"firstName":"Kyle","lastName":"Zingler","creatorType":"author"},{"firstName":"Felix Ikponmwosa","lastName":"Ogbeide","creatorType":"author"},{"firstName":"Arthur","lastName":"Zhao","creatorType":"author"},{"firstName":"Ryan","lastName":"Runge","creatorType":"author"},{"firstName":"Ida","lastName":"Sim","creatorType":"author"},{"firstName":"Jessilyn","lastName":"Dunn","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T22:14:39Z","dateModified":"2023-03-26T22:14:39Z","uri":"http://zotero.org/users/11367251/items/GCDKV6WH","itemID":321,"attachments":[{"key":"S57N3CMZ","version":325,"itemType":"attachment","title":"bentdigitalbiomarkerdiscovery2021.pdf","parentItem":"GCDKV6WH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bentdigitalbiomarkerdiscovery2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_S57N3CMZ/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/3\">Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/3\">Software Specifications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/3\">DBDP Landscape</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/3\">Using the DBDP</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/3\">Contributing to the DBDP</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/4\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/4\">General DBDP</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/4\">DBDP Module 1: RHR</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/5\">DBDP Module 2: Sleep and Circadian Rhythms</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/5\">DBDP Module 3: cgmquantify for CGM Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/5\">DBDP for Education</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/5\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/7\">Code Availability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_S57N3CMZ/7\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-26T22:14:44Z","dateModified":"2023-03-26T22:14:44Z","uri":"http://zotero.org/users/11367251/items/S57N3CMZ","localPath":"/Users/reyvababtista/Projects/Papers/bentdigitalbiomarkerdiscovery2021.pdf","defaultPath":"files/322/bentdigitalbiomarkerdiscovery2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400417,"version":0},"$loki":129},{"itemID":1663,"item":{"key":"GFXBBG2L","version":2085,"itemType":"book","title":"The Organization of Behavior","date":"2005-4-11","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.taylorfrancis.com/books/9781135631918","accessDate":"2023-12-04T03:06:34Z","extra":"DOI: 10.4324/9781410612403","publisher":"Psychology Press","ISBN":"978-1-4106-1240-3","edition":"0","creators":[{"firstName":"D.O.","lastName":"Hebb","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T03:06:34Z","dateModified":"2023-12-04T03:06:34Z","uri":"http://zotero.org/users/11367251/items/GFXBBG2L","itemID":1663,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400417,"version":0},"$loki":130},{"itemID":1398,"item":{"key":"GMNM929L","version":1534,"itemType":"preprint","title":"Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning","abstractNote":"Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across $\\sim$22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces.","date":"2021-08-06","shortTitle":"Screen2Words","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2108.03353","accessDate":"2023-11-08T22:39:37Z","extra":"arXiv:2108.03353 [cs]","repository":"arXiv","archiveID":"arXiv:2108.03353","creators":[{"firstName":"Bryan","lastName":"Wang","creatorType":"author"},{"firstName":"Gang","lastName":"Li","creatorType":"author"},{"firstName":"Xin","lastName":"Zhou","creatorType":"author"},{"firstName":"Zhourong","lastName":"Chen","creatorType":"author"},{"firstName":"Tovi","lastName":"Grossman","creatorType":"author"},{"firstName":"Yang","lastName":"Li","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Human-Computer Interaction","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:39:37Z","dateModified":"2023-11-08T22:39:37Z","uri":"http://zotero.org/users/11367251/items/GMNM929L","itemID":1398,"attachments":[{"key":"3K7RE9GR","version":1537,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2108.03353","accessDate":"2023-11-08T22:39:52Z","parentItem":"GMNM929L","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2108.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:39:52Z","dateModified":"2023-11-08T22:39:52Z","uri":"http://zotero.org/users/11367251/items/3K7RE9GR","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/3K7RE9GR/2108.html","defaultPath":"files/1402/2108.html"},{"key":"4RYE5U4P","version":1535,"itemType":"attachment","title":"wangScreen2WordsAutomaticMobile2021.pdf","parentItem":"GMNM929L","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/wangScreen2WordsAutomaticMobile2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:39:46Z","dateModified":"2023-11-08T22:39:46Z","uri":"http://zotero.org/users/11367251/items/4RYE5U4P","localPath":"/Users/reyvababtista/Projects/Papers/wangScreen2WordsAutomaticMobile2021.pdf","defaultPath":"files/1401/wangScreen2WordsAutomaticMobile2021.pdf"}],"notes":[{"key":"4XCJAXLP","version":1534,"itemType":"note","parentItem":"GMNM929L","note":"Comment: UIST'21","tags":[],"relations":{},"dateAdded":"2023-11-08T22:39:37Z","dateModified":"2023-11-08T22:39:37Z","uri":"http://zotero.org/users/11367251/items/4XCJAXLP"}]},"meta":{"revision":0,"created":1709832400418,"version":0},"$loki":131},{"itemID":514,"item":{"key":"GNFRQ9F7","version":672,"itemType":"journalArticle","title":"AugGPT: Leveraging ChatGPT for Text Data Augmentation","abstractNote":"Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can’t ensure the correct labeling of the generated data (lacking faithfulness) or can’t ensure sufﬁcient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classiﬁcation tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Haixing","lastName":"Dai","creatorType":"author"},{"firstName":"Zhengliang","lastName":"Liu","creatorType":"author"},{"firstName":"Wenxiong","lastName":"Liao","creatorType":"author"},{"firstName":"Xiaoke","lastName":"Huang","creatorType":"author"},{"firstName":"Yihan","lastName":"Cao","creatorType":"author"},{"firstName":"Zihao","lastName":"Wu","creatorType":"author"},{"firstName":"Lin","lastName":"Zhao","creatorType":"author"},{"firstName":"Shaochen","lastName":"Xu","creatorType":"author"},{"firstName":"Wei","lastName":"Liu","creatorType":"author"},{"firstName":"Ninghao","lastName":"Liu","creatorType":"author"},{"firstName":"Sheng","lastName":"Li","creatorType":"author"},{"firstName":"Dajiang","lastName":"Zhu","creatorType":"author"},{"firstName":"Hongmin","lastName":"Cai","creatorType":"author"},{"firstName":"Lichao","lastName":"Sun","creatorType":"author"},{"firstName":"Quanzheng","lastName":"Li","creatorType":"author"},{"firstName":"Dinggang","lastName":"Shen","creatorType":"author"},{"firstName":"Tianming","lastName":"Liu","creatorType":"author"},{"firstName":"Xiang","lastName":"Li","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-22T15:09:25Z","dateModified":"2023-06-22T15:09:25Z","uri":"http://zotero.org/users/11367251/items/GNFRQ9F7","itemID":514,"attachments":[{"key":"5ULTKAS8","version":674,"itemType":"attachment","title":"daiAugGPTLeveragingChatGPT.pdf","parentItem":"GNFRQ9F7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/daiAugGPTLeveragingChatGPT.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5ULTKAS8/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/2\">2 Related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/2\">2.1 Data Augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/2\">2.2 Few-shot Learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/3\">2.3 Very Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/4\">2.4 ChatGPT: Present and Future</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">3 Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">3.1 Amazon dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">3.2 Symptoms Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">3.3 PubMed20k Dataset</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">4 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">4.1 Overall Framework</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/5\">4.2 Data Augmentation with ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/6\">4.3 Few-shot Text Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/7\">4.4 Baseline Methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/7\">4.5 Prompt Design</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/7\">4.6 Evaluation Metrics</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/7\">4.6.1 Embedding Similarity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/8\">4.6.2 TransRate</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/8\">4.7 Direct Classification Performance by ChatGPT</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/9\">5 Experiment Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/9\">5.1 Classification Performance Comparison</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/9\">5.2 Evaluation of Augmented Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/10\">5.3 Performance Comparison with ChatGPT</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/10\">6 Conclusion and Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5ULTKAS8/10\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-22T15:09:37Z","dateModified":"2023-06-22T15:09:38Z","uri":"http://zotero.org/users/11367251/items/5ULTKAS8","localPath":"/Users/reyvababtista/Projects/Papers/daiAugGPTLeveragingChatGPT.pdf","defaultPath":"files/515/daiAugGPTLeveragingChatGPT.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400418,"version":0},"$loki":132},{"itemID":293,"item":{"key":"GP6M7U9Z","version":298,"itemType":"journalArticle","title":"A Framework for Competencies for the Use of Mobile Technologies in Psychiatry and Medicine: Scoping Review","abstractNote":"Background: To ensure quality care, clinicians need skills, knowledge, and attitudes related to technology that can be measured. Objective: This paper sought out competencies for mobile technologies and/or an approach to define them. Methods: A scoping review was conducted to answer the following research question, “What skills are needed for clinicians and trainees to provide quality care via mHealth, have they been published, and how can they be made measurable and reproducible to teach and assess them?” The review was conducted in accordance with the 6-stage scoping review process starting with a keyword search in PubMed/Medical Literature Analysis and Retrieval System Online, APA PsycNET, Cochrane, EMBASE, PsycINFO, Web of Science, and Scopus. The literature search focused on keywords in 4 concept areas: (1) competencies, (2) mobile technologies, (3) telemedicine mode, and (4) health. Moreover, 2 authors independently, in parallel, screened the search results for potentially relevant studies based on titles and abstracts. The authors reviewed the full-text articles for final inclusion based on inclusion/exclusion criteria. Inclusion criteria were keywords used from concept area 1 (competencies) and 2 (mobile technologies) and either 3 (telemedicine mode) or 4 (health). Exclusion criteria included, but were not limited to, keywords used from a concept area in isolation, discussion of skills abstractly, outline or listing of what clinicians need without detail, and listing immeasurable behaviors.","date":"2020-2-21","language":"en","shortTitle":"A Framework for Competencies for the Use of Mobile Technologies in Psychiatry and Medicine","libraryCatalog":"DOI.org (Crossref)","url":"http://mhealth.jmir.org/2020/2/e12229/","accessDate":"2023-03-26T21:25:08Z","volume":"8","pages":"e12229","publicationTitle":"JMIR mHealth and uHealth","DOI":"10.2196/12229","issue":"2","journalAbbreviation":"JMIR Mhealth Uhealth","ISSN":"2291-5222","creators":[{"firstName":"Donald","lastName":"Hilty","creatorType":"author"},{"firstName":"Steven","lastName":"Chan","creatorType":"author"},{"firstName":"John","lastName":"Torous","creatorType":"author"},{"firstName":"John","lastName":"Luo","creatorType":"author"},{"firstName":"Robert","lastName":"Boland","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:25:08Z","dateModified":"2023-03-26T21:25:08Z","uri":"http://zotero.org/users/11367251/items/GP6M7U9Z","itemID":293,"attachments":[{"key":"V7Z6QP3W","version":315,"itemType":"attachment","title":"hiltyFrameworkCompetenciesUse2020.pdf","parentItem":"GP6M7U9Z","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hiltyFrameworkCompetenciesUse2020.pdf","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:22Z","dateModified":"2023-03-26T21:29:22Z","uri":"http://zotero.org/users/11367251/items/V7Z6QP3W","localPath":"/Users/reyvababtista/Projects/Papers/hiltyFrameworkCompetenciesUse2020.pdf","defaultPath":"files/306/hiltyFrameworkCompetenciesUse2020.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400419,"version":0},"$loki":133},{"itemID":1139,"item":{"key":"GPG5CUCX","version":1450,"itemType":"preprint","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge","abstractNote":"Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OKVQA, where the image content is not sufﬁcient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difﬁcult, and large compared to previous knowledgebased VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain. See http://okvqa.allenai.org to download and browse the dataset.","date":"2019-09-04","language":"en","shortTitle":"OK-VQA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1906.00067","accessDate":"2023-11-08T16:00:42Z","extra":"arXiv:1906.00067 [cs]","repository":"arXiv","archiveID":"arXiv:1906.00067","creators":[{"firstName":"Kenneth","lastName":"Marino","creatorType":"author"},{"firstName":"Mohammad","lastName":"Rastegari","creatorType":"author"},{"firstName":"Ali","lastName":"Farhadi","creatorType":"author"},{"firstName":"Roozbeh","lastName":"Mottaghi","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T16:00:42Z","dateModified":"2023-11-08T16:00:43Z","uri":"http://zotero.org/users/11367251/items/GPG5CUCX","itemID":1139,"attachments":[{"key":"AQWVF9S5","version":1453,"itemType":"attachment","title":"marinoOKVQAVisualQuestion2019.pdf","parentItem":"GPG5CUCX","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/marinoOKVQAVisualQuestion2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T16:00:50Z","dateModified":"2023-11-08T16:00:50Z","uri":"http://zotero.org/users/11367251/items/AQWVF9S5","localPath":"/Users/reyvababtista/Projects/Papers/marinoOKVQAVisualQuestion2019.pdf","defaultPath":"files/1141/marinoOKVQAVisualQuestion2019.pdf"}],"notes":[{"key":"ZASME7S5","version":1450,"itemType":"note","parentItem":"GPG5CUCX","note":"Comment: CVPR 2019","tags":[],"relations":{},"dateAdded":"2023-11-08T16:00:42Z","dateModified":"2023-11-08T16:00:42Z","uri":"http://zotero.org/users/11367251/items/ZASME7S5"}]},"meta":{"revision":0,"created":1709832400419,"version":0},"$loki":134},{"itemID":1422,"item":{"key":"GTVCIKEC","version":1559,"itemType":"preprint","title":"Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions","abstractNote":"When people observe events, they are able to abstract key information and build concise summaries of what is happening. These summaries include contextual and semantic information describing the important high-level details (what, where, who and how) of the observed event and exclude background information that is deemed unimportant to the observer. With this in mind, the descriptions people generate for videos of different dynamic events can greatly improve our understanding of the key information of interest in each video. These descriptions can be captured in captions that provide expanded attributes for video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing us to gain new insight into what people find important or necessary to summarize specific events. Existing caption datasets for video understanding are either small in scale or restricted to a specific domain. To address this, we present the Spoken Moments (S-MiT) dataset of 500k spoken captions each attributed to a unique short video depicting a broad range of different events. We collect our descriptions using audio recordings to ensure that they remain as natural and concise as possible while allowing us to scale the size of a large classification dataset. In order to utilize our proposed dataset, we present a novel Adaptive Mean Margin (AMM) approach to contrastive learning and evaluate our models on video/caption retrieval on multiple datasets. We show that our AMM approach consistently improves our results and that models trained on our Spoken Moments dataset generalize better than those trained on other video-caption datasets.","date":"2021-05-10","shortTitle":"Spoken Moments","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2105.04489","accessDate":"2023-11-08T22:45:40Z","extra":"arXiv:2105.04489 [cs, eess]","repository":"arXiv","archiveID":"arXiv:2105.04489","creators":[{"firstName":"Mathew","lastName":"Monfort","creatorType":"author"},{"firstName":"SouYoung","lastName":"Jin","creatorType":"author"},{"firstName":"Alexander","lastName":"Liu","creatorType":"author"},{"firstName":"David","lastName":"Harwath","creatorType":"author"},{"firstName":"Rogerio","lastName":"Feris","creatorType":"author"},{"firstName":"James","lastName":"Glass","creatorType":"author"},{"firstName":"Aude","lastName":"Oliva","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:45:40Z","dateModified":"2023-11-08T22:45:40Z","uri":"http://zotero.org/users/11367251/items/GTVCIKEC","itemID":1422,"attachments":[{"key":"83LG2A87","version":1564,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2105.04489","accessDate":"2023-11-08T22:46:09Z","parentItem":"GTVCIKEC","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2105.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:46:09Z","dateModified":"2023-11-08T22:46:09Z","uri":"http://zotero.org/users/11367251/items/83LG2A87","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/83LG2A87/2105.html","defaultPath":"files/1428/2105.html"},{"key":"PFHYMLY6","version":1560,"itemType":"attachment","title":"monfortSpokenMomentsLearning2021.pdf","parentItem":"GTVCIKEC","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/monfortSpokenMomentsLearning2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:46:04Z","dateModified":"2023-11-08T22:46:04Z","uri":"http://zotero.org/users/11367251/items/PFHYMLY6","localPath":"/Users/reyvababtista/Projects/Papers/monfortSpokenMomentsLearning2021.pdf","defaultPath":"files/1425/monfortSpokenMomentsLearning2021.pdf"}],"notes":[{"key":"QD4QSNPF","version":1559,"itemType":"note","parentItem":"GTVCIKEC","note":"Comment: To appear at CVPR 2021","tags":[],"relations":{},"dateAdded":"2023-11-08T22:45:40Z","dateModified":"2023-11-08T22:45:40Z","uri":"http://zotero.org/users/11367251/items/QD4QSNPF"}]},"meta":{"revision":0,"created":1709832400420,"version":0},"$loki":135},{"itemID":1151,"item":{"key":"GUIY94F4","version":1466,"itemType":"preprint","title":"Captioning Images Taken by People Who Are Blind","abstractNote":"While an important problem in the vision community is to design algorithms that can automatically caption images, few publicly-available datasets for algorithm development directly address the interests of real users. Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of over 39,000 images originating from people who are blind that are each paired with five captions. We analyze this dataset to (1) characterize the typical captions, (2) characterize the diversity of content found in the images, and (3) compare its content to that found in eight popular vision datasets. We also analyze modern image captioning algorithms to identify what makes this new dataset challenging for the vision community. We publicly-share the dataset with captioning challenge instructions at https://vizwiz.org","date":"2020-07-15","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2002.08565","accessDate":"2023-11-08T16:05:28Z","extra":"arXiv:2002.08565 [cs]","repository":"arXiv","archiveID":"arXiv:2002.08565","creators":[{"firstName":"Danna","lastName":"Gurari","creatorType":"author"},{"firstName":"Yinan","lastName":"Zhao","creatorType":"author"},{"firstName":"Meng","lastName":"Zhang","creatorType":"author"},{"firstName":"Nilavra","lastName":"Bhattacharya","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T16:05:28Z","dateModified":"2023-11-08T16:05:28Z","uri":"http://zotero.org/users/11367251/items/GUIY94F4","itemID":1151,"attachments":[{"key":"PQLP5I2E","version":1470,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2002.08565","accessDate":"2023-11-08T16:06:50Z","parentItem":"GUIY94F4","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2002.html","tags":[],"relations":{},"dateAdded":"2023-11-08T16:06:50Z","dateModified":"2023-11-08T16:06:50Z","uri":"http://zotero.org/users/11367251/items/PQLP5I2E","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/PQLP5I2E/2002.html","defaultPath":"files/1154/2002.html"},{"key":"K3LZN4XL","version":1467,"itemType":"attachment","title":"gurariCaptioningImagesTaken2020.pdf","parentItem":"GUIY94F4","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gurariCaptioningImagesTaken2020.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_K3LZN4XL/1\">Captioning Images Taken by People Who Are Blind</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-08T16:06:45Z","dateModified":"2023-11-08T16:06:46Z","uri":"http://zotero.org/users/11367251/items/K3LZN4XL","localPath":"/Users/reyvababtista/Projects/Papers/gurariCaptioningImagesTaken2020.pdf","defaultPath":"files/1153/gurariCaptioningImagesTaken2020.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400421,"version":0},"$loki":136},{"itemID":364,"item":{"key":"GWTFC6CJ","version":405,"itemType":"preprint","title":"Object Detection in 20 Years: A Survey","abstractNote":"Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today's object detection technique as a revolution driven by deep learning, then back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This paper extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.","date":"2023-01-18","language":"en","shortTitle":"Object Detection in 20 Years","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1905.05055","accessDate":"2023-04-11T03:07:43Z","extra":"arXiv:1905.05055 [cs]","repository":"arXiv","archiveID":"arXiv:1905.05055","creators":[{"firstName":"Zhengxia","lastName":"Zou","creatorType":"author"},{"firstName":"Keyan","lastName":"Chen","creatorType":"author"},{"firstName":"Zhenwei","lastName":"Shi","creatorType":"author"},{"firstName":"Yuhong","lastName":"Guo","creatorType":"author"},{"firstName":"Jieping","lastName":"Ye","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-11T03:07:43Z","dateModified":"2023-04-11T03:07:43Z","uri":"http://zotero.org/users/11367251/items/GWTFC6CJ","itemID":364,"attachments":[{"key":"UA4EUIVI","version":407,"itemType":"attachment","title":"zouObjectDetection202023.pdf","parentItem":"GWTFC6CJ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zouObjectDetection202023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UA4EUIVI/1\">I Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/1\">II Object Detection in 20 Years</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/2\">II-A A Road Map of Object Detection</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/2\">II-A1 Milestones: Traditional Detectors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/2\">II-A2 Milestones: CNN based Two-stage Detectors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/3\">II-A3 Milestones: CNN based One-stage Detectors</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/4\">II-B Object Detection Datasets and Metrics</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/4\">II-B1 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/5\">II-B2 Metrics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/6\">II-C Technical Evolution in Object Detection</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/6\">II-C1 Technical Evolution of Multi-Scale Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/7\">II-C2 Technical Evolution of Context Priming</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/8\">II-C3 Technical Evolution of Hard Negative Mining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/8\">II-C4 Technical Evolution of Loss Function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/9\">II-C5 Technical Evolution of Non-Maximum Suppression</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/9\">III Speed-Up of Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-A Feature Map Shared Computation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-B Cascaded Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-C Network Pruning and Quantification</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-D Lightweight Network Design</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-D1 Factorizing Convolutions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-D2 Group Convolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-D3 Depth-wise Separable Convolution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-D4 Bottle-neck Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-D5 Detection with NAS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-E Numerical Acceleration</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/10\">III-E1 Speed Up with Integral Image</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/11\">III-E2 Speed Up in Frequency Domain</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/11\">III-E3 Vector Quantization</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/11\">IV Recent Advances in Object Detection</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/12\">IV-A Beyond Sliding Window Detection</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/12\">IV-B Robust Detection of Rotation and Scale Changes</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/12\">IV-B1 Rotation Robust Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/12\">IV-B2 Scale Robust Detection</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/12\">IV-C Detection with Better Backbones</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/13\">IV-D Improvements of Localization</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/13\">IV-D1 Bounding Box Refinement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/13\">IV-D2 New Loss Functions for Accurate Localization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/13\">IV-E Learning with Segmentation Loss</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/13\">IV-F Adversarial Training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/13\">IV-G Weakly Supervised Object Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/14\">IV-H Detection with Domain Adaptation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UA4EUIVI/14\">V Conclusion and Future Directions</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-11T03:07:50Z","dateModified":"2023-04-11T03:07:50Z","uri":"http://zotero.org/users/11367251/items/UA4EUIVI","localPath":"/Users/reyvababtista/Projects/Papers/zouObjectDetection202023.pdf","defaultPath":"files/366/zouObjectDetection202023.pdf"}],"notes":[{"key":"BPYNS8L8","version":405,"itemType":"note","parentItem":"GWTFC6CJ","note":"Comment: Accepted by Proceedings of the IEEE","tags":[],"relations":{},"dateAdded":"2023-04-11T03:07:43Z","dateModified":"2023-04-11T03:07:43Z","uri":"http://zotero.org/users/11367251/items/BPYNS8L8"}]},"meta":{"revision":0,"created":1709832400422,"version":0},"$loki":137},{"itemID":1642,"item":{"key":"GZ5GJWQY","version":2060,"itemType":"preprint","title":"GPT-4 Technical Report","abstractNote":"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.","date":"2023-03-27","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2303.08774","accessDate":"2023-12-01T22:13:12Z","extra":"arXiv:2303.08774 [cs]","repository":"arXiv","archiveID":"arXiv:2303.08774","creators":[{"firstName":"","lastName":"OpenAI","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-12-01T22:13:12Z","dateModified":"2023-12-01T22:13:13Z","uri":"http://zotero.org/users/11367251/items/GZ5GJWQY","itemID":1642,"attachments":[{"key":"JZ8ABJ2Z","version":2063,"itemType":"attachment","title":"openaiGPT4TechnicalReport2023b.pdf","parentItem":"GZ5GJWQY","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/openaiGPT4TechnicalReport2023b.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/2\">2 Scope and Limitations of this Technical Report</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/2\">3 Predictable Scaling</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/2\">3.1 Loss Prediction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/2\">3.2 Scaling of Capabilities on HumanEval</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/4\">4 Capabilities</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/8\">4.1 Visual Inputs</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/10\">5 Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/11\">6 Risks &amp;amp; mitigations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/14\">7 Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/24\">A Exam Benchmark Methodology</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/24\">A.1 Sourcing.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/24\">A.2 Prompting: multiple-choice</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/24\">A.3 Prompting: free-response</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/25\">A.4 Images</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/25\">A.5 Scoring</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/25\">A.6 Codeforces rating</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/25\">A.7 Model snapshot details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/26\">A.8 Example few-shot prompts</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/28\">B Impact of RLHF on capability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/28\">C Contamination on professional and academic exams</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/29\">D Contamination on academic benchmarks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/29\">E GSM-8K in GPT-4 training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/29\">F Multilingual MMLU</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/29\">G Examples of GPT-4 Visual Input</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_JZ8ABJ2Z/40\">H System Card</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-12-01T22:13:17Z","dateModified":"2023-12-01T22:13:18Z","uri":"http://zotero.org/users/11367251/items/JZ8ABJ2Z","localPath":"/Users/reyvababtista/Projects/Papers/openaiGPT4TechnicalReport2023b.pdf","defaultPath":"files/1644/openaiGPT4TechnicalReport2023b.pdf"}],"notes":[{"key":"KQTSLM9M","version":2060,"itemType":"note","parentItem":"GZ5GJWQY","note":"Comment: 100 pages","tags":[],"relations":{},"dateAdded":"2023-12-01T22:13:12Z","dateModified":"2023-12-01T22:13:12Z","uri":"http://zotero.org/users/11367251/items/KQTSLM9M"}]},"meta":{"revision":0,"created":1709832400423,"version":0},"$loki":138},{"itemID":946,"item":{"key":"H3DKQ6C2","version":1301,"itemType":"book","title":"Inventing the Cloud Century: How Cloudiness Keeps Changing Our Life, Economy and Technology","abstractNote":"This book combines the three dimensions of technology, society and economy to explore the advent of today's cloud ecosystems as successors to older service ecosystems based on networks. Further, it describes the shifting of services to the cloud as a long-term trend that is still progressing rapidly. The book adopts a comprehensive perspective on the key success factors for the technology - compelling business models and ecosystems including private, public and national organizations. The authors explore the evolution of service ecosystems, describe the similarities and differences, and analyze the way they have created and changed industries. Lastly, based on the current status of cloud computing and related technologies like virtualization, the internet of things, fog computing, big data and analytics, cognitive computing and blockchain, the authors provide a revealing outlook on the possibilities of future technologies, the future of the internet, and the potential impacts on business and society","date":"2018","shortTitle":"Inventing the Cloud Century","libraryCatalog":"Library of Congress ISBN","callNumber":"658.4038","extra":"DOI: 10.1007/978-3-319-61161-7","place":"Cham","publisher":"Springer International Publishing : Imprint: Springer","ISBN":"978-3-319-61161-7","edition":"1st ed. 2018","numPages":"1","creators":[{"firstName":"Marcus","lastName":"Oppitz","creatorType":"author"},{"firstName":"Peter","lastName":"Tomsu","creatorType":"author"}],"tags":[{"tag":"Big data","type":1},{"tag":"Big Data/Analytics","type":1},{"tag":"Business IT Infrastructure","type":1},{"tag":"Computer organization","type":1},{"tag":"Computer science","type":1},{"tag":"Computer Systems Organization and Communication Networks","type":1},{"tag":"Computers and civilization","type":1},{"tag":"Computers and Society","type":1},{"tag":"e-Business/e-Commerce","type":1},{"tag":"Electronic commerce","type":1},{"tag":"Management information systems","type":1},{"tag":"Management of Computing and Information Systems","type":1}],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:06:27Z","dateModified":"2023-11-07T02:06:27Z","uri":"http://zotero.org/users/11367251/items/H3DKQ6C2","itemID":946,"attachments":[{"key":"2ZLNLYVC","version":1301,"itemType":"attachment","title":"oppitzInventingCloudCentury2018.pdf","parentItem":"H3DKQ6C2","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/oppitzInventingCloudCentury2018.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2ZLNLYVC/5\">Preface</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2ZLNLYVC/7\">Contents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2ZLNLYVC/17\">Abbreviations</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-07T02:06:30Z","dateModified":"2023-11-07T02:06:30Z","uri":"http://zotero.org/users/11367251/items/2ZLNLYVC","localPath":"/Users/reyvababtista/Projects/Papers/oppitzInventingCloudCentury2018.pdf","defaultPath":"files/949/oppitzInventingCloudCentury2018.pdf"}],"notes":[{"key":"SEC4YLB5","version":1300,"itemType":"note","parentItem":"H3DKQ6C2","note":"Introduction -- A Short History of Service Ecosystems -- Early Information Network Services -- Making of Digital Computers -- Networks for Sharing and Connecting -- Managing Virtual Storage -- From Physical to Virtual Services -- Software Defined Virtual Networks -- Building the Internet -- World Wide Web -- Cloud Computing -- Building Cloud Business and Ecosystems -- Creating Innovation -- Security and Privacy Challenges -- Changes in Society and Politics -- Internet of Things -- Fog Computing -- Big Data Analytics -- Future Technologies of the Cloud Century -- New Paradigms and Big Disruptive Things -- Summary","tags":[],"relations":{},"dateAdded":"2023-11-07T02:06:27Z","dateModified":"2023-11-07T02:06:27Z","uri":"http://zotero.org/users/11367251/items/SEC4YLB5"}]},"meta":{"revision":0,"created":1709832400425,"version":0},"$loki":139},{"itemID":935,"item":{"key":"H55H4SCT","version":1252,"itemType":"blogPost","title":"Introducing the Knowledge Graph: things, not strings","date":"05/16/2012","url":"https://blog.google/products/search/introducing-knowledge-graph-things-not/","accessDate":"2023-11-05","blogTitle":"Introducing the Knowledge Graph: things, not strings","creators":[{"name":"Amit Singhal","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T05:05:00Z","dateModified":"2023-11-06T05:05:33Z","uri":"http://zotero.org/users/11367251/items/H55H4SCT","itemID":935,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400426,"version":0},"$loki":140},{"itemID":1685,"item":{"key":"H7LRW4Y5","version":2143,"itemType":"blogPost","title":"Easy Introduction to Reinforcement Learning","date":"July 14, 2023","url":"https://www.scribbr.com/ai-tools/reinforcement-learning/","accessDate":"2023-12-05","blogTitle":"Easy Introduction to Reinforcement Learning","creators":[{"name":"Kassiani Nikolopoulou","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:35:34Z","dateModified":"2023-12-05T21:36:35Z","uri":"http://zotero.org/users/11367251/items/H7LRW4Y5","itemID":1685,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400427,"version":0},"$loki":141},{"itemID":27,"item":{"key":"H7WZLJ3J","version":200,"itemType":"journalArticle","title":"MR-CNN: A Multi-Scale Region-Based Convolutional Neural Network for Small Traffic Sign Recognition","abstractNote":"Small trafﬁc sign recognition is a challenging problem in computer vision, and its accuracy is important to the safety of intelligent transportation systems (ITS). In this paper, we propose the multiscale region-based convolutional neural network (MR-CNN). At the detection stage, MR-CNN uses a multiscale deconvolution operation to up-sample the features of the deeper convolution layers and concatenates them to those of the shallow layer to construct the fused feature map. The fused feature map has the ability to generate fewer region proposals and achieve higher recall values. At the classiﬁcation stage, we leverage the multi-scale contextual regions to exploit the information surrounding a given object proposal and construct the fused feature for the fully connected layers. The fused feature map inside the region proposal network (RPN) focuses primarily on improving the image resolution and semantic information for small trafﬁc sign detection, while outside the RPN, the fused feature enhances the feature representation by leveraging the contextual information. Finally, we evaluated MR-CNN on the largest dataset, TsinghuaTencent 100K, which is suitable for our problem and more challenging than the GTSDB and GTSRB datasets. The ﬁnal experimental results indicate that the MR-CNN is superior at detecting small trafﬁc signs, and that it achieves the state-of-the-art performance compared with other methods.","date":"2019","language":"en","shortTitle":"MR-CNN","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8701699/","accessDate":"2023-03-20T14:25:13Z","volume":"7","pages":"57120-57128","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2019.2913882","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Zhigang","lastName":"Liu","creatorType":"author"},{"firstName":"Juan","lastName":"Du","creatorType":"author"},{"firstName":"Feng","lastName":"Tian","creatorType":"author"},{"firstName":"Jiazheng","lastName":"Wen","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/BUID2RQN"]},"dateAdded":"2023-03-20T14:25:13Z","dateModified":"2023-03-22T17:03:41Z","uri":"http://zotero.org/users/11367251/items/H7WZLJ3J","itemID":27,"attachments":[{"key":"5644VPBB","version":241,"itemType":"attachment","title":"liuMRCNNMultiScaleRegionBased2019.pdf","parentItem":"H7WZLJ3J","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liuMRCNNMultiScaleRegionBased2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5644VPBB/1\">INTRODUCTION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/2\">RELATED WORK</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5644VPBB/3\">OUR PROPOSED APPROACH</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/3\">MULTI-SCALE FUSED FEATURE MAP</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/4\">MULTI-SCALE CONTEXTUAL INFORMATION</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5644VPBB/5\">EXPERIMENTS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/5\">DATASETS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/5\">DETECTION PERFORMANCE</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/7\">ABLATION ANALYSIS</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/8\">CONCLUSION AND FUTURE WORK</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/8\">REFERENCES</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5644VPBB/9\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/9\">ZHIGANG LIU</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/9\">JUAN DU</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/9\">FENG TIAN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5644VPBB/9\">JIAZHENG WEN</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:04Z","dateModified":"2023-03-22T18:52:23Z","uri":"http://zotero.org/users/11367251/items/5644VPBB","localPath":"/Users/reyvababtista/Projects/Papers/liuMRCNNMultiScaleRegionBased2019.pdf","defaultPath":"files/249/liuMRCNNMultiScaleRegionBased2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400427,"version":0},"$loki":142},{"itemID":1384,"item":{"key":"HBG4CERJ","version":1517,"itemType":"preprint","title":"DocVQA: A Dataset for VQA on Document Images","abstractNote":"We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org","date":"2021-01-05","shortTitle":"DocVQA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2007.00398","accessDate":"2023-11-08T22:36:54Z","extra":"arXiv:2007.00398 [cs]","repository":"arXiv","archiveID":"arXiv:2007.00398","creators":[{"firstName":"Minesh","lastName":"Mathew","creatorType":"author"},{"firstName":"Dimosthenis","lastName":"Karatzas","creatorType":"author"},{"firstName":"C. V.","lastName":"Jawahar","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Information Retrieval","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:36:54Z","dateModified":"2023-11-08T22:36:54Z","uri":"http://zotero.org/users/11367251/items/HBG4CERJ","itemID":1384,"attachments":[{"key":"J449BPTW","version":1521,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2007.00398","accessDate":"2023-11-08T22:37:09Z","parentItem":"HBG4CERJ","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2007.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:37:09Z","dateModified":"2023-11-08T22:37:09Z","uri":"http://zotero.org/users/11367251/items/J449BPTW","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/J449BPTW/2007.html","defaultPath":"files/1388/2007.html"},{"key":"A8K2BHWG","version":1518,"itemType":"attachment","title":"mathewDocVQADatasetVQA2021.pdf","parentItem":"HBG4CERJ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/mathewDocVQADatasetVQA2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:37:03Z","dateModified":"2023-11-08T22:37:03Z","uri":"http://zotero.org/users/11367251/items/A8K2BHWG","localPath":"/Users/reyvababtista/Projects/Papers/mathewDocVQADatasetVQA2021.pdf","defaultPath":"files/1387/mathewDocVQADatasetVQA2021.pdf"}],"notes":[{"key":"DA98FJMX","version":1517,"itemType":"note","parentItem":"HBG4CERJ","note":"Comment: accepted at WACV 2021","tags":[],"relations":{},"dateAdded":"2023-11-08T22:36:54Z","dateModified":"2023-11-08T22:36:54Z","uri":"http://zotero.org/users/11367251/items/DA98FJMX"}]},"meta":{"revision":0,"created":1709832400428,"version":0},"$loki":143},{"itemID":1831,"item":{"key":"HMTAX78S","version":2294,"itemType":"conferencePaper","title":"Dinus Intelligent Assistance (DINA) Chatbot for University Admission Services","date":"9/2018","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8549797/","accessDate":"2024-01-19T15:17:40Z","place":"Semarang","publisher":"IEEE","ISBN":"978-1-5386-7486-4","pages":"417-423","proceedingsTitle":"2018 International Seminar on Application for Technology of Information and Communication","conferenceName":"2018 International Seminar on Application for Technology of Information and Communication (iSemantic)","DOI":"10.1109/ISEMANTIC.2018.8549797","creators":[{"firstName":"Heru","lastName":"Agus Santoso","creatorType":"author"},{"firstName":"Nurul","lastName":"Anisa Sri Winarsih","creatorType":"author"},{"firstName":"Edy","lastName":"Mulyanto","creatorType":"author"},{"firstName":"Galuh","lastName":"Wilujeng Saraswati","creatorType":"author"},{"firstName":"Septian","lastName":"Enggar Sukmana","creatorType":"author"},{"firstName":"Supriadi","lastName":"Rustad","creatorType":"author"},{"firstName":"Muhammad","lastName":"Syaifur Rohman","creatorType":"author"},{"firstName":"Adhitya","lastName":"Nugraha","creatorType":"author"},{"firstName":"Fahri","lastName":"Firdausillah","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:17:40Z","dateModified":"2024-01-19T15:17:40Z","uri":"http://zotero.org/users/11367251/items/HMTAX78S","itemID":1831,"attachments":[{"key":"I9L43P7G","version":2295,"itemType":"attachment","title":"agussantosoDinusIntelligentAssistance2018.pdf","parentItem":"HMTAX78S","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/agussantosoDinusIntelligentAssistance2018.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:18:00Z","dateModified":"2024-01-19T15:18:00Z","uri":"http://zotero.org/users/11367251/items/I9L43P7G","localPath":"/Users/reyvababtista/Projects/Papers/agussantosoDinusIntelligentAssistance2018.pdf","defaultPath":"files/1833/agussantosoDinusIntelligentAssistance2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400429,"version":0},"$loki":144},{"itemID":291,"item":{"key":"HPHLU77J","version":295,"itemType":"journalArticle","title":"Real-time Mobile Monitoring of the Dynamic Associations Among Motor Activity, Energy, Mood, and Sleep in Adults With Bipolar Disorder","abstractNote":"OBJECTIVES To examine the directional associations among motor activity, energy, mood, and sleep using mobile monitoring in a community-identified sample, and to evaluate whether these within-day associations differ between people with a history of bipolar or other mood disorders and controls without mood disorders. DESIGN, SETTING, AND PARTICIPANTS This study used a nested case-control design of 242 adults, a subsample of a community-based sample of adults. Probands were recruited by mail from the greater Washington, DC, metropolitan area from January 2005 to June 2013. Enrichment of the sample for mood disorders was provided by volunteers or referrals from the National Institutes of Health Clinical Center or by participants in the National Institute of Mental Health Mood and Anxiety Disorders Program. The inclusion criteria were the ability to speak English, availability to participate, and consent to contact at least 2 living first-degree relatives. Data analysis was performed from June 2013 through July 2018. MAIN OUTCOMES AND MEASURES Motor activity and sleep duration data were obtained from minute-to-minute activity counts from an actigraphy device worn on the nondominant wrist for 2 weeks. Mood and energy levels were assessed by subjective analogue ratings on the ecological momentary assessment (using a personal digital assistant) by participants 4 times per day for 2 weeks.\nRESULTS Of the total 242 participants, 92 (38.1%) were men and 150 (61.9%) were women, with a mean (SD) age of 48 (16.9) years. Among the participants, 54 (22.3%) had bipolar disorder (25 with bipolar I; 29 with bipolar II), 91 (37.6%) had major depressive disorder, and 97 (40.1%) were controls with no history of mood disorders. A unidirectional association was found between motor activity and subjective mood level (β = –0.018, P = .04). Bidirectional associations were observed between motor activity (β = 0.176; P = .03) and subjective energy level (β = 0.027; P = .03) as well as between motor activity (β = –0.027; P = .04) and sleep duration (β = –0.154; P = .04). Greater cross-domain reactivity was observed in bipolar disorder across all outcomes, including motor activity, sleep, mood, and energy.\nCONCLUSIONS AND RELEVANCE These findings suggest that interventions focused on motor activity and energy may have greater efficacy than current approaches that target depressed mood; both active and passive tracking of multiple regulatory systems are important in designing therapeutic targets.","date":"2019-02-01","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://archpsyc.jamanetwork.com/article.aspx?doi=10.1001/jamapsychiatry.2018.3546","accessDate":"2023-03-26T21:24:38Z","volume":"76","pages":"190","publicationTitle":"JAMA Psychiatry","DOI":"10.1001/jamapsychiatry.2018.3546","issue":"2","journalAbbreviation":"JAMA Psychiatry","ISSN":"2168-622X","creators":[{"firstName":"Kathleen Ries","lastName":"Merikangas","creatorType":"author"},{"firstName":"Joel","lastName":"Swendsen","creatorType":"author"},{"firstName":"Ian B.","lastName":"Hickie","creatorType":"author"},{"firstName":"Lihong","lastName":"Cui","creatorType":"author"},{"firstName":"Haochang","lastName":"Shou","creatorType":"author"},{"firstName":"Alison K.","lastName":"Merikangas","creatorType":"author"},{"firstName":"Jihui","lastName":"Zhang","creatorType":"author"},{"firstName":"Femke","lastName":"Lamers","creatorType":"author"},{"firstName":"Ciprian","lastName":"Crainiceanu","creatorType":"author"},{"firstName":"Nora D.","lastName":"Volkow","creatorType":"author"},{"firstName":"Vadim","lastName":"Zipunnikov","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:24:38Z","dateModified":"2023-03-26T21:24:38Z","uri":"http://zotero.org/users/11367251/items/HPHLU77J","itemID":291,"attachments":[{"key":"RVUEW5BA","version":315,"itemType":"attachment","title":"merikangasRealtimeMobileMonitoring2019.pdf","parentItem":"HPHLU77J","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/merikangasRealtimeMobileMonitoring2019.pdf","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:24Z","dateModified":"2023-03-26T21:29:24Z","uri":"http://zotero.org/users/11367251/items/RVUEW5BA","localPath":"/Users/reyvababtista/Projects/Papers/merikangasRealtimeMobileMonitoring2019.pdf","defaultPath":"files/308/merikangasRealtimeMobileMonitoring2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400429,"version":0},"$loki":145},{"itemID":594,"item":{"key":"HPZXM3FG","version":794,"itemType":"journalArticle","title":"Graph neural networks: A review of methods and applications","abstractNote":"Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.","date":"2020","language":"en","shortTitle":"Graph neural networks","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S2666651021000012","accessDate":"2023-09-05T13:42:27Z","volume":"1","pages":"57-81","publicationTitle":"AI Open","DOI":"10.1016/j.aiopen.2021.01.001","journalAbbreviation":"AI Open","ISSN":"26666510","creators":[{"firstName":"Jie","lastName":"Zhou","creatorType":"author"},{"firstName":"Ganqu","lastName":"Cui","creatorType":"author"},{"firstName":"Shengding","lastName":"Hu","creatorType":"author"},{"firstName":"Zhengyan","lastName":"Zhang","creatorType":"author"},{"firstName":"Cheng","lastName":"Yang","creatorType":"author"},{"firstName":"Zhiyuan","lastName":"Liu","creatorType":"author"},{"firstName":"Lifeng","lastName":"Wang","creatorType":"author"},{"firstName":"Changcheng","lastName":"Li","creatorType":"author"},{"firstName":"Maosong","lastName":"Sun","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:42:27Z","dateModified":"2023-09-05T13:42:27Z","uri":"http://zotero.org/users/11367251/items/HPZXM3FG","itemID":594,"attachments":[{"key":"8Y4VM76Z","version":817,"itemType":"attachment","title":"zhouGraphneuralnetworks2020.pdf","parentItem":"HPZXM3FG","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhouGraphneuralnetworks2020.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_8Y4VM76Z/1\">1. Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/2\">2. General design pipeline of GNNs</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/3\">2.1. Find graph structure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/3\">2.2. Specify graph type and scale</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/3\">2.3. Design loss function</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/3\">2.4. Build model using computational modules</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/4\">3. Instantiations of computational modules</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/4\">3.1. Propagation modules - convolution operator</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/4\">3.1.1. Spectral approaches</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/6\">3.1.2. Basic spatial approaches</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/6\">3.1.3. Attention-based spatial approaches</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/7\">3.1.4. General frameworks for spatial approaches</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/7\">3.2. Propagation modules - recurrent operator</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/8\">3.2.1. Convergence-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/8\">3.2.2. Gate-based methods</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/9\">3.3. Propagation modules - skip connection</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/9\">3.4. Sampling modules</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/9\">3.4.1. Node sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/9\">3.4.2. Layer sampling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/9\">3.4.3. Subgraph sampling</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/10\">3.5. Pooling modules</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/10\">3.5.1. Direct pooling modules</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/10\">3.5.2. Hierarchical pooling modules</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4. Variants considering graph type and scale</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4.1. Directed graphs</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4.2. Heterogeneous graphs</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4.2.1. Meta-path-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4.2.2. Edge-based methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4.2.3. Methods for relational graphs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/11\">4.2.4. Methods for multiplex graphs</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">4.3. Dynamic graphs</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">4.4. Other graph types</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">4.4.1. Hypergraphs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">4.4.2. Signed graphs</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">4.5. Large graphs</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">5. Variants for different training settings</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">5.1. Graph auto-encoders</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/12\">5.2. Contrastive learning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/13\">6. A design example of GNN</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/13\">7. Analyses of GNNs</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/13\">7.1. Theoretical aspect</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/14\">7.1.1. Graph signal processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/14\">7.1.2. Generalization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/14\">7.1.3. Expressivity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">7.1.4. Invariance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">7.1.5. Transferability</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">7.1.6. Label efficiency</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">7.2. Empirical aspect</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">7.2.1. Evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">7.2.2. Benchmarks</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">8. Applications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">8.1. Structural scenarios</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/15\">8.1.1. Graph mining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/16\">8.1.2. Physics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/16\">8.1.3. Chemistry and biology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/16\">8.1.4. Knowledge graph</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/16\">8.1.5. Generative models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/17\">8.1.6. Combinatorial optimization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/17\">8.1.7. Traffic networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/17\">8.1.8. Recommendation systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/17\">8.1.9. Other Applications in structural scenarios</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/17\">8.2. Non-structural scenarios</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/18\">8.2.1. Image</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/18\">8.2.2. Text</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/19\">9. Open problems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/19\">10. Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/19\">Declaration of competing interest</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/19\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/20\">Appendix A. Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/20\">Appendix B. Implementations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_8Y4VM76Z/21\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:30Z","dateModified":"2023-09-05T13:47:30Z","uri":"http://zotero.org/users/11367251/items/8Y4VM76Z","localPath":"/Users/reyvababtista/Projects/Papers/zhouGraphneuralnetworks2020.pdf","defaultPath":"files/620/zhouGraphneuralnetworks2020.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400430,"version":0},"$loki":146},{"itemID":754,"item":{"key":"HV3Z23VX","version":1016,"itemType":"preprint","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model","abstractNote":"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pre-training tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.","date":"2023-06-05","language":"en","shortTitle":"PaLI","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2209.06794","accessDate":"2023-10-05T17:30:51Z","extra":"arXiv:2209.06794 [cs]","repository":"arXiv","archiveID":"arXiv:2209.06794","creators":[{"firstName":"Xi","lastName":"Chen","creatorType":"author"},{"firstName":"Xiao","lastName":"Wang","creatorType":"author"},{"firstName":"Soravit","lastName":"Changpinyo","creatorType":"author"},{"firstName":"A. J.","lastName":"Piergiovanni","creatorType":"author"},{"firstName":"Piotr","lastName":"Padlewski","creatorType":"author"},{"firstName":"Daniel","lastName":"Salz","creatorType":"author"},{"firstName":"Sebastian","lastName":"Goodman","creatorType":"author"},{"firstName":"Adam","lastName":"Grycner","creatorType":"author"},{"firstName":"Basil","lastName":"Mustafa","creatorType":"author"},{"firstName":"Lucas","lastName":"Beyer","creatorType":"author"},{"firstName":"Alexander","lastName":"Kolesnikov","creatorType":"author"},{"firstName":"Joan","lastName":"Puigcerver","creatorType":"author"},{"firstName":"Nan","lastName":"Ding","creatorType":"author"},{"firstName":"Keran","lastName":"Rong","creatorType":"author"},{"firstName":"Hassan","lastName":"Akbari","creatorType":"author"},{"firstName":"Gaurav","lastName":"Mishra","creatorType":"author"},{"firstName":"Linting","lastName":"Xue","creatorType":"author"},{"firstName":"Ashish","lastName":"Thapliyal","creatorType":"author"},{"firstName":"James","lastName":"Bradbury","creatorType":"author"},{"firstName":"Weicheng","lastName":"Kuo","creatorType":"author"},{"firstName":"Mojtaba","lastName":"Seyedhosseini","creatorType":"author"},{"firstName":"Chao","lastName":"Jia","creatorType":"author"},{"firstName":"Burcu Karagol","lastName":"Ayan","creatorType":"author"},{"firstName":"Carlos","lastName":"Riquelme","creatorType":"author"},{"firstName":"Andreas","lastName":"Steiner","creatorType":"author"},{"firstName":"Anelia","lastName":"Angelova","creatorType":"author"},{"firstName":"Xiaohua","lastName":"Zhai","creatorType":"author"},{"firstName":"Neil","lastName":"Houlsby","creatorType":"author"},{"firstName":"Radu","lastName":"Soricut","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-05T17:30:51Z","dateModified":"2023-10-05T17:30:51Z","uri":"http://zotero.org/users/11367251/items/HV3Z23VX","itemID":754,"attachments":[{"key":"HSQ5QU2R","version":1019,"itemType":"attachment","title":"chenPaLIJointlyScaledMultilingual2023.pdf","parentItem":"HV3Z23VX","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenPaLIJointlyScaledMultilingual2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_HSQ5QU2R/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/3\">The PaLI Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/3\">Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/4\">Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/5\">Model Training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/5\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/5\">Image Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/6\">Visual Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/7\">Language-understanding Capabilities</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/8\">Zero-shot Image Classification</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/8\">Model Scaling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/9\">Ablation studies</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/17\">PaLI model additional information</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/17\">PaLI model details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/18\">The Pretraining Task Mixture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/20\">Fine-Tuning details</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/20\">WebLI Dataset Details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/21\">Additional experimental results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/21\">Language-only evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/21\">Additional scaling results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/23\">Additional ablations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/24\">Evaluation of PaLI's Visual Component: ViT-e</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/24\">Results on TextCaps, TextVQA and VizWiz-QA without Detected OCR as Input</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/25\">Detailed VTAB Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/25\">Top 5 Accuracy on Zero-shot ImageNet Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/25\">More Zero-shot Image-text Retrieval Results on Crossmodal-3600</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/25\">Model Fairness, Biases, and Other Potential Issues</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/28\">Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/28\">PaLI Model Card</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_HSQ5QU2R/31\">WebLI Datasheet</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-10-05T17:31:18Z","dateModified":"2023-10-05T17:31:18Z","uri":"http://zotero.org/users/11367251/items/HSQ5QU2R","localPath":"/Users/reyvababtista/Projects/Papers/chenPaLIJointlyScaledMultilingual2023.pdf","defaultPath":"files/756/chenPaLIJointlyScaledMultilingual2023.pdf"}],"notes":[{"key":"6GUAEWCD","version":1014,"itemType":"note","parentItem":"HV3Z23VX","note":"Comment: ICLR 2023 (Notable-top-5%)","tags":[],"relations":{},"dateAdded":"2023-10-05T17:30:51Z","dateModified":"2023-10-05T17:30:51Z","uri":"http://zotero.org/users/11367251/items/6GUAEWCD"}]},"meta":{"revision":0,"created":1709832400430,"version":0},"$loki":147},{"itemID":1679,"item":{"key":"HWNXLGT3","version":2127,"itemType":"blogPost","title":"Hierarchical clustering","date":"2023","url":"https://en.wikipedia.org/w/index.php?title=Hierarchical_clustering&oldid=1179469786","accessDate":"2023-12-05","blogTitle":"Hierarchical clustering","creators":[{"name":"Wikipedia contributors","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:05:11Z","dateModified":"2023-12-05T21:06:56Z","uri":"http://zotero.org/users/11367251/items/HWNXLGT3","itemID":1679,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400431,"version":0},"$loki":148},{"itemID":1687,"item":{"key":"I3AAS5CE","version":2147,"itemType":"journalArticle","title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis","abstractNote":"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models","date":"2023","shortTitle":"SDXL","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2307.01952","accessDate":"2023-12-06T23:02:41Z","rights":"Creative Commons Attribution 4.0 International","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.2307.01952","creators":[{"firstName":"Dustin","lastName":"Podell","creatorType":"author"},{"firstName":"Zion","lastName":"English","creatorType":"author"},{"firstName":"Kyle","lastName":"Lacey","creatorType":"author"},{"firstName":"Andreas","lastName":"Blattmann","creatorType":"author"},{"firstName":"Tim","lastName":"Dockhorn","creatorType":"author"},{"firstName":"Jonas","lastName":"Müller","creatorType":"author"},{"firstName":"Joe","lastName":"Penna","creatorType":"author"},{"firstName":"Robin","lastName":"Rombach","creatorType":"author"}],"tags":[{"tag":"Artificial Intelligence (cs.AI)","type":1},{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:02:41Z","dateModified":"2023-12-06T23:02:41Z","uri":"http://zotero.org/users/11367251/items/I3AAS5CE","itemID":1687,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400432,"version":0},"$loki":149},{"itemID":297,"item":{"key":"I7SFQEGE","version":304,"itemType":"journalArticle","title":"Toward Increasing Engagement in Substance Use Data Collection: Development of the Substance Abuse Research Assistant App and Protocol for a Microrandomized Trial Using Adolescents and Emerging Adults","abstractNote":"Background: Substance use is an alarming public health issue associated with significant morbidity and mortality. Adolescents and emerging adults are at particularly high risk because substance use typically initiates and peaks during this developmental period. Mobile health apps are a promising data collection and intervention delivery tool for substance-using youth as most teens and young adults own a mobile phone. However, engagement with data collection for most mobile health applications is low, and often, large fractions of users stop providing data after a week of use.","date":"2018-07-18","language":"en","shortTitle":"Toward Increasing Engagement in Substance Use Data Collection","libraryCatalog":"DOI.org (Crossref)","url":"http://www.researchprotocols.org/2018/7/e166/","accessDate":"2023-03-26T21:25:48Z","volume":"7","pages":"e166","publicationTitle":"JMIR Research Protocols","DOI":"10.2196/resprot.9850","issue":"7","journalAbbreviation":"JMIR Res Protoc","ISSN":"1929-0748","creators":[{"firstName":"Mashfiqui","lastName":"Rabbi","creatorType":"author"},{"firstName":"Meredith","lastName":"Philyaw Kotov","creatorType":"author"},{"firstName":"Rebecca","lastName":"Cunningham","creatorType":"author"},{"firstName":"Erin E","lastName":"Bonar","creatorType":"author"},{"firstName":"Inbal","lastName":"Nahum-Shani","creatorType":"author"},{"firstName":"Predrag","lastName":"Klasnja","creatorType":"author"},{"firstName":"Maureen","lastName":"Walton","creatorType":"author"},{"firstName":"Susan","lastName":"Murphy","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:25:48Z","dateModified":"2023-03-26T21:25:48Z","uri":"http://zotero.org/users/11367251/items/I7SFQEGE","itemID":297,"attachments":[{"key":"QCVFKMGL","version":315,"itemType":"attachment","title":"rabbiIncreasingEngagementSubstance2018.pdf","parentItem":"I7SFQEGE","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/rabbiIncreasingEngagementSubstance2018.pdf","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:25Z","dateModified":"2023-03-26T21:29:25Z","uri":"http://zotero.org/users/11367251/items/QCVFKMGL","localPath":"/Users/reyvababtista/Projects/Papers/rabbiIncreasingEngagementSubstance2018.pdf","defaultPath":"files/310/rabbiIncreasingEngagementSubstance2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400439,"version":0},"$loki":150},{"itemID":1508,"item":{"key":"IHLBV9ES","version":1712,"itemType":"blogPost","title":"Google Reveals “Hummingbird” Search Algorithm, Other Changes At 15th Birthday Event","date":"2013/09/26","url":"https://searchengineland.com/google-birthday-event-172791","accessDate":"2023-11-09","blogTitle":"Google Reveals “Hummingbird” Search Algorithm, Other Changes At 15th Birthday Event","creators":[{"name":"Danny Sullivan","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:29:41Z","dateModified":"2023-11-09T21:30:21Z","uri":"http://zotero.org/users/11367251/items/IHLBV9ES","itemID":1508,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400440,"version":0},"$loki":151},{"itemID":954,"item":{"key":"ISNEP7YX","version":1332,"itemType":"blogPost","title":"Say hello to bangs.","date":"2023","url":"https://duckduckgo.com/bangs","accessDate":"2023-11-06","blogTitle":"Say hello to bangs.","creators":[{"name":"DuckDuckGo","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:26:11Z","dateModified":"2023-11-07T02:26:42Z","uri":"http://zotero.org/users/11367251/items/ISNEP7YX","itemID":954,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400440,"version":0},"$loki":152},{"itemID":74,"item":{"key":"IYQRG95G","version":201,"itemType":"journalArticle","title":"Reproducible Analysis Pipeline for Data Streams: Open-Source Software to Process Data Collected With Mobile Devices","abstractNote":"Smartphone and wearable devices are widely used in behavioral and clinical research to collect longitudinal data that, along with ground truth data, are used to create models of human behavior. Mobile sensing researchers often program data processing and analysis code from scratch even though many research teams collect data from similar mobile sensors, platforms, and devices. This leads to signiﬁcant inefﬁciency in not being able to replicate and build on others’ work, inconsistency in quality of code and results, and lack of transparency when code is not shared alongside publications. We provide an overview of Reproducible Analysis Pipeline for Data Streams (RAPIDS), a reproducible pipeline to standardize the preprocessing, feature extraction, analysis, visualization, and reporting of data streams coming from mobile sensors. RAPIDS is formed by a group of R and Python scripts that are executed on top of reproducible virtual environments, orchestrated by a workﬂow management system, and organized following a consistent ﬁle structure for data science projects. We share open source, documented, extensible and tested code to preprocess, extract, and visualize behavioral features from data collected with any Android or iOS smartphone sensing app as well as Fitbit and Empatica wearable devices. RAPIDS allows researchers to process mobile sensor data in a rigorous and reproducible way. This saves time and effort during the data analysis phase of a project and facilitates sharing analysis workﬂows alongside publications.","date":"2021-11-18","language":"en","shortTitle":"Reproducible Analysis Pipeline for Data Streams","libraryCatalog":"DOI.org (Crossref)","url":"https://www.frontiersin.org/articles/10.3389/fdgth.2021.769823/full","accessDate":"2023-03-22T15:34:08Z","volume":"3","pages":"769823","publicationTitle":"Frontiers in Digital Health","DOI":"10.3389/fdgth.2021.769823","journalAbbreviation":"Front. Digit. Health","ISSN":"2673-253X","creators":[{"firstName":"Julio","lastName":"Vega","creatorType":"author"},{"firstName":"Meng","lastName":"Li","creatorType":"author"},{"firstName":"Kwesi","lastName":"Aguillera","creatorType":"author"},{"firstName":"Nikunj","lastName":"Goel","creatorType":"author"},{"firstName":"Echhit","lastName":"Joshi","creatorType":"author"},{"firstName":"Kirtiraj","lastName":"Khandekar","creatorType":"author"},{"firstName":"Krina C.","lastName":"Durica","creatorType":"author"},{"firstName":"Abhineeth R.","lastName":"Kunta","creatorType":"author"},{"firstName":"Carissa A.","lastName":"Low","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/AN75XGLF"]},"dateAdded":"2023-03-22T15:34:08Z","dateModified":"2023-03-22T17:03:34Z","uri":"http://zotero.org/users/11367251/items/IYQRG95G","itemID":74,"attachments":[{"key":"J7ZDZE49","version":240,"itemType":"attachment","title":"vegaReproducibleAnalysisPipeline2021.pdf","parentItem":"IYQRG95G","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/vegaReproducibleAnalysisPipeline2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_J7ZDZE49/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/3\">Methods</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/3\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/3\">RAPIDS Capabilities</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/3\">Supported Devices and Sensors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/3\">Flexible Time Segments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/4\">Flexible Time Zones</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/4\">Device Study Management</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/4\">Modular, Scalable, and Transparent Workflows</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">Reproducible Programming Environments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">Tests</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">Web Documentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">Data Visualizations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">Data Analysis in a RAPIDS Workflow</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">Preliminary Usability Evidence</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/5\">RAPIDS Behavioral Features</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/6\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/6\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/6\">Data Availability Statement</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/6\">Author Contributions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/6\">Funding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_J7ZDZE49/6\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:00Z","dateModified":"2023-03-22T18:52:21Z","uri":"http://zotero.org/users/11367251/items/J7ZDZE49","localPath":"/Users/reyvababtista/Projects/Papers/vegaReproducibleAnalysisPipeline2021.pdf","defaultPath":"files/242/vegaReproducibleAnalysisPipeline2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400442,"version":0},"$loki":153},{"itemID":1676,"item":{"key":"IZFKGIB4","version":2115,"itemType":"blogPost","title":"Decision tree","date":"2023","url":"https://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=1178109845","accessDate":"2023-12-05","blogTitle":"Decision tree","creators":[{"name":"Wikipedia contributors","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:01:03Z","dateModified":"2023-12-05T21:01:44Z","uri":"http://zotero.org/users/11367251/items/IZFKGIB4","itemID":1676,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400442,"version":0},"$loki":154},{"itemID":1858,"item":{"key":"JAGLB6UJ","version":2375,"itemType":"conferencePaper","title":"One Ring to Rule Them All: An Open Source Smartring Platform for Finger Motion Analytics and Healthcare Applications","date":"2023-05-09","language":"en","shortTitle":"One Ring to Rule Them All","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/3576842.3582382","accessDate":"2024-02-05T14:31:41Z","place":"San Antonio TX USA","publisher":"ACM","ISBN":"9798400700378","pages":"27-38","proceedingsTitle":"Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation","conferenceName":"IoTDI '23: International Conference on Internet-of-Things Design and Implementation","DOI":"10.1145/3576842.3582382","creators":[{"firstName":"Hao","lastName":"Zhou","creatorType":"author"},{"firstName":"Taiting","lastName":"Lu","creatorType":"author"},{"firstName":"Yilin","lastName":"Liu","creatorType":"author"},{"firstName":"Shijia","lastName":"Zhang","creatorType":"author"},{"firstName":"Runze","lastName":"Liu","creatorType":"author"},{"firstName":"Mahanth","lastName":"Gowda","creatorType":"author"}],"tags":[],"collections":["TAUT9NML"],"relations":{},"dateAdded":"2024-02-05T14:31:41Z","dateModified":"2024-02-05T14:31:41Z","uri":"http://zotero.org/users/11367251/items/JAGLB6UJ","itemID":1858,"attachments":[{"key":"77C8EVC5","version":2376,"itemType":"attachment","title":"zhouOneRingRule2023.pdf","parentItem":"JAGLB6UJ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhouOneRingRule2023.pdf","tags":[],"relations":{},"dateAdded":"2024-02-05T14:31:55Z","dateModified":"2024-02-05T14:31:55Z","uri":"http://zotero.org/users/11367251/items/77C8EVC5","localPath":"/Users/reyvababtista/Projects/Papers/zhouOneRingRule2023.pdf","defaultPath":"files/1860/zhouOneRingRule2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400443,"version":0},"$loki":155},{"itemID":914,"item":{"key":"JB26LNLZ","version":1131,"itemType":"journalArticle","title":"The anatomy of a large-scale hypertextual Web search engine","abstractNote":"In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.","date":"4/1998","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S016975529800110X","accessDate":"2023-11-01T01:06:32Z","volume":"30","pages":"107-117","publicationTitle":"Computer Networks and ISDN Systems","DOI":"10.1016/S0169-7552(98)00110-X","issue":"1-7","journalAbbreviation":"Computer Networks and ISDN Systems","ISSN":"01697552","creators":[{"firstName":"Sergey","lastName":"Brin","creatorType":"author"},{"firstName":"Lawrence","lastName":"Page","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-01T01:06:32Z","dateModified":"2023-11-01T01:06:32Z","uri":"http://zotero.org/users/11367251/items/JB26LNLZ","itemID":914,"attachments":[{"key":"K9DG7XGK","version":1132,"itemType":"attachment","title":"brinanatomylargescalehypertextual1998.pdf","parentItem":"JB26LNLZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/brinanatomylargescalehypertextual1998.pdf","tags":[],"relations":{},"dateAdded":"2023-11-01T01:06:36Z","dateModified":"2023-11-01T01:06:36Z","uri":"http://zotero.org/users/11367251/items/K9DG7XGK","localPath":"/Users/reyvababtista/Projects/Papers/brinanatomylargescalehypertextual1998.pdf","defaultPath":"files/915/brinanatomylargescalehypertextual1998.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400444,"version":0},"$loki":156},{"itemID":473,"item":{"key":"JDITRTQA","version":648,"itemType":"preprint","title":"Is GPT-4 a Good Data Analyst?","abstractNote":"As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of “is GPT-4 a good data analyst?” in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that GPT-4 can replace data analysts. Our code, data and demo are available at: https://github.com/DAMO-NLP-SG/ GPT4-as-DataAnalyst.","date":"2023-05-24","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2305.15038","accessDate":"2023-06-02T19:24:23Z","extra":"arXiv:2305.15038 [cs]","repository":"arXiv","archiveID":"arXiv:2305.15038","creators":[{"firstName":"Liying","lastName":"Cheng","creatorType":"author"},{"firstName":"Xingxuan","lastName":"Li","creatorType":"author"},{"firstName":"Lidong","lastName":"Bing","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-02T19:24:23Z","dateModified":"2023-06-02T19:24:24Z","uri":"http://zotero.org/users/11367251/items/JDITRTQA","itemID":473,"attachments":[{"key":"JJZKVM3U","version":569,"itemType":"attachment","title":"chengGPT4GoodData2023.pdf","parentItem":"JDITRTQA","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chengGPT4GoodData2023.pdf","tags":[],"relations":{},"dateAdded":"2023-06-02T19:24:34Z","dateModified":"2023-06-02T19:24:34Z","uri":"http://zotero.org/users/11367251/items/JJZKVM3U","localPath":"/Users/reyvababtista/Projects/Papers/chengGPT4GoodData2023.pdf","defaultPath":"files/475/chengGPT4GoodData2023.pdf"}],"notes":[{"key":"BMCPY8M5","version":565,"itemType":"note","parentItem":"JDITRTQA","note":"Comment: 11 pages, 2 figures","tags":[],"relations":{},"dateAdded":"2023-06-02T19:24:23Z","dateModified":"2023-06-02T19:24:23Z","uri":"http://zotero.org/users/11367251/items/BMCPY8M5"}]},"meta":{"revision":0,"created":1709832400446,"version":0},"$loki":157},{"itemID":413,"item":{"key":"JER47U3G","version":466,"itemType":"conferencePaper","title":"You Only Look Once: Unified, Real-Time Object Detection","abstractNote":"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.","date":"6/2016","language":"en","shortTitle":"You Only Look Once","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/7780460/","accessDate":"2023-04-26T01:18:53Z","place":"Las Vegas, NV, USA","publisher":"IEEE","ISBN":"978-1-4673-8851-1","pages":"779-788","proceedingsTitle":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2016.91","creators":[{"firstName":"Joseph","lastName":"Redmon","creatorType":"author"},{"firstName":"Santosh","lastName":"Divvala","creatorType":"author"},{"firstName":"Ross","lastName":"Girshick","creatorType":"author"},{"firstName":"Ali","lastName":"Farhadi","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-26T01:18:53Z","dateModified":"2023-04-26T01:18:54Z","uri":"http://zotero.org/users/11367251/items/JER47U3G","itemID":413,"attachments":[{"key":"IC4CRUKV","version":469,"itemType":"attachment","title":"redmonYouOnlyLook2016.pdf","parentItem":"JER47U3G","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/redmonYouOnlyLook2016.pdf","tags":[],"relations":{},"dateAdded":"2023-04-26T01:19:00Z","dateModified":"2023-04-26T01:19:00Z","uri":"http://zotero.org/users/11367251/items/IC4CRUKV","localPath":"/Users/reyvababtista/Projects/Papers/redmonYouOnlyLook2016.pdf","defaultPath":"files/414/redmonYouOnlyLook2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400447,"version":0},"$loki":158},{"itemID":1674,"item":{"key":"JVCFHL72","version":2104,"itemType":"bookSection","title":"Supervised learning","date":"2021","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/B9780128219294000044","accessDate":"2023-12-05T20:57:50Z","extra":"DOI: 10.1016/B978-0-12-821929-4.00004-4","publisher":"Elsevier","ISBN":"978-0-12-821929-4","pages":"169-295","bookTitle":"Machine Learning Guide for Oil and Gas Using Python","creators":[{"firstName":"Hoss","lastName":"Belyadi","creatorType":"author"},{"firstName":"Alireza","lastName":"Haghighat","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T20:57:50Z","dateModified":"2023-12-05T20:57:50Z","uri":"http://zotero.org/users/11367251/items/JVCFHL72","itemID":1674,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400448,"version":0},"$loki":159},{"itemID":1665,"item":{"key":"JWR8Q57E","version":2087,"itemType":"journalArticle","title":"Learning representations by back-propagating errors","date":"10/1986","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.nature.com/articles/323533a0","accessDate":"2023-12-04T03:11:21Z","volume":"323","pages":"533-536","publicationTitle":"Nature","DOI":"10.1038/323533a0","issue":"6088","journalAbbreviation":"Nature","ISSN":"0028-0836, 1476-4687","creators":[{"firstName":"David E.","lastName":"Rumelhart","creatorType":"author"},{"firstName":"Geoffrey E.","lastName":"Hinton","creatorType":"author"},{"firstName":"Ronald J.","lastName":"Williams","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T03:11:21Z","dateModified":"2023-12-04T03:11:21Z","uri":"http://zotero.org/users/11367251/items/JWR8Q57E","itemID":1665,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400448,"version":0},"$loki":160},{"itemID":78,"item":{"key":"JYW85TFH","version":329,"itemType":"journalArticle","title":"Snakemake—a scalable bioinformatics workflow engine","abstractNote":"Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.","date":"2012-10-01","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://academic.oup.com/bioinformatics/article/28/19/2520/290322","accessDate":"2023-03-22T15:34:15Z","volume":"28","pages":"2520-2522","publicationTitle":"Bioinformatics","DOI":"10.1093/bioinformatics/bts480","issue":"19","ISSN":"1367-4811, 1367-4803","creators":[{"firstName":"Johannes","lastName":"Köster","creatorType":"author"},{"firstName":"Sven","lastName":"Rahmann","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/8NUGRY7W","http://zotero.org/users/11367251/items/UPWRSYY9"]},"dateAdded":"2023-03-22T15:34:15Z","dateModified":"2023-03-27T02:16:40Z","uri":"http://zotero.org/users/11367251/items/JYW85TFH","itemID":78,"attachments":[{"key":"7LGHRMNB","version":329,"itemType":"attachment","title":"kosterSnakemakescalablebioinformatics2012.pdf","parentItem":"JYW85TFH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kosterSnakemakescalablebioinformatics2012.pdf","tags":[],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/DEGIU2XQ"]},"dateAdded":"2023-03-22T18:52:10Z","dateModified":"2023-03-27T02:16:40Z","uri":"http://zotero.org/users/11367251/items/7LGHRMNB","localPath":"/Users/reyvababtista/Projects/Papers/kosterSnakemakescalablebioinformatics2012.pdf","defaultPath":"files/257/kosterSnakemakescalablebioinformatics2012.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400449,"version":0},"$loki":161},{"itemID":919,"item":{"key":"KFS43JE7","version":1157,"itemType":"blogPost","title":"MUM: A new AI milestone for understanding information","date":"05/18/2021","url":"https://blog.google/products/search/introducing-mum/","accessDate":"2023-11-04","blogTitle":"MUM: A new AI milestone for understanding information","creators":[{"name":"Pandu Nayak","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T21:53:18Z","dateModified":"2023-11-04T21:54:02Z","uri":"http://zotero.org/users/11367251/items/KFS43JE7","itemID":919,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400449,"version":0},"$loki":162},{"itemID":350,"item":{"key":"KKVDAFTU","version":383,"itemType":"journalArticle","title":"Research electronic data capture (REDCap)—A metadata-driven methodology and workflow process for providing translational research informatics support","abstractNote":"Research electronic data capture (REDCap) is a novel workﬂow methodology and software solution designed for rapid development and deployment of electronic data capture tools to support clinical and translational research. We present: (1) a brief description of the REDCap metadata-driven software toolset; (2) detail concerning the capture and use of study-related metadata from scientiﬁc research teams; (3) measures of impact for REDCap; (4) details concerning a consortium network of domestic and international institutions collaborating on the project; and (5) strengths and limitations of the REDCap system. REDCap is currently supporting 286 translational research projects in a growing collaborative network including 27 active partner institutions.","date":"04/2009","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S1532046408001226","accessDate":"2023-03-29T07:24:20Z","volume":"42","pages":"377-381","publicationTitle":"Journal of Biomedical Informatics","DOI":"10.1016/j.jbi.2008.08.010","issue":"2","journalAbbreviation":"Journal of Biomedical Informatics","ISSN":"15320464","creators":[{"firstName":"Paul A.","lastName":"Harris","creatorType":"author"},{"firstName":"Robert","lastName":"Taylor","creatorType":"author"},{"firstName":"Robert","lastName":"Thielke","creatorType":"author"},{"firstName":"Jonathon","lastName":"Payne","creatorType":"author"},{"firstName":"Nathaniel","lastName":"Gonzalez","creatorType":"author"},{"firstName":"Jose G.","lastName":"Conde","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-29T07:24:20Z","dateModified":"2023-03-29T07:24:21Z","uri":"http://zotero.org/users/11367251/items/KKVDAFTU","itemID":350,"attachments":[{"key":"PX9SGJTM","version":386,"itemType":"attachment","title":"harrisResearchelectronicdata2009.pdf","parentItem":"KKVDAFTU","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/harrisResearchelectronicdata22.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PX9SGJTM/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/1\">Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/2\">Study Creation Workflowcreation workflow</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/2\">User Interfaceinterface</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/2\">Architecture</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/3\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/3\">Research Utilizationutilization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/4\">Multi-Institutional Consortium – Software Availability Multi-institutional consortium: software availability and Collaborative Networkcollaborative network</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/4\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/4\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/5\">AcknowledgementsAcknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PX9SGJTM/5\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-29T07:24:27Z","dateModified":"2023-03-29T07:24:28Z","uri":"http://zotero.org/users/11367251/items/PX9SGJTM","localPath":"/Users/reyvababtista/Projects/Papers/harrisResearchelectronicdata22.pdf","defaultPath":"files/351/harrisResearchelectronicdata22.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400450,"version":0},"$loki":163},{"itemID":1501,"item":{"key":"KNRJK367","version":1675,"itemType":"blogPost","title":"Google AR & VR","date":"2023","url":"https://arvr.google.com","accessDate":"2023-11-09","blogTitle":"Google AR & VR","creators":[{"name":"Google","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T20:54:06Z","dateModified":"2023-11-09T20:54:31Z","uri":"http://zotero.org/users/11367251/items/KNRJK367","itemID":1501,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400450,"version":0},"$loki":164},{"itemID":1703,"item":{"key":"KUDS4SC5","version":2167,"itemType":"journalArticle","title":"Deep Residual Learning for Image Recognition","abstractNote":"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","date":"2015","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/1512.03385","accessDate":"2023-12-07T15:05:05Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.1512.03385","creators":[{"firstName":"Kaiming","lastName":"He","creatorType":"author"},{"firstName":"Xiangyu","lastName":"Zhang","creatorType":"author"},{"firstName":"Shaoqing","lastName":"Ren","creatorType":"author"},{"firstName":"Jian","lastName":"Sun","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-07T15:05:05Z","dateModified":"2023-12-07T15:05:05Z","uri":"http://zotero.org/users/11367251/items/KUDS4SC5","itemID":1703,"attachments":[],"notes":[{"key":"V4X5SYZX","version":2167,"itemType":"note","parentItem":"KUDS4SC5","note":"<h2>Other</h2>\nTech report","tags":[],"relations":{},"dateAdded":"2023-12-07T15:05:05Z","dateModified":"2023-12-07T15:05:05Z","uri":"http://zotero.org/users/11367251/items/V4X5SYZX"}]},"meta":{"revision":0,"created":1709832400451,"version":0},"$loki":165},{"itemID":523,"item":{"key":"KUMP9IYY","version":682,"itemType":"journalArticle","title":"Investigating Code Generation Performance of ChatGPT with Crowdsourcing Social Data","abstractNote":"The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users’ attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT’s code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT’s code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screenshots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Yunhe","lastName":"Feng","creatorType":"author"},{"firstName":"Sreecharan","lastName":"Vanam","creatorType":"author"},{"firstName":"Manasa","lastName":"Cherukupally","creatorType":"author"},{"firstName":"Weijian","lastName":"Zheng","creatorType":"author"},{"firstName":"Meikang","lastName":"Qiu","creatorType":"author"},{"firstName":"Haihua","lastName":"Chen","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-22T15:11:10Z","dateModified":"2023-06-22T15:11:10Z","uri":"http://zotero.org/users/11367251/items/KUMP9IYY","itemID":523,"attachments":[{"key":"IFQAITLH","version":682,"itemType":"attachment","title":"fengInvestigatingCodeGeneration.pdf","parentItem":"KUMP9IYY","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/fengInvestigatingCodeGeneration.pdf","tags":[],"relations":{},"dateAdded":"2023-06-22T15:11:14Z","dateModified":"2023-06-22T15:11:14Z","uri":"http://zotero.org/users/11367251/items/IFQAITLH","localPath":"/Users/reyvababtista/Projects/Papers/fengInvestigatingCodeGeneration.pdf","defaultPath":"files/524/fengInvestigatingCodeGeneration.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400451,"version":0},"$loki":166},{"itemID":543,"item":{"key":"KUYY6P2U","version":717,"itemType":"preprint","title":"CollabCoder: A GPT-Powered Workflow for Collaborative Qualitative Analysis","abstractNote":"The Collaborative Qualitative Analysis (CQA) process can be time-consuming and resource-intensive, requiring multiple discussions among team members to refine codes and ideas before reaching a consensus. To address these challenges, we introduce CollabCoder, a system leveraging Large Language Models (LLMs) to support three CQA stages: independent open coding, iterative discussions, and the development of a final codebook. In the independent open coding phase, CollabCoder provides AI-generated code suggestions on demand, and allows users to record coding decision-making information (e.g. keywords and certainty) as support for the process. During the discussion phase, CollabCoder helps to build mutual understanding and productive discussion by sharing coding decision-making information with the team. It also helps to quickly identify agreements and disagreements through quantitative metrics, in order to build a final consensus. During the code grouping phase, CollabCoder employs a top-down approach for primary code group recommendations, reducing the cognitive burden of generating the final codebook. An evaluation involving 16 users confirmed the usability and effectiveness of CollabCoder and offered empirical insights into the LLMs' roles in CQA.","date":"2023-04-18","language":"en","shortTitle":"CollabCoder","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2304.07366","accessDate":"2023-06-30T14:16:29Z","extra":"arXiv:2304.07366 [cs]","repository":"arXiv","archiveID":"arXiv:2304.07366","creators":[{"firstName":"Jie","lastName":"Gao","creatorType":"author"},{"firstName":"Yuchen","lastName":"Guo","creatorType":"author"},{"firstName":"Gionnieve","lastName":"Lim","creatorType":"author"},{"firstName":"Tianqin","lastName":"Zhang","creatorType":"author"},{"firstName":"Zheng","lastName":"Zhang","creatorType":"author"},{"firstName":"Toby Jia-Jun","lastName":"Li","creatorType":"author"},{"firstName":"Simon Tangi","lastName":"Perrault","creatorType":"author"}],"tags":[{"tag":"Computer Science - Human-Computer Interaction","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-30T14:16:29Z","dateModified":"2023-06-30T14:16:29Z","uri":"http://zotero.org/users/11367251/items/KUYY6P2U","itemID":543,"attachments":[{"key":"LY84P6JK","version":719,"itemType":"attachment","title":"gaoCollabCoderGPTPoweredWorkflow2023.pdf","parentItem":"KUYY6P2U","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gaoCollabCoderGPTPoweredWorkflow2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LY84P6JK/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/2\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LY84P6JK/3\">2 Background and Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/3\">2.1 Collaborative Qualitative Analysis (CQA)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/3\">2.2 (AI-assisted) QA Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/3\">2.3 Agreements and Consensuses Building</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LY84P6JK/4\">3 Design Consideration and Exploration</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/4\">3.1 Methodology</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/4\">3.2 Design Considerations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LY84P6JK/5\">4 CollabCoder System</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/5\">4.1 CollabCoder Workflow &amp;amp; Usage Scenario</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/6\">4.2 Key Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/8\">4.3 System Implementation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LY84P6JK/10\">5 User Evaluation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/10\">5.1 Participants and Ethics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/11\">5.2 Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/11\">5.3 Conditions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/11\">5.4 Procedure</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LY84P6JK/11\">6 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/11\">6.1 Quantitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/11\">6.2 Qualitative Results</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LY84P6JK/13\">7 Discussion and Design Implications</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/14\">7.1 LLMs as ``Suggestion Provider'' in Open Coding: Helper, not Replacement.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/14\">7.2 LLMs as ``Mediator'' for Different Group Dynamics in the Discussion Phase</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/15\">7.3 LLMs as ``Facilitator'' in Streamlining Primary Code Grouping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/15\">7.4 Facilitating Discussion by Building Common Ground</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/16\">8 Limitations and Future Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/16\">9 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LY84P6JK/16\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-30T14:16:35Z","dateModified":"2023-06-30T14:16:36Z","uri":"http://zotero.org/users/11367251/items/LY84P6JK","localPath":"/Users/reyvababtista/Projects/Papers/gaoCollabCoderGPTPoweredWorkflow2023.pdf","defaultPath":"files/544/gaoCollabCoderGPTPoweredWorkflow2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400451,"version":0},"$loki":167},{"itemID":1825,"item":{"key":"L35U3H99","version":2267,"itemType":"conferencePaper","title":"Artificial Neural Network Based University Chatbot System","date":"7/2019","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8973095/","accessDate":"2024-01-19T03:04:22Z","place":"Mumbai, India","publisher":"IEEE","ISBN":"978-1-5386-7401-7","pages":"1-6","proceedingsTitle":"2019 IEEE Bombay Section Signature Conference (IBSSC)","conferenceName":"2019 IEEE Bombay Section Signature Conference (IBSSC)","DOI":"10.1109/IBSSC47189.2019.8973095","creators":[{"firstName":"Namrata","lastName":"Bhartiya","creatorType":"author"},{"firstName":"Namrata","lastName":"Jangid","creatorType":"author"},{"firstName":"Sheetal","lastName":"Jannu","creatorType":"author"},{"firstName":"Purvika","lastName":"Shukla","creatorType":"author"},{"firstName":"Radhika","lastName":"Chapaneri","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T03:04:22Z","dateModified":"2024-01-19T03:04:22Z","uri":"http://zotero.org/users/11367251/items/L35U3H99","itemID":1825,"attachments":[{"key":"2BIGTGHT","version":2268,"itemType":"attachment","title":"bhartiyaArtificialNeuralNetwork2019.pdf","parentItem":"L35U3H99","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bhartiyaArtificialNeuralNetwork2019.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T03:04:37Z","dateModified":"2024-01-19T03:04:37Z","uri":"http://zotero.org/users/11367251/items/2BIGTGHT","localPath":"/Users/reyvababtista/Projects/Papers/bhartiyaArtificialNeuralNetwork2019.pdf","defaultPath":"files/1827/bhartiyaArtificialNeuralNetwork2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400452,"version":0},"$loki":168},{"itemID":1635,"item":{"key":"L3E4IPGT","version":2031,"itemType":"preprint","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","abstractNote":"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","date":"2023-07-19","shortTitle":"Llama 2","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2307.09288","accessDate":"2023-11-29T07:51:56Z","extra":"arXiv:2307.09288 [cs]","repository":"arXiv","archiveID":"arXiv:2307.09288","creators":[{"firstName":"Hugo","lastName":"Touvron","creatorType":"author"},{"firstName":"Louis","lastName":"Martin","creatorType":"author"},{"firstName":"Kevin","lastName":"Stone","creatorType":"author"},{"firstName":"Peter","lastName":"Albert","creatorType":"author"},{"firstName":"Amjad","lastName":"Almahairi","creatorType":"author"},{"firstName":"Yasmine","lastName":"Babaei","creatorType":"author"},{"firstName":"Nikolay","lastName":"Bashlykov","creatorType":"author"},{"firstName":"Soumya","lastName":"Batra","creatorType":"author"},{"firstName":"Prajjwal","lastName":"Bhargava","creatorType":"author"},{"firstName":"Shruti","lastName":"Bhosale","creatorType":"author"},{"firstName":"Dan","lastName":"Bikel","creatorType":"author"},{"firstName":"Lukas","lastName":"Blecher","creatorType":"author"},{"firstName":"Cristian Canton","lastName":"Ferrer","creatorType":"author"},{"firstName":"Moya","lastName":"Chen","creatorType":"author"},{"firstName":"Guillem","lastName":"Cucurull","creatorType":"author"},{"firstName":"David","lastName":"Esiobu","creatorType":"author"},{"firstName":"Jude","lastName":"Fernandes","creatorType":"author"},{"firstName":"Jeremy","lastName":"Fu","creatorType":"author"},{"firstName":"Wenyin","lastName":"Fu","creatorType":"author"},{"firstName":"Brian","lastName":"Fuller","creatorType":"author"},{"firstName":"Cynthia","lastName":"Gao","creatorType":"author"},{"firstName":"Vedanuj","lastName":"Goswami","creatorType":"author"},{"firstName":"Naman","lastName":"Goyal","creatorType":"author"},{"firstName":"Anthony","lastName":"Hartshorn","creatorType":"author"},{"firstName":"Saghar","lastName":"Hosseini","creatorType":"author"},{"firstName":"Rui","lastName":"Hou","creatorType":"author"},{"firstName":"Hakan","lastName":"Inan","creatorType":"author"},{"firstName":"Marcin","lastName":"Kardas","creatorType":"author"},{"firstName":"Viktor","lastName":"Kerkez","creatorType":"author"},{"firstName":"Madian","lastName":"Khabsa","creatorType":"author"},{"firstName":"Isabel","lastName":"Kloumann","creatorType":"author"},{"firstName":"Artem","lastName":"Korenev","creatorType":"author"},{"firstName":"Punit Singh","lastName":"Koura","creatorType":"author"},{"firstName":"Marie-Anne","lastName":"Lachaux","creatorType":"author"},{"firstName":"Thibaut","lastName":"Lavril","creatorType":"author"},{"firstName":"Jenya","lastName":"Lee","creatorType":"author"},{"firstName":"Diana","lastName":"Liskovich","creatorType":"author"},{"firstName":"Yinghai","lastName":"Lu","creatorType":"author"},{"firstName":"Yuning","lastName":"Mao","creatorType":"author"},{"firstName":"Xavier","lastName":"Martinet","creatorType":"author"},{"firstName":"Todor","lastName":"Mihaylov","creatorType":"author"},{"firstName":"Pushkar","lastName":"Mishra","creatorType":"author"},{"firstName":"Igor","lastName":"Molybog","creatorType":"author"},{"firstName":"Yixin","lastName":"Nie","creatorType":"author"},{"firstName":"Andrew","lastName":"Poulton","creatorType":"author"},{"firstName":"Jeremy","lastName":"Reizenstein","creatorType":"author"},{"firstName":"Rashi","lastName":"Rungta","creatorType":"author"},{"firstName":"Kalyan","lastName":"Saladi","creatorType":"author"},{"firstName":"Alan","lastName":"Schelten","creatorType":"author"},{"firstName":"Ruan","lastName":"Silva","creatorType":"author"},{"firstName":"Eric Michael","lastName":"Smith","creatorType":"author"},{"firstName":"Ranjan","lastName":"Subramanian","creatorType":"author"},{"firstName":"Xiaoqing Ellen","lastName":"Tan","creatorType":"author"},{"firstName":"Binh","lastName":"Tang","creatorType":"author"},{"firstName":"Ross","lastName":"Taylor","creatorType":"author"},{"firstName":"Adina","lastName":"Williams","creatorType":"author"},{"firstName":"Jian Xiang","lastName":"Kuan","creatorType":"author"},{"firstName":"Puxin","lastName":"Xu","creatorType":"author"},{"firstName":"Zheng","lastName":"Yan","creatorType":"author"},{"firstName":"Iliyan","lastName":"Zarov","creatorType":"author"},{"firstName":"Yuchen","lastName":"Zhang","creatorType":"author"},{"firstName":"Angela","lastName":"Fan","creatorType":"author"},{"firstName":"Melanie","lastName":"Kambadur","creatorType":"author"},{"firstName":"Sharan","lastName":"Narang","creatorType":"author"},{"firstName":"Aurelien","lastName":"Rodriguez","creatorType":"author"},{"firstName":"Robert","lastName":"Stojnic","creatorType":"author"},{"firstName":"Sergey","lastName":"Edunov","creatorType":"author"},{"firstName":"Thomas","lastName":"Scialom","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T07:51:56Z","dateModified":"2023-11-29T07:51:56Z","uri":"http://zotero.org/users/11367251/items/L3E4IPGT","itemID":1635,"attachments":[{"key":"NV7KMALP","version":2034,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2307.09288","accessDate":"2023-11-29T07:52:10Z","parentItem":"L3E4IPGT","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2307.html","tags":[],"relations":{},"dateAdded":"2023-11-29T07:52:10Z","dateModified":"2023-11-29T07:52:10Z","uri":"http://zotero.org/users/11367251/items/NV7KMALP","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/NV7KMALP/2307.html","defaultPath":"files/1638/2307.html"},{"key":"6RUB5J6M","version":2032,"itemType":"attachment","title":"touvronLlamaOpenFoundation2023.pdf","parentItem":"L3E4IPGT","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/touvronLlamaOpenFoundation2023.pdf","tags":[],"relations":{},"dateAdded":"2023-11-29T07:52:05Z","dateModified":"2023-11-29T07:52:05Z","uri":"http://zotero.org/users/11367251/items/6RUB5J6M","localPath":"/Users/reyvababtista/Projects/Papers/touvronLlamaOpenFoundation2023.pdf","defaultPath":"files/1637/touvronLlamaOpenFoundation2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400454,"version":0},"$loki":169},{"itemID":1443,"item":{"key":"L77MM7E9","version":1582,"itemType":"preprint","title":"Natural Adversarial Examples","abstractNote":"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.","date":"2021-03-04","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1907.07174","accessDate":"2023-11-08T22:49:40Z","extra":"arXiv:1907.07174 [cs, stat]","repository":"arXiv","archiveID":"arXiv:1907.07174","creators":[{"firstName":"Dan","lastName":"Hendrycks","creatorType":"author"},{"firstName":"Kevin","lastName":"Zhao","creatorType":"author"},{"firstName":"Steven","lastName":"Basart","creatorType":"author"},{"firstName":"Jacob","lastName":"Steinhardt","creatorType":"author"},{"firstName":"Dawn","lastName":"Song","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:49:40Z","dateModified":"2023-11-08T22:49:40Z","uri":"http://zotero.org/users/11367251/items/L77MM7E9","itemID":1443,"attachments":[{"key":"8DVI536Y","version":1586,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1907.07174","accessDate":"2023-11-08T22:50:10Z","parentItem":"L77MM7E9","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1907.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:50:10Z","dateModified":"2023-11-08T22:50:10Z","uri":"http://zotero.org/users/11367251/items/8DVI536Y","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/8DVI536Y/1907.html","defaultPath":"files/1447/1907.html"},{"key":"VZFR4VYD","version":1583,"itemType":"attachment","title":"hendrycksNaturalAdversarialExamples2021.pdf","parentItem":"L77MM7E9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hendrycksNaturalAdversarialExamples2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:50:05Z","dateModified":"2023-11-08T22:50:05Z","uri":"http://zotero.org/users/11367251/items/VZFR4VYD","localPath":"/Users/reyvababtista/Projects/Papers/hendrycksNaturalAdversarialExamples2021.pdf","defaultPath":"files/1446/hendrycksNaturalAdversarialExamples2021.pdf"}],"notes":[{"key":"Y4EQD4M6","version":1582,"itemType":"note","parentItem":"L77MM7E9","note":"Comment: CVPR 2021; dataset and code available at https://github.com/hendrycks/natural-adv-examples","tags":[],"relations":{},"dateAdded":"2023-11-08T22:49:40Z","dateModified":"2023-11-08T22:49:40Z","uri":"http://zotero.org/users/11367251/items/Y4EQD4M6"}]},"meta":{"revision":0,"created":1709832400456,"version":0},"$loki":170},{"itemID":630,"item":{"key":"L85GH7GQ","version":820,"itemType":"preprint","title":"The Role of ChatGPT in Democratizing Data Science: An Exploration of AI-facilitated Data Analysis in Telematics","abstractNote":"The realm of data science, once reserved for specialists, is undergoing a revolution with the rapid emergence of generative AI, particularly through tools like ChatGPT. This paper posits ChatGPT as a pivotal bridge, drastically lowering the steep learning curve traditionally associated with complex data analysis. By generating intuitive data narratives and offering real-time assistance, ChatGPT democratizes the field, enabling a wider audience to glean insights from intricate datasets. A notable illustration of this transformative potential is provided through the examination of a synthetically generated telematics dataset, wherein ChatGPT aids in distilling complex patterns and insights. However, the journey to democratization is not without its hurdles. The paper delves into challenges presented by such AI, from potential biases in analysis to ChatGPT’s limited reasoning capabilities. While the promise of a democratized data science landscape beckons, it is imperative to approach this transition with caution, cognizance, and an ever-evolving understanding of the tool’s capabilities and constraints.","date":"2023-07-26","language":"en","shortTitle":"The Role of ChatGPT in Democratizing Data Science","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2308.02045","accessDate":"2023-09-07T21:36:49Z","extra":"arXiv:2308.02045 [cs]","repository":"arXiv","archiveID":"arXiv:2308.02045","creators":[{"firstName":"Ryan","lastName":"Lingo","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computers and Society","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-07T21:36:49Z","dateModified":"2023-09-07T21:36:49Z","uri":"http://zotero.org/users/11367251/items/L85GH7GQ","itemID":630,"attachments":[{"key":"FDD2FCCQ","version":823,"itemType":"attachment","title":"lingoRoleChatGPTDemocratizing2023.pdf","parentItem":"L85GH7GQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/lingoRoleChatGPTDemocratizing2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_FDD2FCCQ/1\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/2\">Purpose and Organization of the Paper</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/2\">Potential Benefits and Challenges of Using ChatGPT in Data Analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/2\">Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/2\">A Brief History of Natural Language Processing and The Rise of Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/3\">Literature Review</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/3\">Applications of ChatGPT in Data Analysis</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/3\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/4\">Telematics Data: an Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/4\">Creation of Synthetic Telematics Data</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/6\">Data Cleaning and Preprocessing</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/6\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/7\">Significance of Data Cleaning in Telematics:</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/7\">Removing or Filling Missing Values in Telematics Data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/9\">Converting Data Types with ChatGPT's Assistance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/10\">Using ChatGPT to Help with Outlier Detection</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/12\">Understanding Feature Engineering: A Key to Enhanced Data Analysis</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/13\">Extracting the Day of the Week from Timestamps</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/14\">Computing the Average Speed for Each Vehicle Across All Trips</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/14\">Calculating Total Distance Traveled for Each Trip</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/15\">Guided EDA with ChatGPT: Delving into Telematics Data</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/15\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/16\">Summary Statistics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/17\">Data Grouping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/19\">Pattern Recognition</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/20\">Anomaly Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/21\">Hypothesis Testing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/22\">Visual Representations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/24\">EDA Conclusion</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/25\">Democratizing Data Science</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/25\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/25\">Lowering Barriers to Entry for Beginners through Natural Language Interface</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/25\">Shifting Focus: From Complex Programming Syntax to Problem-Solving and Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/25\">The Impact of Rapid Feedback from ChatGPT on Speed and Efficiency of Data Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/26\">Democratizing Data Science for Non-tech Domain Experts</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/26\">Expanding the Data Science Community and Promoting Diversity</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/27\">Limitations and Challenges</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/27\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/27\">Bias and Problematic Outputs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/27\">Reasoning Limitations of ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/27\">Overreliance and the Importance of Human Oversight</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/28\">Ethical Considerations in AI-powered Data Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_FDD2FCCQ/28\">Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-07T21:37:37Z","dateModified":"2023-09-07T21:37:38Z","uri":"http://zotero.org/users/11367251/items/FDD2FCCQ","localPath":"/Users/reyvababtista/Projects/Papers/lingoRoleChatGPTDemocratizing2023.pdf","defaultPath":"files/632/lingoRoleChatGPTDemocratizing2023.pdf"}],"notes":[{"key":"HJREHHWM","version":820,"itemType":"note","parentItem":"L85GH7GQ","note":"Comment: 29 pages, 21 figures","tags":[],"relations":{},"dateAdded":"2023-09-07T21:36:49Z","dateModified":"2023-09-07T21:36:49Z","uri":"http://zotero.org/users/11367251/items/HJREHHWM"}]},"meta":{"revision":0,"created":1709832400458,"version":0},"$loki":171},{"itemID":1707,"item":{"key":"L8GVK28T","version":2169,"itemType":"journalArticle","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","abstractNote":"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","date":"2020","shortTitle":"An Image is Worth 16x16 Words","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2010.11929","accessDate":"2023-12-07T15:07:04Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 2","DOI":"10.48550/ARXIV.2010.11929","creators":[{"firstName":"Alexey","lastName":"Dosovitskiy","creatorType":"author"},{"firstName":"Lucas","lastName":"Beyer","creatorType":"author"},{"firstName":"Alexander","lastName":"Kolesnikov","creatorType":"author"},{"firstName":"Dirk","lastName":"Weissenborn","creatorType":"author"},{"firstName":"Xiaohua","lastName":"Zhai","creatorType":"author"},{"firstName":"Thomas","lastName":"Unterthiner","creatorType":"author"},{"firstName":"Mostafa","lastName":"Dehghani","creatorType":"author"},{"firstName":"Matthias","lastName":"Minderer","creatorType":"author"},{"firstName":"Georg","lastName":"Heigold","creatorType":"author"},{"firstName":"Sylvain","lastName":"Gelly","creatorType":"author"},{"firstName":"Jakob","lastName":"Uszkoreit","creatorType":"author"},{"firstName":"Neil","lastName":"Houlsby","creatorType":"author"}],"tags":[{"tag":"Artificial Intelligence (cs.AI)","type":1},{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-07T15:07:04Z","dateModified":"2023-12-07T15:07:04Z","uri":"http://zotero.org/users/11367251/items/L8GVK28T","itemID":1707,"attachments":[],"notes":[{"key":"XK6G3J2C","version":2169,"itemType":"note","parentItem":"L8GVK28T","note":"<h2>Other</h2>\nFine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)","tags":[],"relations":{},"dateAdded":"2023-12-07T15:07:04Z","dateModified":"2023-12-07T15:07:04Z","uri":"http://zotero.org/users/11367251/items/XK6G3J2C"}]},"meta":{"revision":0,"created":1709832400459,"version":0},"$loki":172},{"itemID":1796,"item":{"key":"LKPTFGC4","version":2237,"itemType":"preprint","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","abstractNote":"Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.","date":"2024-01-04","shortTitle":"Retrieval-Augmented Generation for Large Language Models","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2312.10997","accessDate":"2024-01-18T20:24:25Z","extra":"arXiv:2312.10997 [cs]","repository":"arXiv","archiveID":"arXiv:2312.10997","creators":[{"firstName":"Yunfan","lastName":"Gao","creatorType":"author"},{"firstName":"Yun","lastName":"Xiong","creatorType":"author"},{"firstName":"Xinyu","lastName":"Gao","creatorType":"author"},{"firstName":"Kangxiang","lastName":"Jia","creatorType":"author"},{"firstName":"Jinliu","lastName":"Pan","creatorType":"author"},{"firstName":"Yuxi","lastName":"Bi","creatorType":"author"},{"firstName":"Yi","lastName":"Dai","creatorType":"author"},{"firstName":"Jiawei","lastName":"Sun","creatorType":"author"},{"firstName":"Qianyu","lastName":"Guo","creatorType":"author"},{"firstName":"Meng","lastName":"Wang","creatorType":"author"},{"firstName":"Haofen","lastName":"Wang","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2024-01-18T20:24:25Z","dateModified":"2024-01-18T20:24:25Z","uri":"http://zotero.org/users/11367251/items/LKPTFGC4","itemID":1796,"attachments":[{"key":"C2K96ZNF","version":2240,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2312.10997","accessDate":"2024-01-18T20:24:35Z","parentItem":"LKPTFGC4","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2312.html","tags":[],"relations":{},"dateAdded":"2024-01-18T20:24:35Z","dateModified":"2024-01-18T20:24:35Z","uri":"http://zotero.org/users/11367251/items/C2K96ZNF","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/C2K96ZNF/2312.html","defaultPath":"files/1800/2312.html"},{"key":"CWUHY38K","version":2237,"itemType":"attachment","title":"gaoRetrievalAugmentedGenerationLarge2024.pdf","parentItem":"LKPTFGC4","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gaoRetrievalAugmentedGenerationLarge2024.pdf","tags":[],"relations":{},"dateAdded":"2024-01-18T20:24:29Z","dateModified":"2024-01-18T20:24:29Z","uri":"http://zotero.org/users/11367251/items/CWUHY38K","localPath":"/Users/reyvababtista/Projects/Papers/gaoRetrievalAugmentedGenerationLarge2024.pdf","defaultPath":"files/1799/gaoRetrievalAugmentedGenerationLarge2024.pdf"}],"notes":[{"key":"7QKJQN6L","version":2237,"itemType":"note","parentItem":"LKPTFGC4","note":"Comment: Ongoing Work","tags":[],"relations":{},"dateAdded":"2024-01-18T20:24:25Z","dateModified":"2024-01-18T20:24:25Z","uri":"http://zotero.org/users/11367251/items/7QKJQN6L"}]},"meta":{"revision":0,"created":1709832400460,"version":0},"$loki":173},{"itemID":75,"item":{"key":"LMSKIQZ2","version":202,"itemType":"conferencePaper","title":"Deep Learning at Your Fingertips","abstractNote":"From SurveyMonkey to Google Forms, online surveys have become a cornerstone of modern research. However, these survey platforms lack the ability to provide advanced analysis to researchers, often requiring expensive third-party analytics software to rectify their shortcomings. We propose to solve this problem by adding data analysis capabilities onto TigerAware, an existing data collection platform. TigerAware offers a generic and customizable tool which allows researchers without technical expertise to create, manage, and deploy custom mobile surveys to participants in real-time. We seek to add data analysis functionalities to the TigerAware platform ranging from basic statistics functions to emotion recognition via deep learning. Our analysis platform uses data collected by TigerAware to give researchers real-time analytics throughout the duration of their study. Through our additions to the TigerAware platform, we present a novel all-in-one tool for researchers to facilitate effective survey creation, survey administration, data collection, and data analysis.","date":"1/2019","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8651868/","accessDate":"2023-03-22T15:34:10Z","place":"Las Vegas, NV, USA","publisher":"IEEE","ISBN":"978-1-5386-5553-5","pages":"1-4","proceedingsTitle":"2019 16th IEEE Annual Consumer Communications & Networking Conference (CCNC)","conferenceName":"2019 16th IEEE Annual Consumer Communications & Networking Conference (CCNC)","DOI":"10.1109/CCNC.2019.8651868","creators":[{"firstName":"Jonathan","lastName":"Rogers","creatorType":"author"},{"firstName":"Dylan","lastName":"Simmons","creatorType":"author"},{"firstName":"Milesh","lastName":"Shah","creatorType":"author"},{"firstName":"Connor","lastName":"Rowland","creatorType":"author"},{"firstName":"Yi","lastName":"Shang","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/VNRNT4NV"]},"dateAdded":"2023-03-22T15:34:10Z","dateModified":"2023-03-22T17:03:40Z","uri":"http://zotero.org/users/11367251/items/LMSKIQZ2","itemID":75,"attachments":[{"key":"BH3UHMWE","version":240,"itemType":"attachment","title":"rogersDeepLearningYour2019.pdf","parentItem":"LMSKIQZ2","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/rogersDeepLearningYour2019.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:03Z","dateModified":"2023-03-22T18:52:22Z","uri":"http://zotero.org/users/11367251/items/BH3UHMWE","localPath":"/Users/reyvababtista/Projects/Papers/rogersDeepLearningYour2019.pdf","defaultPath":"files/248/rogersDeepLearningYour2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400460,"version":0},"$loki":174},{"itemID":1684,"item":{"key":"LWBGHHMN","version":2138,"itemType":"journalArticle","title":"Performance Evaluation of Supervised Machine Learning Algorithms for Intrusion Detection","date":"2016","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S187705091631081X","accessDate":"2023-12-05T21:28:18Z","volume":"89","pages":"117-123","publicationTitle":"Procedia Computer Science","DOI":"10.1016/j.procs.2016.06.016","journalAbbreviation":"Procedia Computer Science","ISSN":"18770509","creators":[{"firstName":"Manjula C.","lastName":"Belavagi","creatorType":"author"},{"firstName":"Balachandra","lastName":"Muniyal","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:28:18Z","dateModified":"2023-12-05T21:28:18Z","uri":"http://zotero.org/users/11367251/items/LWBGHHMN","itemID":1684,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400461,"version":0},"$loki":175},{"itemID":967,"item":{"key":"LWX4PEH8","version":1346,"itemType":"preprint","title":"Scaling Vision Transformers","abstractNote":"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model’s scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we reﬁne the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.","date":"2022-06-20","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2106.04560","accessDate":"2023-11-07T21:22:24Z","extra":"arXiv:2106.04560 [cs]","repository":"arXiv","archiveID":"arXiv:2106.04560","creators":[{"firstName":"Xiaohua","lastName":"Zhai","creatorType":"author"},{"firstName":"Alexander","lastName":"Kolesnikov","creatorType":"author"},{"firstName":"Neil","lastName":"Houlsby","creatorType":"author"},{"firstName":"Lucas","lastName":"Beyer","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T21:22:24Z","dateModified":"2023-11-07T21:22:24Z","uri":"http://zotero.org/users/11367251/items/LWX4PEH8","itemID":967,"attachments":[{"key":"UGFDBLBZ","version":1350,"itemType":"attachment","title":"zhaiScalingVisionTransformers2022.pdf","parentItem":"LWX4PEH8","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhaiScalingVisionTransformers2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T21:22:28Z","dateModified":"2023-11-07T21:22:28Z","uri":"http://zotero.org/users/11367251/items/UGFDBLBZ","localPath":"/Users/reyvababtista/Projects/Papers/zhaiScalingVisionTransformers2022.pdf","defaultPath":"files/969/zhaiScalingVisionTransformers2022.pdf"}],"notes":[{"key":"FBFNKM4T","version":1346,"itemType":"note","parentItem":"LWX4PEH8","note":"Comment: Xiaohua, Alex, and Lucas contributed equally; CVPR 2022","tags":[],"relations":{},"dateAdded":"2023-11-07T21:22:24Z","dateModified":"2023-11-07T21:22:24Z","uri":"http://zotero.org/users/11367251/items/FBFNKM4T"}]},"meta":{"revision":0,"created":1709832400461,"version":0},"$loki":176},{"itemID":368,"item":{"key":"LXTABNT7","version":410,"itemType":"journalArticle","title":"A Survey of Deep Learning-based Object Detection","abstractNote":"Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in peoples life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning networks for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline, thoroughly and deeply, in this survey, we ﬁrst analyze the methods of existing typical detection models and describe the benchmark datasets. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efﬁcient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.","date":"2019","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1907.09408","accessDate":"2023-04-11T03:10:45Z","extra":"arXiv:1907.09408 [cs]","volume":"7","pages":"128837-128868","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2019.2939201","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Licheng","lastName":"Jiao","creatorType":"author"},{"firstName":"Fan","lastName":"Zhang","creatorType":"author"},{"firstName":"Fang","lastName":"Liu","creatorType":"author"},{"firstName":"Shuyuan","lastName":"Yang","creatorType":"author"},{"firstName":"Lingling","lastName":"Li","creatorType":"author"},{"firstName":"Zhixi","lastName":"Feng","creatorType":"author"},{"firstName":"Rong","lastName":"Qu","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-11T03:10:45Z","dateModified":"2023-04-11T03:10:46Z","uri":"http://zotero.org/users/11367251/items/LXTABNT7","itemID":368,"attachments":[{"key":"GTAPZT4C","version":413,"itemType":"attachment","title":"jiaoSurveyDeepLearningbased2019.pdf","parentItem":"LXTABNT7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/jiaoSurveyDeepLearningbased2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GTAPZT4C/1\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/2\">II Backbone networks</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/2\">III Typical baselines</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/2\">III-A Two-stage Detectors</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/2\">III-A1 R-CNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/3\">III-A2 Fast R-CNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/4\">III-A3 Faster R-CNN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/4\">III-A4 Mask R-CNN</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/4\">III-B One-stage Detectors</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/4\">III-B1 YOLO</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/5\">III-B2 YOLOv2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/6\">III-B3 YOLOv3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/6\">III-B4 SSD</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/6\">III-B5 DSSD</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/6\">III-B6 RetinaNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/6\">III-B7 M2Det</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/7\">III-B8 RefineDet</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/7\">III-C Latest Detectors</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/7\">III-C1 Relation Networks for Object Detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/7\">III-C2 DCNv2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/8\">III-C3 NAS-FPN</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/9\">IV Datasets and metrics</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/9\">IV-A PASCAL VOC dataset</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/9\">IV-A1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/9\">IV-A2 Metric</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/9\">IV-B MS COCO benchmark</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/9\">IV-B1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/11\">IV-B2 Metric</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/11\">IV-C ImageNet benchmark</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/11\">IV-C1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/11\">IV-C2 Metric</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/11\">IV-D VisDrone2018 benchmark</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/12\">IV-E Open Images V5</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/12\">IV-E1 Dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/12\">IV-E2 Metric</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/12\">IV-F Pedestrian detection datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/12\">V Analysis of general image object detection methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/12\">V-A Enhanced features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/13\">V-B Increasing localization accuracy</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/14\">V-C Solving negatives-positives imbalance issue</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/14\">V-D Improving post-processing NMS methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/14\">V-E Combining one-stage and two-stage detectors to make good results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/14\">V-F Complicated scene solutions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/15\">V-G anchor-free</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/15\">V-H Training from scratch</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/15\">V-I Designing new architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/15\">V-J Speeding up detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/15\">V-K Achieving Fast and Accurate Detections</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/16\">VI Applications and branches</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/16\">VI-A Typical application areas</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/16\">VI-A1 Security field</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/17\">VI-A2 Military field</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/17\">VI-A3 Transportation field</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/17\">VI-A4 Medical field</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/18\">VI-A5 Life field</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/18\">VI-B Object detection branches</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/18\">VI-B1 Weakly supervised object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/18\">VI-B2 Salient object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/19\">VI-B3 Highlight detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/19\">VI-B4 Edge detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/19\">VI-B5 Text detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/19\">VI-B6 Multi-domain object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/19\">VI-B7 Object detection in videos</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/20\">VI-B8 Point clouds 3D object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/20\">VI-B9 2D, 3D pose detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/20\">VI-B10 Fine-Grained Visual Recognition</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII Conclusions and trends</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-A Conclusions</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B Trends</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B1 Combining one-stage and two-stage detectors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B2 Video object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B3 Efficient post-processing methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B4 Weakly supervised object detection methods</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B5 Multi-domain object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B6 3D object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B7 Salient object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B8 Unsupervised object detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B9 Multi-task learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/22\">VII-B10 Multi-source information assistance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/23\">VII-B11 Constructing terminal object detection system</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/23\">VII-B12 Medical imaging and diagnosis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/23\">VII-B13 Advanced medical biometrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/23\">VII-B14 Remote sensing airborne and real-time detection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/23\">VII-B15 GAN based detector</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GTAPZT4C/23\">Reference</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-04-11T03:10:53Z","dateModified":"2023-04-11T03:10:54Z","uri":"http://zotero.org/users/11367251/items/GTAPZT4C","localPath":"/Users/reyvababtista/Projects/Papers/jiaoSurveyDeepLearningbased2019.pdf","defaultPath":"files/370/jiaoSurveyDeepLearningbased2019.pdf"}],"notes":[{"key":"RVIDKJ5L","version":410,"itemType":"note","parentItem":"LXTABNT7","note":"Comment: 30 pages,12 figures","tags":[],"relations":{},"dateAdded":"2023-04-11T03:10:45Z","dateModified":"2023-04-11T03:10:45Z","uri":"http://zotero.org/users/11367251/items/RVIDKJ5L"}]},"meta":{"revision":0,"created":1709832400462,"version":0},"$loki":177},{"itemID":1229,"item":{"key":"LZDZGYP7","version":1482,"itemType":"preprint","title":"Deep Residual Learning for Image Recognition","abstractNote":"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","date":"2015-12-10","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1512.03385","accessDate":"2023-11-08T19:43:32Z","extra":"arXiv:1512.03385 [cs]","repository":"arXiv","archiveID":"arXiv:1512.03385","creators":[{"firstName":"Kaiming","lastName":"He","creatorType":"author"},{"firstName":"Xiangyu","lastName":"Zhang","creatorType":"author"},{"firstName":"Shaoqing","lastName":"Ren","creatorType":"author"},{"firstName":"Jian","lastName":"Sun","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T19:43:32Z","dateModified":"2023-11-08T19:43:32Z","uri":"http://zotero.org/users/11367251/items/LZDZGYP7","itemID":1229,"attachments":[{"key":"UWNVJD4B","version":1484,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1512.03385","accessDate":"2023-11-08T19:43:40Z","parentItem":"LZDZGYP7","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1512.html","tags":[],"relations":{},"dateAdded":"2023-11-08T19:43:40Z","dateModified":"2023-11-08T19:43:40Z","uri":"http://zotero.org/users/11367251/items/UWNVJD4B","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/UWNVJD4B/1512.html","defaultPath":"files/1233/1512.html"},{"key":"RPLEE26G","version":1482,"itemType":"attachment","title":"heDeepResidualLearning2015.pdf","parentItem":"LZDZGYP7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/heDeepResidualLearning2015.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T19:43:35Z","dateModified":"2023-11-08T19:43:35Z","uri":"http://zotero.org/users/11367251/items/RPLEE26G","localPath":"/Users/reyvababtista/Projects/Papers/heDeepResidualLearning2015.pdf","defaultPath":"files/1232/heDeepResidualLearning2015.pdf"}],"notes":[{"key":"GCEGS3FA","version":1482,"itemType":"note","parentItem":"LZDZGYP7","note":"Comment: Tech report","tags":[],"relations":{},"dateAdded":"2023-11-08T19:43:32Z","dateModified":"2023-11-08T19:43:32Z","uri":"http://zotero.org/users/11367251/items/GCEGS3FA"}]},"meta":{"revision":0,"created":1709832400463,"version":0},"$loki":178},{"itemID":481,"item":{"key":"LZJLE59Y","version":648,"itemType":"journalArticle","title":"The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field","abstractNote":"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workﬂow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be ﬁne-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the beneﬁts outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workﬂows and is likely to become an increasingly important tool for intelligence augmentation in the ﬁeld of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classiﬁcation. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be ﬁne-tuned for speciﬁc use cases, it may not perform well on certain tasks if it has not been speciﬁcally trained for them. Additionally, the output of ChatGPT may be difﬁcult to interpret, which could pose challenges for decision-making in data science applications.","date":"2023-03-27","language":"en","shortTitle":"The Role of ChatGPT in Data Science","libraryCatalog":"DOI.org (Crossref)","url":"https://www.mdpi.com/2504-2289/7/2/62","accessDate":"2023-06-16T01:07:19Z","volume":"7","pages":"62","publicationTitle":"Big Data and Cognitive Computing","DOI":"10.3390/bdcc7020062","issue":"2","journalAbbreviation":"BDCC","ISSN":"2504-2289","creators":[{"firstName":"Hossein","lastName":"Hassani","creatorType":"author"},{"firstName":"Emmanuel Sirmal","lastName":"Silva","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-16T01:07:19Z","dateModified":"2023-06-16T01:07:19Z","uri":"http://zotero.org/users/11367251/items/LZJLE59Y","itemID":481,"attachments":[{"key":"5IUVFSBU","version":644,"itemType":"attachment","title":"hassaniRoleChatGPTData2023.pdf","parentItem":"LZJLE59Y","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hassaniRoleChatGPTData2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5IUVFSBU/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/3\">What Is ChatGPT?</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/3\">A Short Overview on Its Structure</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/3\">More on ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/4\">Some Applications</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/4\">Generating Synthetic Data Using ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/5\">Comparing ChatGPT to Other Similar Applications</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/6\">The Use of ChatGPT in Data Science</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/7\">Using ChatGPT to Assist in Programming</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/11\">The Future of ChatGPT in Data Science</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/12\">ChatGPT Is Not Always Correct</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/13\">ChatGPT in University and Data Science Programs: Emphasizing Ethics and Integrity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/14\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5IUVFSBU/14\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-16T01:08:58Z","dateModified":"2023-06-16T01:08:58Z","uri":"http://zotero.org/users/11367251/items/5IUVFSBU","localPath":"/Users/reyvababtista/Projects/Papers/hassaniRoleChatGPTData2023.pdf","defaultPath":"files/484/hassaniRoleChatGPTData2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400463,"version":0},"$loki":179},{"itemID":931,"item":{"key":"M7CTQBBE","version":1228,"itemType":"blogPost","title":"Understanding Core Web Vitals and Google search results","date":"05/23/2023","url":"https://developers.google.com/search/docs/appearance/core-web-vitals","accessDate":"2023-11-05","blogTitle":"Understanding Core Web Vitals and Google search results","creators":[{"name":"Google Search Central","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T04:06:13Z","dateModified":"2023-11-06T04:06:54Z","uri":"http://zotero.org/users/11367251/items/M7CTQBBE","itemID":931,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400464,"version":0},"$loki":180},{"itemID":1413,"item":{"key":"M9FQ9TMG","version":1550,"itemType":"preprint","title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?","abstractNote":"Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entity recognition can be used to improve performance on InfoSeek by retrieving relevant documents, showing a significant space for improvement.","date":"2023-10-17","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2302.11713","accessDate":"2023-11-08T22:42:48Z","extra":"arXiv:2302.11713 [cs]","repository":"arXiv","archiveID":"arXiv:2302.11713","creators":[{"firstName":"Yang","lastName":"Chen","creatorType":"author"},{"firstName":"Hexiang","lastName":"Hu","creatorType":"author"},{"firstName":"Yi","lastName":"Luan","creatorType":"author"},{"firstName":"Haitian","lastName":"Sun","creatorType":"author"},{"firstName":"Soravit","lastName":"Changpinyo","creatorType":"author"},{"firstName":"Alan","lastName":"Ritter","creatorType":"author"},{"firstName":"Ming-Wei","lastName":"Chang","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:42:48Z","dateModified":"2023-11-08T22:42:48Z","uri":"http://zotero.org/users/11367251/items/M9FQ9TMG","itemID":1413,"attachments":[{"key":"5YDYZCTB","version":1553,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2302.11713","accessDate":"2023-11-08T22:42:59Z","parentItem":"M9FQ9TMG","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2302.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:42:59Z","dateModified":"2023-11-08T22:42:59Z","uri":"http://zotero.org/users/11367251/items/5YDYZCTB","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/5YDYZCTB/2302.html","defaultPath":"files/1417/2302.html"},{"key":"7PEF45WT","version":1551,"itemType":"attachment","title":"chenCanPretrainedVision2023.pdf","parentItem":"M9FQ9TMG","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenCanPretrainedVision2023.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:42:54Z","dateModified":"2023-11-08T22:42:54Z","uri":"http://zotero.org/users/11367251/items/7PEF45WT","localPath":"/Users/reyvababtista/Projects/Papers/chenCanPretrainedVision2023.pdf","defaultPath":"files/1416/chenCanPretrainedVision2023.pdf"}],"notes":[{"key":"P374KNLH","version":1550,"itemType":"note","parentItem":"M9FQ9TMG","note":"Comment: EMNLP 2023 (main conference); Our dataset and evaluation is available at https://open-vision-language.github.io/infoseek/","tags":[],"relations":{},"dateAdded":"2023-11-08T22:42:48Z","dateModified":"2023-11-08T22:42:48Z","uri":"http://zotero.org/users/11367251/items/P374KNLH"}]},"meta":{"revision":0,"created":1709832400466,"version":0},"$loki":181},{"itemID":24,"item":{"key":"MC99V2LS","version":200,"itemType":"journalArticle","title":"Transfer Learning Based Traffic Sign Recognition Using Inception-v3 Model","abstractNote":"Traffic sign recognition is critical for advanced driver assistant system and road infrastructure survey. Traditional traffic sign recognition algorithms can't efficiently recognize traffic signs due to its limitation, yet deep learning-based technique requires huge amount of training data before its use, which is time consuming and labor intensive. In this study, transfer learning-based method is introduced for traffic sign recognition and classification, which significantly reduces the amount of training data and alleviates computation expense using Inception-v3 model. In our experiment, Belgium Traffic Sign Database is chosen and augmented by data pre-processing technique. Subsequently the layer-wise features extracted using different convolution and pooling operations are compared and analyzed. Finally transfer learning-based model is repetitively retrained several times with fine-tuning parameters at different learning rate, and excellent reliability and repeatability are observed based on statistical analysis. The results show that transfer learning model can achieve a high-level recognition performance in traffic sign recognition, which is up to 99.18 % of recognition accuracy at 0.05 learning rate (average accuracy of 99.09 %). This study would be beneficial in other traffic infrastructure recognition such as road lane marking and roadside protection facilities, and so on.","date":"2018-08-16","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://pp.bme.hu/tr/article/view/11480","accessDate":"2023-03-20T14:25:08Z","volume":"47","pages":"242-250","publicationTitle":"Periodica Polytechnica Transportation Engineering","DOI":"10.3311/PPtr.11480","issue":"3","journalAbbreviation":"Period. Polytech. Transp. Eng.","ISSN":"1587-3811, 0303-7800","creators":[{"firstName":"Chunmian","lastName":"Lin","creatorType":"author"},{"firstName":"Lin","lastName":"Li","creatorType":"author"},{"firstName":"Wenting","lastName":"Luo","creatorType":"author"},{"firstName":"Kelvin C. P.","lastName":"Wang","creatorType":"author"},{"firstName":"Jiangang","lastName":"Guo","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/2YU7NRIX"]},"dateAdded":"2023-03-20T14:25:08Z","dateModified":"2023-03-22T17:03:44Z","uri":"http://zotero.org/users/11367251/items/MC99V2LS","itemID":24,"attachments":[{"key":"IN4AFYX4","version":241,"itemType":"attachment","title":"linTransferLearningBased2018.pdf","parentItem":"MC99V2LS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/linTransferLearningBased2018.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_IN4AFYX4/1\">1 Introduction </a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/2\">2 Model Architecture </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/2\">2.1 Convolutional Neural Network </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/3\">2.2 Inception-v3 Model </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/4\">2.3 Transfer Learning Model </a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/5\">3 Case Study </a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/5\">3.1 Data Pre-processing </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/5\">3.2 Feature Representation  </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/6\">3.3 Training Details </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/7\">3.4 Result Analysis </a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/7\">4 Conclusions </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/8\">5 Recommendations  </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/8\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IN4AFYX4/8\">References </a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:08Z","dateModified":"2023-03-22T18:52:24Z","uri":"http://zotero.org/users/11367251/items/IN4AFYX4","localPath":"/Users/reyvababtista/Projects/Papers/linTransferLearningBased2018.pdf","defaultPath":"files/255/linTransferLearningBased2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400467,"version":0},"$loki":182},{"itemID":998,"item":{"key":"MCQGV8AH","version":1386,"itemType":"preprint","title":"UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training","abstractNote":"We present UnifiedQA-v2, a QA model built with the same process as UnifiedQA, except that it utilizes more supervision -- roughly 3x the number of datasets used for UnifiedQA. This generally leads to better in-domain and cross-domain results.","date":"2022-02-23","language":"en","shortTitle":"UnifiedQA-v2","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2202.12359","accessDate":"2023-11-07T23:32:09Z","extra":"arXiv:2202.12359 [cs]","repository":"arXiv","archiveID":"arXiv:2202.12359","creators":[{"firstName":"Daniel","lastName":"Khashabi","creatorType":"author"},{"firstName":"Yeganeh","lastName":"Kordi","creatorType":"author"},{"firstName":"Hannaneh","lastName":"Hajishirzi","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T23:32:09Z","dateModified":"2023-11-07T23:32:10Z","uri":"http://zotero.org/users/11367251/items/MCQGV8AH","itemID":998,"attachments":[{"key":"82LZSPX4","version":1389,"itemType":"attachment","title":"khashabiUnifiedQAv2StrongerGeneralization2022.pdf","parentItem":"MCQGV8AH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/khashabiUnifiedQAv2StrongerGeneralization2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T23:32:24Z","dateModified":"2023-11-07T23:32:24Z","uri":"http://zotero.org/users/11367251/items/82LZSPX4","localPath":"/Users/reyvababtista/Projects/Papers/khashabiUnifiedQAv2StrongerGeneralization2022.pdf","defaultPath":"files/1001/khashabiUnifiedQAv2StrongerGeneralization2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400467,"version":0},"$loki":183},{"itemID":295,"item":{"key":"ME43V57D","version":301,"itemType":"conferencePaper","title":"ExtraSensory App: Data Collection In-the-Wild with Rich User Interface to Self-Report Behavior","abstractNote":"We introduce a mobile app for collecting in-the-wild data, including sensor measurements and self-reported labels describing people’s behavioral context (e.g. driving, eating, in class, shower). Labeled data is necessary for developing contextrecognition systems that serve health monitoring, aging care, and more. Acquiring labels without observers is challenging and previous solutions compromised ecological validity, range of behaviors, or amount of data. Our user interface combines past and near-future self-reporting of combinations of relevant context-labels. We deployed the app on the personal smartphones of 60 users and analyzed quantitative data collected in-the-wild and qualitative user-experience reports. The interface’s ﬂexibility was important to gain frequent, detailed labels, support diverse behavioral situations, and engage different users: most preferred reporting their past behavior through a daily journal, but some preferred reporting what they’re about to do. We integrated insights from this work back into the app, which we make available to researchers for conducting in-the-wild studies.","date":"2018-04-21","language":"en","shortTitle":"ExtraSensory App","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/3173574.3174128","accessDate":"2023-03-26T21:25:23Z","place":"Montreal QC Canada","publisher":"ACM","ISBN":"978-1-4503-5620-6","pages":"1-12","proceedingsTitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","conferenceName":"CHI '18: CHI Conference on Human Factors in Computing Systems","DOI":"10.1145/3173574.3174128","creators":[{"firstName":"Yonatan","lastName":"Vaizman","creatorType":"author"},{"firstName":"Katherine","lastName":"Ellis","creatorType":"author"},{"firstName":"Gert","lastName":"Lanckriet","creatorType":"author"},{"firstName":"Nadir","lastName":"Weibel","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:25:23Z","dateModified":"2023-03-26T21:25:23Z","uri":"http://zotero.org/users/11367251/items/ME43V57D","itemID":295,"attachments":[{"key":"ZR75U2KJ","version":315,"itemType":"attachment","title":"vaizmanExtraSensoryAppData2018.pdf","parentItem":"ME43V57D","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/vaizmanExtraSensoryAppData2018.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZR75U2KJ/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/2\">Related work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/2\">Camera-based Approaches</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/2\">Self-Reporting In-Situ</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/2\">Self-Reporting by Recall</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/2\">Mixed Self-Reporting Approaches</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/3\">The ExtraSensory Mobile App</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/3\">Recording Sensors</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/3\">Reporting Context Labels</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/4\">Additional Visual Features</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/5\">User Deployment, Analysis and Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/5\">Quantitative Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/7\">Qualitative analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/9\">Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/10\">Revised ExtranSensory App</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/10\">Future directions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/10\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZR75U2KJ/11\">References </a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:24Z","dateModified":"2023-03-26T21:29:25Z","uri":"http://zotero.org/users/11367251/items/ZR75U2KJ","localPath":"/Users/reyvababtista/Projects/Papers/vaizmanExtraSensoryAppData2018.pdf","defaultPath":"files/309/vaizmanExtraSensoryAppData2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400468,"version":0},"$loki":184},{"itemID":926,"item":{"key":"MEUFJ3PS","version":1196,"itemType":"blogPost","title":"Rolling out the mobile-friendly update","date":"04/21/2015","url":"https://developers.google.com/search/blog/2015/04/rolling-out-mobile-friendly-update?safe=active","accessDate":"2023-11-04","blogTitle":"Rolling out the mobile-friendly update","creators":[{"name":"Takaki Makino","creatorType":"author"},{"name":"Doantam Phan","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-05T14:39:05Z","dateModified":"2023-11-05T14:40:12Z","uri":"http://zotero.org/users/11367251/items/MEUFJ3PS","itemID":926,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400468,"version":0},"$loki":185},{"itemID":982,"item":{"key":"MGE4A4HI","version":1362,"itemType":"preprint","title":"All You May Need for VQA are Image Captions","abstractNote":"Visual Question Answering (VQA) has beneﬁted from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-theart zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data.","date":"2022-05-04","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2205.01883","accessDate":"2023-11-07T22:45:42Z","extra":"arXiv:2205.01883 [cs]","repository":"arXiv","archiveID":"arXiv:2205.01883","creators":[{"firstName":"Soravit","lastName":"Changpinyo","creatorType":"author"},{"firstName":"Doron","lastName":"Kukliansky","creatorType":"author"},{"firstName":"Idan","lastName":"Szpektor","creatorType":"author"},{"firstName":"Xi","lastName":"Chen","creatorType":"author"},{"firstName":"Nan","lastName":"Ding","creatorType":"author"},{"firstName":"Radu","lastName":"Soricut","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T22:45:42Z","dateModified":"2023-11-07T22:45:42Z","uri":"http://zotero.org/users/11367251/items/MGE4A4HI","itemID":982,"attachments":[{"key":"S7HYAJV9","version":1364,"itemType":"attachment","title":"changpinyoAllYouMay2022.pdf","parentItem":"MGE4A4HI","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/changpinyoAllYouMay2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T22:45:47Z","dateModified":"2023-11-07T22:45:47Z","uri":"http://zotero.org/users/11367251/items/S7HYAJV9","localPath":"/Users/reyvababtista/Projects/Papers/changpinyoAllYouMay2022.pdf","defaultPath":"files/984/changpinyoAllYouMay2022.pdf"}],"notes":[{"key":"RLEELZW6","version":1362,"itemType":"note","parentItem":"MGE4A4HI","note":"Comment: 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2022)","tags":[],"relations":{},"dateAdded":"2023-11-07T22:45:42Z","dateModified":"2023-11-07T22:45:42Z","uri":"http://zotero.org/users/11367251/items/RLEELZW6"}]},"meta":{"revision":0,"created":1709832400469,"version":0},"$loki":186},{"itemID":590,"item":{"key":"MHP3GPNE","version":787,"itemType":"journalArticle","title":"The self-organizing map","date":"Sept./1990","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/58325/","accessDate":"2023-09-05T13:41:23Z","volume":"78","pages":"1464-1480","publicationTitle":"Proceedings of the IEEE","DOI":"10.1109/5.58325","issue":"9","journalAbbreviation":"Proc. IEEE","ISSN":"00189219","creators":[{"firstName":"T.","lastName":"Kohonen","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:41:23Z","dateModified":"2023-09-05T13:41:23Z","uri":"http://zotero.org/users/11367251/items/MHP3GPNE","itemID":590,"attachments":[{"key":"QVFRT999","version":818,"itemType":"attachment","title":"kohonenselforganizingmap1990.pdf","parentItem":"MHP3GPNE","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kohonenselforganizingmap1990.pdf","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:34Z","dateModified":"2023-09-05T13:47:34Z","uri":"http://zotero.org/users/11367251/items/QVFRT999","localPath":"/Users/reyvababtista/Projects/Papers/kohonenselforganizingmap1990.pdf","defaultPath":"files/626/kohonenselforganizingmap1990.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400469,"version":0},"$loki":187},{"itemID":427,"item":{"key":"MHTIJCUK","version":520,"itemType":"webpage","title":"Tesseract Open Source OCR Engine","date":"2023-05-04","url":"https://github.com/tesseract-ocr/tesseract","accessDate":"2023-05-04","websiteTitle":"Tesseract Open Source OCR Engine","creators":[{"name":"tesseract","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T03:35:30Z","dateModified":"2023-05-05T03:36:17Z","uri":"http://zotero.org/users/11367251/items/MHTIJCUK","itemID":427,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400469,"version":0},"$loki":188},{"itemID":1504,"item":{"key":"MM66WZUM","version":1689,"itemType":"blogPost","title":"Featured snippets and your website","date":"2023-08-21","url":"https://developers.google.com/search/docs/appearance/featured-snippets","accessDate":"2023-11-09","blogTitle":"Featured snippets and your website","creators":[{"name":"Google Search Central","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:23:00Z","dateModified":"2023-11-09T21:23:46Z","uri":"http://zotero.org/users/11367251/items/MM66WZUM","itemID":1504,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400470,"version":0},"$loki":189},{"itemID":746,"item":{"key":"MP7D688T","version":1001,"itemType":"preprint","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training","abstractNote":"Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNPVQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We ﬁrst generate questionguided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 (Goyal et al., 2017) and GQA (Hudson and Manning, 2019). With 11B parameters, it outperforms the 80Bparameter Flamingo model (Alayrac et al., 2022) by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1% on GQA over FewVLM (Jin et al., 2022) with 740M PLM parameters.","date":"2023-03-19","language":"en","shortTitle":"Plug-and-Play VQA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2210.08773","accessDate":"2023-10-02T02:08:54Z","extra":"arXiv:2210.08773 [cs]","repository":"arXiv","archiveID":"arXiv:2210.08773","creators":[{"firstName":"Anthony Meng Huat","lastName":"Tiong","creatorType":"author"},{"firstName":"Junnan","lastName":"Li","creatorType":"author"},{"firstName":"Boyang","lastName":"Li","creatorType":"author"},{"firstName":"Silvio","lastName":"Savarese","creatorType":"author"},{"firstName":"Steven C. H.","lastName":"Hoi","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-02T02:08:54Z","dateModified":"2023-10-02T02:08:54Z","uri":"http://zotero.org/users/11367251/items/MP7D688T","itemID":746,"attachments":[{"key":"X6KP2P23","version":1002,"itemType":"attachment","title":"tiongPlugandPlayVQAZeroshot2023.pdf","parentItem":"MP7D688T","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/tiongPlugandPlayVQAZeroshot2023.pdf","tags":[],"relations":{},"dateAdded":"2023-10-02T02:09:10Z","dateModified":"2023-10-02T02:09:10Z","uri":"http://zotero.org/users/11367251/items/X6KP2P23","localPath":"/Users/reyvababtista/Projects/Papers/tiongPlugandPlayVQAZeroshot2023.pdf","defaultPath":"files/748/tiongPlugandPlayVQAZeroshot2023.pdf"}],"notes":[{"key":"NIAI7FE6","version":1001,"itemType":"note","parentItem":"MP7D688T","note":"Comment: EMNLP 2022 (Findings); correct typos in Equation 2 on page 4","tags":[],"relations":{},"dateAdded":"2023-10-02T02:08:54Z","dateModified":"2023-10-02T02:08:54Z","uri":"http://zotero.org/users/11367251/items/NIAI7FE6"}]},"meta":{"revision":0,"created":1709832400470,"version":0},"$loki":190},{"itemID":1155,"item":{"key":"MR6GRNDN","version":1472,"itemType":"preprint","title":"Scene Text Visual Question Answering","abstractNote":"Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.","date":"2019-10-16","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1905.13648","accessDate":"2023-11-08T16:07:11Z","extra":"arXiv:1905.13648 [cs]","repository":"arXiv","archiveID":"arXiv:1905.13648","creators":[{"firstName":"Ali Furkan","lastName":"Biten","creatorType":"author"},{"firstName":"Ruben","lastName":"Tito","creatorType":"author"},{"firstName":"Andres","lastName":"Mafla","creatorType":"author"},{"firstName":"Lluis","lastName":"Gomez","creatorType":"author"},{"firstName":"Marçal","lastName":"Rusiñol","creatorType":"author"},{"firstName":"Ernest","lastName":"Valveny","creatorType":"author"},{"firstName":"C. V.","lastName":"Jawahar","creatorType":"author"},{"firstName":"Dimosthenis","lastName":"Karatzas","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T16:07:11Z","dateModified":"2023-11-08T16:07:11Z","uri":"http://zotero.org/users/11367251/items/MR6GRNDN","itemID":1155,"attachments":[{"key":"UBAG6MRM","version":1476,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1905.13648","accessDate":"2023-11-08T16:07:22Z","parentItem":"MR6GRNDN","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1905.html","tags":[],"relations":{},"dateAdded":"2023-11-08T16:07:22Z","dateModified":"2023-11-08T16:07:22Z","uri":"http://zotero.org/users/11367251/items/UBAG6MRM","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/UBAG6MRM/1905.html","defaultPath":"files/1159/1905.html"},{"key":"W5NYKES7","version":1473,"itemType":"attachment","title":"bitenSceneTextVisual2019.pdf","parentItem":"MR6GRNDN","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bitenSceneTextVisual2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T16:07:16Z","dateModified":"2023-11-08T16:07:16Z","uri":"http://zotero.org/users/11367251/items/W5NYKES7","localPath":"/Users/reyvababtista/Projects/Papers/bitenSceneTextVisual2019.pdf","defaultPath":"files/1158/bitenSceneTextVisual2019.pdf"}],"notes":[{"key":"AF4Q3QYI","version":1472,"itemType":"note","parentItem":"MR6GRNDN","note":"Comment: International Conference on Computer Vision (ICCV 2019)","tags":[],"relations":{},"dateAdded":"2023-11-08T16:07:11Z","dateModified":"2023-11-08T16:07:11Z","uri":"http://zotero.org/users/11367251/items/AF4Q3QYI"}]},"meta":{"revision":0,"created":1709832400471,"version":0},"$loki":191},{"itemID":438,"item":{"key":"MUT6I79G","version":544,"itemType":"conferencePaper","title":"Feature Pyramid Networks for Object Detection","abstractNote":"Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.","date":"7/2017","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/8099589/","accessDate":"2023-05-08T16:32:36Z","place":"Honolulu, HI","publisher":"IEEE","ISBN":"978-1-5386-0457-1","pages":"936-944","proceedingsTitle":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2017.106","creators":[{"firstName":"Tsung-Yi","lastName":"Lin","creatorType":"author"},{"firstName":"Piotr","lastName":"Dollar","creatorType":"author"},{"firstName":"Ross","lastName":"Girshick","creatorType":"author"},{"firstName":"Kaiming","lastName":"He","creatorType":"author"},{"firstName":"Bharath","lastName":"Hariharan","creatorType":"author"},{"firstName":"Serge","lastName":"Belongie","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-05-08T16:32:36Z","dateModified":"2023-05-08T16:32:36Z","uri":"http://zotero.org/users/11367251/items/MUT6I79G","itemID":438,"attachments":[{"key":"2BK7FTLD","version":547,"itemType":"attachment","title":"linFeaturePyramidNetworks2017.pdf","parentItem":"MUT6I79G","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/linFeaturePyramidNetworks2017.pdf","tags":[],"relations":{},"dateAdded":"2023-05-08T16:32:53Z","dateModified":"2023-05-08T16:32:53Z","uri":"http://zotero.org/users/11367251/items/2BK7FTLD","localPath":"/Users/reyvababtista/Projects/Papers/linFeaturePyramidNetworks2017.pdf","defaultPath":"files/439/linFeaturePyramidNetworks2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400471,"version":0},"$loki":192},{"itemID":1834,"item":{"key":"MUXYWQHB","version":2301,"itemType":"journalArticle","title":"NEU-chatbot: Chatbot for admission of National Economics University","date":"2021","language":"en","shortTitle":"NEU-chatbot","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S2666920X21000308","accessDate":"2024-01-19T15:20:32Z","volume":"2","pages":"100036","publicationTitle":"Computers and Education: Artificial Intelligence","DOI":"10.1016/j.caeai.2021.100036","journalAbbreviation":"Computers and Education: Artificial Intelligence","ISSN":"2666920X","creators":[{"firstName":"Trung Thanh","lastName":"Nguyen","creatorType":"author"},{"firstName":"Anh Duc","lastName":"Le","creatorType":"author"},{"firstName":"Ha Thanh","lastName":"Hoang","creatorType":"author"},{"firstName":"Tuan","lastName":"Nguyen","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:20:32Z","dateModified":"2024-01-19T15:20:32Z","uri":"http://zotero.org/users/11367251/items/MUXYWQHB","itemID":1834,"attachments":[{"key":"IYCYISMG","version":2302,"itemType":"attachment","title":"nguyenNEUchatbotChatbotadmission2021.pdf","parentItem":"MUXYWQHB","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/nguyenNEUchatbotChatbotadmission2021.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:20:42Z","dateModified":"2024-01-19T15:20:42Z","uri":"http://zotero.org/users/11367251/items/IYCYISMG","localPath":"/Users/reyvababtista/Projects/Papers/nguyenNEUchatbotChatbotadmission2021.pdf","defaultPath":"files/1836/nguyenNEUchatbotChatbotadmission2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400472,"version":0},"$loki":193},{"itemID":789,"item":{"key":"MVXDT24X","version":1041,"itemType":"journalArticle","title":"Deep learning","date":"2015-05-28","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.nature.com/articles/nature14539","accessDate":"2023-10-13T15:56:16Z","volume":"521","pages":"436-444","publicationTitle":"Nature","DOI":"10.1038/nature14539","issue":"7553","journalAbbreviation":"Nature","ISSN":"0028-0836, 1476-4687","creators":[{"firstName":"Yann","lastName":"LeCun","creatorType":"author"},{"firstName":"Yoshua","lastName":"Bengio","creatorType":"author"},{"firstName":"Geoffrey","lastName":"Hinton","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-13T15:56:16Z","dateModified":"2023-10-13T15:56:16Z","uri":"http://zotero.org/users/11367251/items/MVXDT24X","itemID":789,"attachments":[{"key":"XX9NYZYN","version":1044,"itemType":"attachment","title":"lecunDeeplearning2015.pdf","parentItem":"MVXDT24X","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/lecunDeeplearning2015.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_XX9NYZYN/1\">Main</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/1\">Supervised learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/3\">Backpropagation to train multilayer architectures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/4\">Convolutional neural networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/4\">Image understanding with deep convolutional networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/5\">Distributed representations and language processing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/6\">Recurrent neural networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/7\">The future of deep learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/9\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_XX9NYZYN/1\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-10-13T15:56:23Z","dateModified":"2023-10-13T15:56:23Z","uri":"http://zotero.org/users/11367251/items/XX9NYZYN","localPath":"/Users/reyvababtista/Projects/Papers/lecunDeeplearning2015.pdf","defaultPath":"files/790/lecunDeeplearning2015.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400472,"version":0},"$loki":194},{"itemID":22,"item":{"key":"MWLFNQCZ","version":200,"itemType":"journalArticle","title":"Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics","abstractNote":"Trafﬁc signs are critical for maintaining the safety and efﬁciency of our roads. Therefore, we need to carefully assess the capabilities and limitations of automated trafﬁc sign detection systems. Existing trafﬁc sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. To overcome the shortcomings in existing datasets, we introduced the CURE-TSDReal dataset, which is based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. We test the performance of two benchmark algorithms and show that severe conditions can result in an average performance degradation of 29% in precision and 68% in recall. We investigate the effect of challenging conditions through spectral analysis and show that challenging conditions can lead to distinct magnitude spectrum characteristics. Moreover, we show that mean magnitude spectrum of changes in video sequences under challenging conditions can be an indicator of detection performance. CURE-TSD-Real dataset is available online at https://github.com/olivesgatech/CURE-TSD.","date":"9/2020","language":"en","shortTitle":"Traffic Sign Detection under Challenging Conditions","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1908.11262","accessDate":"2023-03-20T14:25:06Z","extra":"arXiv:1908.11262 [cs, eess]","volume":"21","pages":"3663-3673","publicationTitle":"IEEE Transactions on Intelligent Transportation Systems","DOI":"10.1109/TITS.2019.2931429","issue":"9","journalAbbreviation":"IEEE Trans. Intell. Transport. Syst.","ISSN":"1524-9050, 1558-0016","creators":[{"firstName":"Dogancan","lastName":"Temel","creatorType":"author"},{"firstName":"Min-Hung","lastName":"Chen","creatorType":"author"},{"firstName":"Ghassan","lastName":"AlRegib","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Electrical Engineering and Systems Science - Image and Video Processing","type":1},{"tag":"Electrical Engineering and Systems Science - Signal Processing","type":1},{"tag":"I.2","type":1},{"tag":"I.4","type":1},{"tag":"I.5","type":1}],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/WGUHKTMD"]},"dateAdded":"2023-03-20T14:25:06Z","dateModified":"2023-03-22T17:03:37Z","uri":"http://zotero.org/users/11367251/items/MWLFNQCZ","itemID":22,"attachments":[{"key":"LS5NF52I","version":240,"itemType":"attachment","title":"temelTrafficSignDetection2020.pdf","parentItem":"MWLFNQCZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/temelTrafficSignDetection2020.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_LS5NF52I/2\">I Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/3\">II Existing Datasets</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LS5NF52I/3\">III CURE-TSD-Real Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/3\">III-A General Information</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/5\">III-B Challenging Conditions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/7\">III-C Benchmark Algorithms</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LS5NF52I/7\">IV Traffic Sign Detection under Challenging Conditions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/7\">IV-A Performance Metrics</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/7\">IV-B Training and Test Sets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/7\">IV-C Performance Variation under Challenging Conditions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/9\">IV-D Spectral Analysis of Challenging Conditions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/11\">IV-E Detection Performance versus Spectral Characteristics</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/12\">V Conclusion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_LS5NF52I/13\">Biographies</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/13\">Dogancan Temel</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/13\">Min-Hung Chen</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_LS5NF52I/14\">Ghassan AlRegib</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:01Z","dateModified":"2023-03-22T18:52:22Z","uri":"http://zotero.org/users/11367251/items/LS5NF52I","localPath":"/Users/reyvababtista/Projects/Papers/temelTrafficSignDetection2020.pdf","defaultPath":"files/245/temelTrafficSignDetection2020.pdf"}],"notes":[{"key":"8SJRUL4M","version":6,"itemType":"note","parentItem":"MWLFNQCZ","note":"Comment: 13 pages, 9 figures, 4 tables. IEEE Transactions on Intelligent Transportation Systems, 2019","tags":[],"relations":{},"dateAdded":"2023-03-20T14:25:06Z","dateModified":"2023-03-20T14:25:06Z","uri":"http://zotero.org/users/11367251/items/8SJRUL4M"},{"key":"MFWYTPNF","version":200,"itemType":"note","parentItem":"MWLFNQCZ","note":"Comment: 13 pages, 9 figures, 4 tables. IEEE Transactions on Intelligent Transportation Systems, 2019","tags":[],"relations":{},"dateAdded":"2023-03-22T17:02:33Z","dateModified":"2023-03-22T17:03:37Z","uri":"http://zotero.org/users/11367251/items/MFWYTPNF"}]},"meta":{"revision":0,"created":1709832400474,"version":0},"$loki":195},{"itemID":1692,"item":{"key":"N3A7WFZM","version":2151,"itemType":"journalArticle","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation","abstractNote":"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.","date":"2022","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2206.10789","accessDate":"2023-12-06T23:07:04Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.2206.10789","creators":[{"firstName":"Jiahui","lastName":"Yu","creatorType":"author"},{"firstName":"Yuanzhong","lastName":"Xu","creatorType":"author"},{"firstName":"Jing Yu","lastName":"Koh","creatorType":"author"},{"firstName":"Thang","lastName":"Luong","creatorType":"author"},{"firstName":"Gunjan","lastName":"Baid","creatorType":"author"},{"firstName":"Zirui","lastName":"Wang","creatorType":"author"},{"firstName":"Vijay","lastName":"Vasudevan","creatorType":"author"},{"firstName":"Alexander","lastName":"Ku","creatorType":"author"},{"firstName":"Yinfei","lastName":"Yang","creatorType":"author"},{"firstName":"Burcu Karagol","lastName":"Ayan","creatorType":"author"},{"firstName":"Ben","lastName":"Hutchinson","creatorType":"author"},{"firstName":"Wei","lastName":"Han","creatorType":"author"},{"firstName":"Zarana","lastName":"Parekh","creatorType":"author"},{"firstName":"Xin","lastName":"Li","creatorType":"author"},{"firstName":"Han","lastName":"Zhang","creatorType":"author"},{"firstName":"Jason","lastName":"Baldridge","creatorType":"author"},{"firstName":"Yonghui","lastName":"Wu","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:07:04Z","dateModified":"2023-12-06T23:07:04Z","uri":"http://zotero.org/users/11367251/items/N3A7WFZM","itemID":1692,"attachments":[],"notes":[{"key":"RMIV5GJ5","version":2151,"itemType":"note","parentItem":"N3A7WFZM","note":"<h2>Other</h2>\nPreprint","tags":[],"relations":{},"dateAdded":"2023-12-06T23:07:04Z","dateModified":"2023-12-06T23:07:04Z","uri":"http://zotero.org/users/11367251/items/RMIV5GJ5"}]},"meta":{"revision":0,"created":1709832400475,"version":0},"$loki":196},{"itemID":569,"item":{"key":"N4QXN8XD","version":763,"itemType":"journalArticle","title":"Transformer in Transformer","abstractNote":"Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers ﬁrst divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not ﬁne enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Speciﬁcally, we regard the local patches (e.g., 16×16) as “visual sentences” and present to further divide them into smaller patches (e.g., 4×4) as “visual words”. The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/ tree/master/research/cv/TNT.","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Kai","lastName":"Han","creatorType":"author"},{"firstName":"An","lastName":"Xiao","creatorType":"author"},{"firstName":"Enhua","lastName":"Wu","creatorType":"author"},{"firstName":"Jianyuan","lastName":"Guo","creatorType":"author"},{"firstName":"Chunjing","lastName":"Xu","creatorType":"author"},{"firstName":"Yunhe","lastName":"Wang","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:32:43Z","dateModified":"2023-09-05T13:32:44Z","uri":"http://zotero.org/users/11367251/items/N4QXN8XD","itemID":569,"attachments":[{"key":"2HMTTL6F","version":818,"itemType":"attachment","title":"hanTransformerTransformer.pdf","parentItem":"N4QXN8XD","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hanTransformerTransformer.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_2HMTTL6F/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/3\">Approach</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/3\">Preliminaries</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/3\">Transformer in Transformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/4\">Complexity Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/5\">Network Architecture</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/5\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/6\">Datasets and Experimental Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/6\">TNT on ImageNet</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/7\">Ablation Studies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/8\">Visualization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/9\">Transfer Learning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_2HMTTL6F/10\">Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:34Z","dateModified":"2023-09-05T13:47:35Z","uri":"http://zotero.org/users/11367251/items/2HMTTL6F","localPath":"/Users/reyvababtista/Projects/Papers/hanTransformerTransformer.pdf","defaultPath":"files/627/hanTransformerTransformer.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400475,"version":0},"$loki":197},{"itemID":304,"item":{"key":"N4W6HAQX","version":310,"itemType":"journalArticle","title":"Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch: The Apple Heart Study","abstractNote":"Background Smartwatch and fitness band wearable consumer electronics can passively measure pulse rate from the wrist using photoplethysmography (PPG). Identification of pulse irregularity or variability from these data has the potential to identify atrial fibrillation or atrial flutter (AF, collectively). The rapidly expanding consumer base of these devices allows for detection of undiagnosed AF at scale.\nMethods The Apple Heart Study is a prospective, single arm pragmatic study that has enrolled 419,093 participants (NCT03335800). The primary objective is to measure the proportion of participants with an irregular pulse detected by the Apple Watch (Apple Inc, Cupertino, CA) with AF on subsequent ambulatory ECG patch monitoring. The secondary objectives are to: 1) characterize the concordance of pulse irregularity notification episodes from the Apple Watch with simultaneously recorded ambulatory ECGs; 2) estimate the rate of initial contact with a health care provider within 3 months after notification of pulse irregularity. The study is conducted virtually, with screening, consent and data collection performed electronically from within an accompanying smartphone app. Study visits are performed by telehealth study physicians via video chat through the app, and ambulatory ECG patches are mailed to the participants.\nConclusions The results of this trial will provide initial evidence for the ability of a smartwatch algorithm to identify pulse irregularity and variability which may reflect previously unknown AF. The Apple Heart Study will help provide a foundation for how wearable technology can inform the clinical approach to AF identification and screening. (Am Heart J 2019;207:66-75.)","date":"01/2019","language":"en","shortTitle":"Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S0002870318302710","accessDate":"2023-03-26T21:27:50Z","volume":"207","pages":"66-75","publicationTitle":"American Heart Journal","DOI":"10.1016/j.ahj.2018.09.002","journalAbbreviation":"American Heart Journal","ISSN":"00028703","creators":[{"firstName":"Mintu P.","lastName":"Turakhia","creatorType":"author"},{"firstName":"Manisha","lastName":"Desai","creatorType":"author"},{"firstName":"Haley","lastName":"Hedlin","creatorType":"author"},{"firstName":"Amol","lastName":"Rajmane","creatorType":"author"},{"firstName":"Nisha","lastName":"Talati","creatorType":"author"},{"firstName":"Todd","lastName":"Ferris","creatorType":"author"},{"firstName":"Sumbul","lastName":"Desai","creatorType":"author"},{"firstName":"Divya","lastName":"Nag","creatorType":"author"},{"firstName":"Mithun","lastName":"Patel","creatorType":"author"},{"firstName":"Peter","lastName":"Kowey","creatorType":"author"},{"firstName":"John S.","lastName":"Rumsfeld","creatorType":"author"},{"firstName":"Andrea M.","lastName":"Russo","creatorType":"author"},{"firstName":"Mellanie True","lastName":"Hills","creatorType":"author"},{"firstName":"Christopher B.","lastName":"Granger","creatorType":"author"},{"firstName":"Kenneth W.","lastName":"Mahaffey","creatorType":"author"},{"firstName":"Marco V.","lastName":"Perez","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:27:50Z","dateModified":"2023-03-26T21:27:50Z","uri":"http://zotero.org/users/11367251/items/N4W6HAQX","itemID":304,"attachments":[{"key":"AIDKMA89","version":315,"itemType":"attachment","title":"turakhiaRationaledesignlargescale2019.pdf","parentItem":"N4W6HAQX","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/turakhiaRationaledesignlargescale2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_AIDKMA89/2\">Rationale for the Apple Heart Study</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AIDKMA89/2\">Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/2\">Trial overview and objectives</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AIDKMA89/2\">Study procedures</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/2\">Screening, eligibility, and consent</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/4\">Monitoring and study intervention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/4\">Initial Study Telehealth Visit</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/4\">Ambulatory ECG monitoring</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/5\">Study Visit #2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/5\">Adverse events description and ascertainment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/5\">Participant-reported outcomes</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AIDKMA89/6\">Statistical considerations</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/6\">Planned analyses</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/6\">Sample size considerations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/7\">Study organization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/7\">Data flow, privacy, and security</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/8\">Study launch and enrollment</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_AIDKMA89/8\">Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/8\">Limitations</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/9\">Summary</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/9\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/9\">Disclosures</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/9\">Appendix. Supplementary data</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_AIDKMA89/9\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:23Z","dateModified":"2023-03-26T21:29:24Z","uri":"http://zotero.org/users/11367251/items/AIDKMA89","localPath":"/Users/reyvababtista/Projects/Papers/turakhiaRationaledesignlargescale2019.pdf","defaultPath":"files/307/turakhiaRationaledesignlargescale2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400476,"version":0},"$loki":198},{"itemID":372,"item":{"key":"N687ZCF7","version":415,"itemType":"journalArticle","title":"Multi-modal data collection for measuring health, behavior, and living environment of large-scale participant cohorts","abstractNote":"Abstract\n            \n              Background\n              As mobile technologies become ever more sensor-rich, portable, and ubiquitous, data captured by smart devices are lending rich insights into users’ daily lives with unprecedented comprehensiveness and ecological validity. A number of human-subject studies have been conducted to examine the use of mobile sensing to uncover individual behavioral patterns and health outcomes, yet minimal attention has been placed on measuring living environments together with other human-centered sensing data. Moreover, the participant sample size in most existing studies falls well below a few hundred, leaving questions open about the reliability of findings on the relations between mobile sensing signals and human outcomes.\n            \n            \n              Results\n              To address these limitations, we developed a home environment sensor kit for continuous indoor air quality tracking and deployed it in conjunction with smartphones, Fitbits, and ecological momentary assessments in a cohort study of up to 1,584 college student participants per data type for 3 weeks. We propose a conceptual framework that systematically organizes human-centric data modalities by their temporal coverage and spatial freedom. Then we report our study procedure, technologies and methods deployed, and descriptive statistics of the collected data that reflect the participants’ mood, sleep, behavior, and living environment.\n            \n            \n              Conclusions\n              We were able to collect from a large participant cohort satisfactorily complete multi-modal sensing and survey data in terms of both data continuity and participant adherence. Our novel data and conceptual development provide important guidance for data collection and hypothesis generation in future human-centered sensing studies.","date":"2021-06-21","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giab044/6307430","accessDate":"2023-04-11T22:08:44Z","volume":"10","pages":"giab044","publicationTitle":"GigaScience","DOI":"10.1093/gigascience/giab044","issue":"6","ISSN":"2047-217X","creators":[{"firstName":"Congyu","lastName":"Wu","creatorType":"author"},{"firstName":"Hagen","lastName":"Fritz","creatorType":"author"},{"firstName":"Sepehr","lastName":"Bastami","creatorType":"author"},{"firstName":"Juan P","lastName":"Maestre","creatorType":"author"},{"firstName":"Edison","lastName":"Thomaz","creatorType":"author"},{"firstName":"Christine","lastName":"Julien","creatorType":"author"},{"firstName":"Darla M","lastName":"Castelli","creatorType":"author"},{"firstName":"Kaya","lastName":"de Barbaro","creatorType":"author"},{"firstName":"Sarah Kate","lastName":"Bearman","creatorType":"author"},{"firstName":"Gabriella M","lastName":"Harari","creatorType":"author"},{"firstName":"R","lastName":"Cameron Craddock","creatorType":"author"},{"firstName":"Kerry A","lastName":"Kinney","creatorType":"author"},{"firstName":"Samuel D","lastName":"Gosling","creatorType":"author"},{"firstName":"David M","lastName":"Schnyer","creatorType":"author"},{"firstName":"Zoltan","lastName":"Nagy","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-04-11T22:08:44Z","dateModified":"2023-04-11T22:08:44Z","uri":"http://zotero.org/users/11367251/items/N687ZCF7","itemID":372,"attachments":[{"key":"RABXBVM7","version":416,"itemType":"attachment","title":"wuMultimodaldatacollection2021.pdf","parentItem":"N687ZCF7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/wuMultimodaldatacollection2021.pdf","tags":[],"relations":{},"dateAdded":"2023-04-11T22:08:48Z","dateModified":"2023-04-11T22:08:48Z","uri":"http://zotero.org/users/11367251/items/RABXBVM7","localPath":"/Users/reyvababtista/Projects/Papers/wuMultimodaldatacollection2021.pdf","defaultPath":"files/373/wuMultimodaldatacollection2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400476,"version":0},"$loki":199},{"itemID":1650,"item":{"key":"N6GNAKCE","version":2075,"itemType":"journalArticle","title":"Greedy function approximation: A gradient boosting machine.","date":"2001-10-1","shortTitle":"Greedy function approximation","libraryCatalog":"DOI.org (Crossref)","url":"https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full","accessDate":"2023-12-04T00:25:37Z","volume":"29","publicationTitle":"The Annals of Statistics","DOI":"10.1214/aos/1013203451","issue":"5","journalAbbreviation":"Ann. Statist.","ISSN":"0090-5364","creators":[{"firstName":"Jerome H.","lastName":"Friedman","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T00:25:37Z","dateModified":"2023-12-04T00:25:37Z","uri":"http://zotero.org/users/11367251/items/N6GNAKCE","itemID":1650,"attachments":[{"key":"PY4XRU97","version":2075,"itemType":"attachment","title":"friedmanGreedyfunctionapproximation2001.pdf","parentItem":"N6GNAKCE","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/friedmanGreedyfunctionapproximation2001.pdf","tags":[],"relations":{},"dateAdded":"2023-12-04T00:25:39Z","dateModified":"2023-12-04T00:25:39Z","uri":"http://zotero.org/users/11367251/items/PY4XRU97","localPath":"/Users/reyvababtista/Projects/Papers/friedmanGreedyfunctionapproximation2001.pdf","defaultPath":"files/1652/friedmanGreedyfunctionapproximation2001.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400478,"version":0},"$loki":200},{"itemID":330,"item":{"key":"N7K6349N","version":338,"itemType":"preprint","title":"Extraction of Behavioral Features from Smartphone and Wearable Data","abstractNote":"The rich set of sensors in smartphones and wearable devices provides the possibility to passively collect streams of data in the wild. The raw data streams, however, can rarely be directly used in the modeling pipeline. We provide a generic framework that can process raw data streams and extract useful features related to non-verbal human behavior. This framework can be used by researchers in the field who are interested in processing data from smartphones and Wearable devices.","date":"2019-01-08","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1812.10394","accessDate":"2023-03-27T03:05:30Z","extra":"arXiv:1812.10394 [cs, stat]","repository":"arXiv","archiveID":"arXiv:1812.10394","creators":[{"firstName":"Afsaneh","lastName":"Doryab","creatorType":"author"},{"firstName":"Prerna","lastName":"Chikarsel","creatorType":"author"},{"firstName":"Xinwen","lastName":"Liu","creatorType":"author"},{"firstName":"Anind K.","lastName":"Dey","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computers and Society","type":1},{"tag":"Computer Science - Human-Computer Interaction","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-27T03:05:30Z","dateModified":"2023-03-27T03:05:30Z","uri":"http://zotero.org/users/11367251/items/N7K6349N","itemID":330,"attachments":[{"key":"MCL5DZ8N","version":341,"itemType":"attachment","title":"doryabExtractionBehavioralFeatures2019.pdf","parentItem":"N7K6349N","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/doryabExtractionBehavioralFeatures2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MCL5DZ8N/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/2\">2 Data Processing and Feature Extraction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/2\">2.1 Bluetooth Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/3\">2.2 Calls Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/3\">2.3 Location Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/4\">2.4 Places Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/4\">2.5 Phone Usage Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/4\">2.6 Sleep Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/5\">2.7 Steps Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/5\">2.8 Behavioral Change Features</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/5\">3 SUMMARY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MCL5DZ8N/5\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-27T03:06:02Z","dateModified":"2023-03-27T03:06:02Z","uri":"http://zotero.org/users/11367251/items/MCL5DZ8N","localPath":"/Users/reyvababtista/Projects/Papers/doryabExtractionBehavioralFeatures2019.pdf","defaultPath":"files/331/doryabExtractionBehavioralFeatures2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400548,"version":0},"$loki":201},{"itemID":1664,"item":{"key":"N9G4EIHF","version":2086,"itemType":"book","title":"Perceptrons: An Introduction to Computational Geometry","date":"2017","language":"en","shortTitle":"Perceptrons","libraryCatalog":"DOI.org (Crossref)","url":"https://direct.mit.edu/books/book/3132/perceptronsan-introduction-to-computational","accessDate":"2023-12-04T03:09:13Z","extra":"DOI: 10.7551/mitpress/11301.001.0001","publisher":"The MIT Press","ISBN":"978-0-262-34393-0","creators":[{"firstName":"Marvin","lastName":"Minsky","creatorType":"author"},{"firstName":"Seymour A.","lastName":"Papert","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T03:09:13Z","dateModified":"2023-12-04T03:09:13Z","uri":"http://zotero.org/users/11367251/items/N9G4EIHF","itemID":1664,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400549,"version":0},"$loki":202},{"itemID":933,"item":{"key":"N9T28ZEH","version":1241,"itemType":"blogPost","title":"What is Multimodal Search: \"LLMs with vision\" change businesses","date":"08/21/2023","url":"https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search","accessDate":"2023-11-05","blogTitle":"What is Multimodal Search: \"LLMs with vision\" change businesses","creators":[{"name":"Kaz Sato","creatorType":"author"},{"name":"Ivan Cheung","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T04:23:52Z","dateModified":"2023-11-06T04:25:19Z","uri":"http://zotero.org/users/11367251/items/N9T28ZEH","itemID":933,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400550,"version":0},"$loki":203},{"itemID":1683,"item":{"key":"NBD56BZS","version":2137,"itemType":"blogPost","title":"Supervised Learning","date":"2023","url":"https://databasetown.com/supervised-learning-algorithms/","accessDate":"2023-12-05","blogTitle":"Supervised Learning","creators":[{"name":"Database Town","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:13:55Z","dateModified":"2023-12-05T21:15:11Z","uri":"http://zotero.org/users/11367251/items/NBD56BZS","itemID":1683,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400550,"version":0},"$loki":204},{"itemID":1619,"item":{"key":"NHWDJ3NF","version":1973,"itemType":"preprint","title":"Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions","abstractNote":"Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. These claims are supported by extensive numerical experiments and a detailed error analysis.","date":"2010-12-14","shortTitle":"Finding structure with randomness","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/0909.4061","accessDate":"2023-11-29T05:01:55Z","extra":"arXiv:0909.4061 [math]","repository":"arXiv","archiveID":"arXiv:0909.4061","creators":[{"firstName":"Nathan","lastName":"Halko","creatorType":"author"},{"firstName":"Per-Gunnar","lastName":"Martinsson","creatorType":"author"},{"firstName":"Joel A.","lastName":"Tropp","creatorType":"author"}],"tags":[{"tag":"Mathematics - Numerical Analysis","type":1},{"tag":"Mathematics - Probability","type":1}],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T05:01:55Z","dateModified":"2023-11-29T05:01:55Z","uri":"http://zotero.org/users/11367251/items/NHWDJ3NF","itemID":1619,"attachments":[{"key":"WU2AS5SK","version":1975,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/0909.4061","accessDate":"2023-11-29T05:02:03Z","parentItem":"NHWDJ3NF","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"0909.html","tags":[],"relations":{},"dateAdded":"2023-11-29T05:02:03Z","dateModified":"2023-11-29T05:02:03Z","uri":"http://zotero.org/users/11367251/items/WU2AS5SK","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/WU2AS5SK/0909.html","defaultPath":"files/1622/0909.html"},{"key":"SDC9P7GV","version":1973,"itemType":"attachment","title":"halkoFindingstructurerandomness2010.pdf","parentItem":"NHWDJ3NF","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/halkoFindingstructurerandomness2010.pdf","tags":[],"relations":{},"dateAdded":"2023-11-29T05:01:58Z","dateModified":"2023-11-29T05:01:58Z","uri":"http://zotero.org/users/11367251/items/SDC9P7GV","localPath":"/Users/reyvababtista/Projects/Papers/halkoFindingstructurerandomness2010.pdf","defaultPath":"files/1621/halkoFindingstructurerandomness2010.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400553,"version":0},"$loki":205},{"itemID":637,"item":{"key":"NI3367WG","version":830,"itemType":"preprint","title":"MyCrunchGPT: A chatGPT assisted framework for scientific machine learning","abstractNote":"Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still timeconsuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate MyCrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of MyCrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the MyCrunchGPT, and create an infrastructure that can facilitate a broader vision, we built a webapp based guided user interface, that includes options for a comprehensive summary report. The overall objective is to extend MyCrunchGPT to handle diverse problems in computational mechanics, design, optimization and controls, and general scientific computing tasks involved in SciML, hence using it as a research assistant tool but also as an educational tool. While here the examples focus in fluid mechanics, future versions will target solid mechanics and materials science, geophysics, systems biology and bioinformatics.","date":"2023-07-31","language":"en","shortTitle":"MyCrunchGPT","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2306.15551","accessDate":"2023-09-07T21:48:36Z","extra":"arXiv:2306.15551 [physics]","repository":"arXiv","archiveID":"arXiv:2306.15551","creators":[{"firstName":"Varun","lastName":"Kumar","creatorType":"author"},{"firstName":"Leonard","lastName":"Gleyzer","creatorType":"author"},{"firstName":"Adar","lastName":"Kahana","creatorType":"author"},{"firstName":"Khemraj","lastName":"Shukla","creatorType":"author"},{"firstName":"George Em","lastName":"Karniadakis","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Physics - Physics and Society","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-07T21:48:36Z","dateModified":"2023-09-07T21:48:36Z","uri":"http://zotero.org/users/11367251/items/NI3367WG","itemID":637,"attachments":[{"key":"3H3WTMKW","version":832,"itemType":"attachment","title":"kumarMyCrunchGPTchatGPTassisted2023.pdf","parentItem":"NI3367WG","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kumarMyCrunchGPTchatGPTassisted2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_3H3WTMKW/1\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/2\">Objective and contributions</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/2\">Scientific Machine Learning (SciML)</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/3\">Physics-Informed Neural Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/3\">Deep Operator Network (DeepONet)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/4\">MyCrunchGPT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/4\">MyCrunchGPT for Operator-based Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/5\">MyCrunchGPT for PINN-based simulations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/5\">MyCrunchGPT web application interface</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/6\">Exemplar for MyCrunchGPT assisted Operator-based Design framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/7\">Problem subset 1: Generating new airfoil designs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/7\">Problem subset 2: Analyzing flow fields around new designs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/8\">Problem subset3: Generate optimized airfoil geometry</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/8\">Use-case analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/9\">Workflow for DeepONet-based airfoil design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/14\">MyCrunchGPT assisted result validation and comparison</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/16\">Exemplar for AI-assisted PINN-based analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_3H3WTMKW/18\">Summary</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-07T21:48:41Z","dateModified":"2023-09-07T21:48:42Z","uri":"http://zotero.org/users/11367251/items/3H3WTMKW","localPath":"/Users/reyvababtista/Projects/Papers/kumarMyCrunchGPTchatGPTassisted2023.pdf","defaultPath":"files/639/kumarMyCrunchGPTchatGPTassisted2023.pdf"}],"notes":[{"key":"SN3SKFK5","version":830,"itemType":"note","parentItem":"NI3367WG","note":"Comment: Updated title, abstract and added references","tags":[],"relations":{},"dateAdded":"2023-09-07T21:48:36Z","dateModified":"2023-09-07T21:48:36Z","uri":"http://zotero.org/users/11367251/items/SN3SKFK5"}]},"meta":{"revision":0,"created":1709832400558,"version":0},"$loki":206},{"itemID":327,"item":{"key":"NK3NS8AS","version":333,"itemType":"journalArticle","title":"A “one-size-fits-most” walking recognition method for smartphones, smartwatches, and wearable accelerometers","abstractNote":"Abstract\n            The ubiquity of personal digital devices offers unprecedented opportunities to study human behavior. Current state-of-the-art methods quantify physical activity using “activity counts,” a measure which overlooks specific types of physical activities. We propose a walking recognition method for sub-second tri-axial accelerometer data, in which activity classification is based on the inherent features of walking: intensity, periodicity, and duration. We validate our method against 20 publicly available, annotated datasets on walking activity data collected at various body locations (thigh, waist, chest, arm, wrist). We demonstrate that our method can estimate walking periods with high sensitivity and specificity: average sensitivity ranged between 0.92 and 0.97 across various body locations, and average specificity for common daily activities was typically above 0.95. We also assess the method’s algorithmic fairness to demographic and anthropometric variables and measurement contexts (body location, environment). Finally, we release our method as open-source software in Python and MATLAB.","date":"2023-02-23","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://www.nature.com/articles/s41746-022-00745-z","accessDate":"2023-03-27T02:42:11Z","volume":"6","pages":"29","publicationTitle":"npj Digital Medicine","DOI":"10.1038/s41746-022-00745-z","issue":"1","journalAbbreviation":"npj Digit. Med.","ISSN":"2398-6352","creators":[{"firstName":"Marcin","lastName":"Straczkiewicz","creatorType":"author"},{"firstName":"Emily J.","lastName":"Huang","creatorType":"author"},{"firstName":"Jukka-Pekka","lastName":"Onnela","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-27T02:42:11Z","dateModified":"2023-03-27T02:42:11Z","uri":"http://zotero.org/users/11367251/items/NK3NS8AS","itemID":327,"attachments":[{"key":"G5Q6TY8Y","version":336,"itemType":"attachment","title":"straczkiewiczonesizefitsmostwalkingrecognition2023.pdf","parentItem":"NK3NS8AS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/straczkiewiczonesizefitsmostwalkingrecognition2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_G5Q6TY8Y/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/2\">Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/2\">Method summary</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/4\">Data summary</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/5\">Tuning parameter selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/5\">Method evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/5\">Bias estimation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/6\">Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/9\">Methods</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/9\">Acceleration signal of walking activity</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/10\">Continuous wavelet transform</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/11\">Walking recognition algorithm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/13\">Data description</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/13\">Data preprocessing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/14\">Tuning parameter selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/14\">Method evaluation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/14\">Bias estimation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/15\">Reporting summary</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/15\">DATA AVAILABILITY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/15\">References</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/16\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/16\">Author contributions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/16\">Competing interests</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_G5Q6TY8Y/16\">ADDITIONAL INFORMATION</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-27T02:42:39Z","dateModified":"2023-03-27T02:42:39Z","uri":"http://zotero.org/users/11367251/items/G5Q6TY8Y","localPath":"/Users/reyvababtista/Projects/Papers/straczkiewiczonesizefitsmostwalkingrecognition2023.pdf","defaultPath":"files/328/straczkiewiczonesizefitsmostwalkingrecognition2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400560,"version":0},"$loki":207},{"itemID":1448,"item":{"key":"NQI9WD8H","version":1588,"itemType":"preprint","title":"Learning Robust Global Representations by Penalizing Local Predictive Power","abstractNote":"Despite their renowned predictive power on i.i.d. data, convolutional neural networks are known to rely more on high-frequency patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structures of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization out of the domain. Also, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images, that matches the ImageNet classification validation set in categories and scale.","date":"2019-11-04","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1905.13549","accessDate":"2023-11-08T22:52:29Z","extra":"arXiv:1905.13549 [cs]","repository":"arXiv","archiveID":"arXiv:1905.13549","creators":[{"firstName":"Haohan","lastName":"Wang","creatorType":"author"},{"firstName":"Songwei","lastName":"Ge","creatorType":"author"},{"firstName":"Eric P.","lastName":"Xing","creatorType":"author"},{"firstName":"Zachary C.","lastName":"Lipton","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:52:29Z","dateModified":"2023-11-08T22:52:29Z","uri":"http://zotero.org/users/11367251/items/NQI9WD8H","itemID":1448,"attachments":[{"key":"28IZGIQ5","version":1592,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1905.13549","accessDate":"2023-11-08T22:53:10Z","parentItem":"NQI9WD8H","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1905.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:53:10Z","dateModified":"2023-11-08T22:53:10Z","uri":"http://zotero.org/users/11367251/items/28IZGIQ5","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/28IZGIQ5/1905.html","defaultPath":"files/1451/1905.html"},{"key":"W74JUS3J","version":1589,"itemType":"attachment","title":"wangLearningRobustGlobal2019.pdf","parentItem":"NQI9WD8H","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/wangLearningRobustGlobal2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:53:05Z","dateModified":"2023-11-08T22:53:05Z","uri":"http://zotero.org/users/11367251/items/W74JUS3J","localPath":"/Users/reyvababtista/Projects/Papers/wangLearningRobustGlobal2019.pdf","defaultPath":"files/1450/wangLearningRobustGlobal2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400562,"version":0},"$loki":208},{"itemID":1614,"item":{"key":"NS6IG4KC","version":2190,"itemType":"webpage","title":"Streamlit","date":"11/28/2023","url":"https://streamlit.io/","accessDate":"2023-11-28","websiteTitle":"Streamlit","creators":[{"name":"Streamlit","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T04:27:29Z","dateModified":"2023-12-08T17:19:07Z","uri":"http://zotero.org/users/11367251/items/NS6IG4KC","itemID":1614,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400563,"version":0},"$loki":209},{"itemID":1626,"item":{"key":"NSIIB2W5","version":1986,"itemType":"attachment","title":"vandermaaten08a.bib","linkMode":"imported_file","contentType":"text/plain","charset":"windows-1252","filename":"vandermaaten08a.bib","tags":[],"collections":[],"relations":{},"dateAdded":"2023-11-29T05:07:15Z","dateModified":"2023-11-29T05:07:16Z","uri":"http://zotero.org/users/11367251/items/NSIIB2W5","itemID":1626,"localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/NSIIB2W5/vandermaaten08a.bib","defaultPath":"files/1626/vandermaaten08a.bib"},"meta":{"revision":0,"created":1709832400564,"version":0},"$loki":210},{"itemID":429,"item":{"key":"P2U2TAUC","version":527,"itemType":"webpage","title":"MMOCR","date":"2023-05-04","url":"https://github.com/open-mmlab/mmocr","accessDate":"2023-05-04","websiteTitle":"MMOCR","creators":[{"name":"OpenMMLab","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T03:44:51Z","dateModified":"2023-05-05T03:46:16Z","uri":"http://zotero.org/users/11367251/items/P2U2TAUC","itemID":429,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400564,"version":0},"$loki":211},{"itemID":604,"item":{"key":"P68A77UU","version":808,"itemType":"preprint","title":"Pix2seq: A Language Modeling Framework for Object Detection","abstractNote":"We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.","date":"2022-03-27","language":"en","shortTitle":"Pix2seq","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2109.10852","accessDate":"2023-09-05T13:45:18Z","extra":"arXiv:2109.10852 [cs]","repository":"arXiv","archiveID":"arXiv:2109.10852","creators":[{"firstName":"Ting","lastName":"Chen","creatorType":"author"},{"firstName":"Saurabh","lastName":"Saxena","creatorType":"author"},{"firstName":"Lala","lastName":"Li","creatorType":"author"},{"firstName":"David J.","lastName":"Fleet","creatorType":"author"},{"firstName":"Geoffrey","lastName":"Hinton","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:45:18Z","dateModified":"2023-09-05T13:45:19Z","uri":"http://zotero.org/users/11367251/items/P68A77UU","itemID":604,"attachments":[{"key":"ZSD455U2","version":818,"itemType":"attachment","title":"chenPix2seqLanguageModeling2022.pdf","parentItem":"P68A77UU","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenPix2seqLanguageModeling2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZSD455U2/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZSD455U2/2\">2 The Pix2Seq framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/3\">2.1 Sequence construction from object descriptions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/4\">2.2 Architecture, Objective and inference</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/4\">2.3 Sequence augmentation to integrate task priors</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZSD455U2/5\">3 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/5\">3.1 Experimental Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/6\">3.2 Main comparisons</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/7\">3.3 Ablation on sequence construction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/8\">3.4 Ablation on sequence augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/8\">3.5 Visualization of decoder's cross attention map</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/9\">4 Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/9\">5 Conclusion and future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/14\">A Quantization and dequantization of coordinates</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/14\">B Training details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/15\">C Ablation on inference (`39`42`&amp;quot;613A``45`47`&amp;quot;603Aargmax vs nucleus sampling)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/15\">D Visualization of similarity among coordinate tokens</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/15\">E The ability to direct the attention with given coordinates</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/16\">F More visualization on decoder's cross attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZSD455U2/17\">G Visualization of detection results</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:32Z","dateModified":"2023-09-05T13:47:33Z","uri":"http://zotero.org/users/11367251/items/ZSD455U2","localPath":"/Users/reyvababtista/Projects/Papers/chenPix2seqLanguageModeling2022.pdf","defaultPath":"files/623/chenPix2seqLanguageModeling2022.pdf"}],"notes":[{"key":"2R9VPI28","version":808,"itemType":"note","parentItem":"P68A77UU","note":"Comment: ICLR'22. Code and pretrained models at https://github.com/google-research/pix2seq","tags":[],"relations":{},"dateAdded":"2023-09-05T13:45:18Z","dateModified":"2023-09-05T13:45:18Z","uri":"http://zotero.org/users/11367251/items/2R9VPI28"}]},"meta":{"revision":0,"created":1709832400566,"version":0},"$loki":212},{"itemID":1828,"item":{"key":"P78SREWT","version":2288,"itemType":"conferencePaper","title":"AI and Web-Based Human-Like Interactive University Chatbot (UNIBOT)","date":"6/2019","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8822176/","accessDate":"2024-01-19T15:15:25Z","place":"Coimbatore, India","publisher":"IEEE","ISBN":"978-1-72810-167-5","pages":"148-150","proceedingsTitle":"2019 3rd International conference on Electronics, Communication and Aerospace Technology (ICECA)","conferenceName":"2019 3rd International conference on Electronics, Communication and Aerospace Technology (ICECA)","DOI":"10.1109/ICECA.2019.8822176","creators":[{"firstName":"Neelkumar P.","lastName":"Patel","creatorType":"author"},{"firstName":"Devangi R.","lastName":"Parikh","creatorType":"author"},{"firstName":"Darshan A.","lastName":"Patel","creatorType":"author"},{"firstName":"Ronak R.","lastName":"Patel","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:15:25Z","dateModified":"2024-01-19T15:15:25Z","uri":"http://zotero.org/users/11367251/items/P78SREWT","itemID":1828,"attachments":[{"key":"J86BWP9Q","version":2290,"itemType":"attachment","title":"patelAIWebBasedHumanLike2019.pdf","parentItem":"P78SREWT","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/patelAIWebBasedHumanLike2019.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:15:49Z","dateModified":"2024-01-19T15:15:49Z","uri":"http://zotero.org/users/11367251/items/J86BWP9Q","localPath":"/Users/reyvababtista/Projects/Papers/patelAIWebBasedHumanLike2019.pdf","defaultPath":"files/1830/patelAIWebBasedHumanLike2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400567,"version":0},"$loki":213},{"itemID":990,"item":{"key":"PC4Z58T3","version":1373,"itemType":"preprint","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation","abstractNote":"Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers ﬂexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a ﬁlter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.","date":"2022-02-15","language":"en","shortTitle":"BLIP","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2201.12086","accessDate":"2023-11-07T23:26:34Z","extra":"arXiv:2201.12086 [cs]","repository":"arXiv","archiveID":"arXiv:2201.12086","creators":[{"firstName":"Junnan","lastName":"Li","creatorType":"author"},{"firstName":"Dongxu","lastName":"Li","creatorType":"author"},{"firstName":"Caiming","lastName":"Xiong","creatorType":"author"},{"firstName":"Steven","lastName":"Hoi","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T23:26:34Z","dateModified":"2023-11-07T23:26:35Z","uri":"http://zotero.org/users/11367251/items/PC4Z58T3","itemID":990,"attachments":[{"key":"LN223IR4","version":1376,"itemType":"attachment","title":"liBLIPBootstrappingLanguageImage2022.pdf","parentItem":"PC4Z58T3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liBLIPBootstrappingLanguageImage2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T23:26:41Z","dateModified":"2023-11-07T23:26:41Z","uri":"http://zotero.org/users/11367251/items/LN223IR4","localPath":"/Users/reyvababtista/Projects/Papers/liBLIPBootstrappingLanguageImage2022.pdf","defaultPath":"files/991/liBLIPBootstrappingLanguageImage2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400567,"version":0},"$loki":214},{"itemID":1700,"item":{"key":"PC8HWUB7","version":2165,"itemType":"journalArticle","title":"ImageNet classification with deep convolutional neural networks","abstractNote":"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.","date":"2017-05-24","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/3065386","accessDate":"2023-12-07T15:04:11Z","volume":"60","pages":"84-90","publicationTitle":"Communications of the ACM","DOI":"10.1145/3065386","issue":"6","journalAbbreviation":"Commun. ACM","ISSN":"0001-0782, 1557-7317","creators":[{"firstName":"Alex","lastName":"Krizhevsky","creatorType":"author"},{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"},{"firstName":"Geoffrey E.","lastName":"Hinton","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-07T15:04:11Z","dateModified":"2023-12-07T15:04:11Z","uri":"http://zotero.org/users/11367251/items/PC8HWUB7","itemID":1700,"attachments":[{"key":"JA8U8NMW","version":2165,"itemType":"attachment","title":"krizhevskyImageNetclassificationdeep2017.pdf","parentItem":"PC8HWUB7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/krizhevskyImageNetclassificationdeep2017.pdf","tags":[],"relations":{},"dateAdded":"2023-12-07T15:04:13Z","dateModified":"2023-12-07T15:04:13Z","uri":"http://zotero.org/users/11367251/items/JA8U8NMW","localPath":"/Users/reyvababtista/Projects/Papers/krizhevskyImageNetclassificationdeep2017.pdf","defaultPath":"files/1702/krizhevskyImageNetclassificationdeep2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400568,"version":0},"$loki":215},{"itemID":978,"item":{"key":"PP264JII","version":1360,"itemType":"preprint","title":"PreSTU: Pre-Training for Scene-Text Understanding","abstractNote":"The ability to recognize and reason about text embedded in visual inputs is often lacking in vision-and-language (V&L) models, perhaps because V&L pre-training methods have often failed to include such an ability in their training objective. In this paper, we propose PRESTU, a novel pre-training recipe dedicated to scene-text understanding (STU). PRESTU introduces OCR-aware pre-training objectives that encourage the model to recognize text from an image and connect it to the rest of the image content. We implement PRESTU using a simple transformer-based encoder-decoder architecture, combined with large-scale image-text datasets with scene text obtained from an off-theshelf OCR system. We empirically demonstrate the effectiveness of this pre-training approach on eight visual question answering and four image captioning benchmarks.","date":"2023-08-19","language":"en","shortTitle":"PreSTU","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2209.05534","accessDate":"2023-11-07T22:40:14Z","extra":"arXiv:2209.05534 [cs]","repository":"arXiv","archiveID":"arXiv:2209.05534","creators":[{"firstName":"Jihyung","lastName":"Kil","creatorType":"author"},{"firstName":"Soravit","lastName":"Changpinyo","creatorType":"author"},{"firstName":"Xi","lastName":"Chen","creatorType":"author"},{"firstName":"Hexiang","lastName":"Hu","creatorType":"author"},{"firstName":"Sebastian","lastName":"Goodman","creatorType":"author"},{"firstName":"Wei-Lun","lastName":"Chao","creatorType":"author"},{"firstName":"Radu","lastName":"Soricut","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T22:40:14Z","dateModified":"2023-11-07T22:40:15Z","uri":"http://zotero.org/users/11367251/items/PP264JII","itemID":978,"attachments":[{"key":"MVIDVN7G","version":1360,"itemType":"attachment","title":"kilPreSTUPreTrainingSceneText2023.pdf","parentItem":"PP264JII","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kilPreSTUPreTrainingSceneText2023.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T22:40:19Z","dateModified":"2023-11-07T22:40:19Z","uri":"http://zotero.org/users/11367251/items/MVIDVN7G","localPath":"/Users/reyvababtista/Projects/Papers/kilPreSTUPreTrainingSceneText2023.pdf","defaultPath":"files/980/kilPreSTUPreTrainingSceneText2023.pdf"}],"notes":[{"key":"Z8WNUM9K","version":1360,"itemType":"note","parentItem":"PP264JII","note":"Comment: Accepted to ICCV 2023","tags":[],"relations":{},"dateAdded":"2023-11-07T22:40:14Z","dateModified":"2023-11-07T22:40:14Z","uri":"http://zotero.org/users/11367251/items/Z8WNUM9K"}]},"meta":{"revision":0,"created":1709832400568,"version":0},"$loki":216},{"itemID":1393,"item":{"key":"PPYA3JPZ","version":1528,"itemType":"preprint","title":"ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning","abstractNote":"Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.","date":"2022-03-19","shortTitle":"ChartQA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2203.10244","accessDate":"2023-11-08T22:38:56Z","extra":"arXiv:2203.10244 [cs]","repository":"arXiv","archiveID":"arXiv:2203.10244","creators":[{"firstName":"Ahmed","lastName":"Masry","creatorType":"author"},{"firstName":"Do Xuan","lastName":"Long","creatorType":"author"},{"firstName":"Jia Qing","lastName":"Tan","creatorType":"author"},{"firstName":"Shafiq","lastName":"Joty","creatorType":"author"},{"firstName":"Enamul","lastName":"Hoque","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:38:56Z","dateModified":"2023-11-08T22:38:56Z","uri":"http://zotero.org/users/11367251/items/PPYA3JPZ","itemID":1393,"attachments":[{"key":"CK5HH73L","version":1532,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2203.10244","accessDate":"2023-11-08T22:39:10Z","parentItem":"PPYA3JPZ","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2203.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:39:10Z","dateModified":"2023-11-08T22:39:10Z","uri":"http://zotero.org/users/11367251/items/CK5HH73L","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/CK5HH73L/2203.html","defaultPath":"files/1397/2203.html"},{"key":"4KHFMV7Z","version":1529,"itemType":"attachment","title":"masryChartQABenchmarkQuestion2022.pdf","parentItem":"PPYA3JPZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/masryChartQABenchmarkQuestion2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:39:01Z","dateModified":"2023-11-08T22:39:01Z","uri":"http://zotero.org/users/11367251/items/4KHFMV7Z","localPath":"/Users/reyvababtista/Projects/Papers/masryChartQABenchmarkQuestion2022.pdf","defaultPath":"files/1396/masryChartQABenchmarkQuestion2022.pdf"}],"notes":[{"key":"N3QC8LQQ","version":1528,"itemType":"note","parentItem":"PPYA3JPZ","note":"Comment: Accepted by ACL 2022 Findings","tags":[],"relations":{},"dateAdded":"2023-11-08T22:38:56Z","dateModified":"2023-11-08T22:38:56Z","uri":"http://zotero.org/users/11367251/items/N3QC8LQQ"}]},"meta":{"revision":0,"created":1709832400572,"version":0},"$loki":217},{"itemID":962,"item":{"key":"PUPLAWL8","version":1342,"itemType":"preprint","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","abstractNote":"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","date":"2021-06-03","language":"en","shortTitle":"An Image is Worth 16x16 Words","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2010.11929","accessDate":"2023-11-07T21:17:42Z","extra":"arXiv:2010.11929 [cs]","repository":"arXiv","archiveID":"arXiv:2010.11929","creators":[{"firstName":"Alexey","lastName":"Dosovitskiy","creatorType":"author"},{"firstName":"Lucas","lastName":"Beyer","creatorType":"author"},{"firstName":"Alexander","lastName":"Kolesnikov","creatorType":"author"},{"firstName":"Dirk","lastName":"Weissenborn","creatorType":"author"},{"firstName":"Xiaohua","lastName":"Zhai","creatorType":"author"},{"firstName":"Thomas","lastName":"Unterthiner","creatorType":"author"},{"firstName":"Mostafa","lastName":"Dehghani","creatorType":"author"},{"firstName":"Matthias","lastName":"Minderer","creatorType":"author"},{"firstName":"Georg","lastName":"Heigold","creatorType":"author"},{"firstName":"Sylvain","lastName":"Gelly","creatorType":"author"},{"firstName":"Jakob","lastName":"Uszkoreit","creatorType":"author"},{"firstName":"Neil","lastName":"Houlsby","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T21:17:42Z","dateModified":"2023-11-07T21:17:42Z","uri":"http://zotero.org/users/11367251/items/PUPLAWL8","itemID":962,"attachments":[{"key":"5VNQY7ZA","version":1344,"itemType":"attachment","title":"dosovitskiyImageWorth16x162021.pdf","parentItem":"PUPLAWL8","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/dosovitskiyImageWorth16x162021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5VNQY7ZA/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/3\">3 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/3\">3.1 Vision Transformer (ViT)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/4\">3.2 Fine-tuning and Higher Resolution</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/4\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/4\">4.1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/5\">4.2 Comparison to State of the Art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/6\">4.3 Pre-training Data Requirements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/8\">4.4 Scaling Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/8\">4.5 Inspecting Vision Transformer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/8\">4.6 Self-supervision</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/9\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/13\">A Multihead Self-attention</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/13\">B Experiment details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/13\">B.1 Training</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/13\">B.1.1 Fine-tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/14\">B.1.2 Self-supervision</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/14\">C Additional Results</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/15\">D Additional Analyses</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/15\">D.1 SGD vs. Adam for ResNets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/16\">D.2 Transformer shape</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/16\">D.3 Head Type and class token</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/17\">D.4 Positional Embedding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/18\">D.5 Empirical Computational Costs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/19\">D.6 Axial Attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/20\">D.7 Attention Distance</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/20\">D.8 Attention Maps</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/20\">D.9 ObjectNet Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5VNQY7ZA/20\">D.10 VTAB Breakdown</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-07T21:18:02Z","dateModified":"2023-11-07T21:18:02Z","uri":"http://zotero.org/users/11367251/items/5VNQY7ZA","localPath":"/Users/reyvababtista/Projects/Papers/dosovitskiyImageWorth16x162021.pdf","defaultPath":"files/965/dosovitskiyImageWorth16x162021.pdf"}],"notes":[{"key":"TQUHR8NC","version":1342,"itemType":"note","parentItem":"PUPLAWL8","note":"Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)","tags":[],"relations":{},"dateAdded":"2023-11-07T21:17:42Z","dateModified":"2023-11-07T21:17:42Z","uri":"http://zotero.org/users/11367251/items/TQUHR8NC"}]},"meta":{"revision":0,"created":1709832400577,"version":0},"$loki":218},{"itemID":1379,"item":{"key":"PYMAWKG7","version":1511,"itemType":"conferencePaper","title":"OCR-VQA: Visual Question Answering by Reading Text in Images","date":"9/2019","shortTitle":"OCR-VQA","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8978122/","accessDate":"2023-11-08T22:34:21Z","place":"Sydney, Australia","publisher":"IEEE","ISBN":"978-1-72813-014-9","pages":"947-952","proceedingsTitle":"2019 International Conference on Document Analysis and Recognition (ICDAR)","conferenceName":"2019 International Conference on Document Analysis and Recognition (ICDAR)","DOI":"10.1109/ICDAR.2019.00156","creators":[{"firstName":"Anand","lastName":"Mishra","creatorType":"author"},{"firstName":"Shashank","lastName":"Shekhar","creatorType":"author"},{"firstName":"Ajeet Kumar","lastName":"Singh","creatorType":"author"},{"firstName":"Anirban","lastName":"Chakraborty","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:34:21Z","dateModified":"2023-11-08T22:34:21Z","uri":"http://zotero.org/users/11367251/items/PYMAWKG7","itemID":1379,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400579,"version":0},"$loki":219},{"itemID":1438,"item":{"key":"Q69FDNQI","version":1576,"itemType":"preprint","title":"The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization","abstractNote":"We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.","date":"2021-07-24","shortTitle":"The Many Faces of Robustness","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2006.16241","accessDate":"2023-11-08T22:49:15Z","extra":"arXiv:2006.16241 [cs, stat]","repository":"arXiv","archiveID":"arXiv:2006.16241","creators":[{"firstName":"Dan","lastName":"Hendrycks","creatorType":"author"},{"firstName":"Steven","lastName":"Basart","creatorType":"author"},{"firstName":"Norman","lastName":"Mu","creatorType":"author"},{"firstName":"Saurav","lastName":"Kadavath","creatorType":"author"},{"firstName":"Frank","lastName":"Wang","creatorType":"author"},{"firstName":"Evan","lastName":"Dorundo","creatorType":"author"},{"firstName":"Rahul","lastName":"Desai","creatorType":"author"},{"firstName":"Tyler","lastName":"Zhu","creatorType":"author"},{"firstName":"Samyak","lastName":"Parajuli","creatorType":"author"},{"firstName":"Mike","lastName":"Guo","creatorType":"author"},{"firstName":"Dawn","lastName":"Song","creatorType":"author"},{"firstName":"Jacob","lastName":"Steinhardt","creatorType":"author"},{"firstName":"Justin","lastName":"Gilmer","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:49:15Z","dateModified":"2023-11-08T22:49:15Z","uri":"http://zotero.org/users/11367251/items/Q69FDNQI","itemID":1438,"attachments":[{"key":"MSVQH57I","version":1580,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/2006.16241","accessDate":"2023-11-08T22:49:27Z","parentItem":"Q69FDNQI","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"2006.html","tags":[],"relations":{},"dateAdded":"2023-11-08T22:49:27Z","dateModified":"2023-11-08T22:49:27Z","uri":"http://zotero.org/users/11367251/items/MSVQH57I","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/MSVQH57I/2006.html","defaultPath":"files/1442/2006.html"},{"key":"QAEAB6GM","version":1577,"itemType":"attachment","title":"hendrycksManyFacesRobustness2021.pdf","parentItem":"Q69FDNQI","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hendrycksManyFacesRobustness2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:49:22Z","dateModified":"2023-11-08T22:49:22Z","uri":"http://zotero.org/users/11367251/items/QAEAB6GM","localPath":"/Users/reyvababtista/Projects/Papers/hendrycksManyFacesRobustness2021.pdf","defaultPath":"files/1441/hendrycksManyFacesRobustness2021.pdf"}],"notes":[{"key":"2NVVMLVZ","version":1576,"itemType":"note","parentItem":"Q69FDNQI","note":"Comment: ICCV 2021; Datasets, code, and models available at https://github.com/hendrycks/imagenet-r","tags":[],"relations":{},"dateAdded":"2023-11-08T22:49:15Z","dateModified":"2023-11-08T22:49:15Z","uri":"http://zotero.org/users/11367251/items/2NVVMLVZ"}]},"meta":{"revision":0,"created":1709832400588,"version":0},"$loki":220},{"itemID":961,"item":{"key":"Q6TZQ9HH","version":1338,"itemType":"preprint","title":"mT5: A massively multilingual pre-trained text-to-text transformer","abstractNote":"The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \"accidental translation\" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.","date":"2021-03-11","language":"en","shortTitle":"mT5","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2010.11934","accessDate":"2023-11-07T21:17:39Z","extra":"arXiv:2010.11934 [cs]","repository":"arXiv","archiveID":"arXiv:2010.11934","creators":[{"firstName":"Linting","lastName":"Xue","creatorType":"author"},{"firstName":"Noah","lastName":"Constant","creatorType":"author"},{"firstName":"Adam","lastName":"Roberts","creatorType":"author"},{"firstName":"Mihir","lastName":"Kale","creatorType":"author"},{"firstName":"Rami","lastName":"Al-Rfou","creatorType":"author"},{"firstName":"Aditya","lastName":"Siddhant","creatorType":"author"},{"firstName":"Aditya","lastName":"Barua","creatorType":"author"},{"firstName":"Colin","lastName":"Raffel","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T21:17:39Z","dateModified":"2023-11-07T21:17:39Z","uri":"http://zotero.org/users/11367251/items/Q6TZQ9HH","itemID":961,"attachments":[{"key":"RDGFT5YQ","version":1344,"itemType":"attachment","title":"xuemT5massivelymultilingual2021.pdf","parentItem":"Q6TZQ9HH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/xuemT5massivelymultilingual2021.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T21:18:01Z","dateModified":"2023-11-07T21:18:01Z","uri":"http://zotero.org/users/11367251/items/RDGFT5YQ","localPath":"/Users/reyvababtista/Projects/Papers/xuemT5massivelymultilingual2021.pdf","defaultPath":"files/964/xuemT5massivelymultilingual2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400589,"version":0},"$loki":221},{"itemID":566,"item":{"key":"Q77M8CTV","version":757,"itemType":"conferencePaper","title":"A New Method Using LLMs for Keypoints Generation in Qualitative Data Analysis","abstractNote":"Qualitative data analysis (QDA) is useful for identifying patterns, themes, and relationships among data. In this paper, we propose a new method that uses large language models (LLMs), such as GPT-based Models, to improve QDA, in Ecological Momentary Assessment (EMA) studies as an example, by automating keypoints extraction and relevance evaluation. Experimental results on the IBM-ArgKP-2021 dataset show improved performance of the new method over existing work, achieving higher accuracy while reducing time and effort in the coding process of QDA, and demonstrate the effectiveness of our proposed method in various application settings.","date":"6/2023","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/10194998/","accessDate":"2023-08-30T13:33:06Z","place":"Santa Clara, CA, USA","publisher":"IEEE","ISBN":"9798350339840","pages":"333-334","proceedingsTitle":"2023 IEEE Conference on Artificial Intelligence (CAI)","conferenceName":"2023 IEEE Conference on Artificial Intelligence (CAI)","DOI":"10.1109/CAI54212.2023.00147","creators":[{"firstName":"Fengxiang","lastName":"Zhao","creatorType":"author"},{"firstName":"Fan","lastName":"Yu","creatorType":"author"},{"firstName":"Timothy","lastName":"Trull","creatorType":"author"},{"firstName":"Yi","lastName":"Shang","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-08-30T13:33:06Z","dateModified":"2023-08-30T13:33:06Z","uri":"http://zotero.org/users/11367251/items/Q77M8CTV","itemID":566,"attachments":[{"key":"2FWN4WDM","version":759,"itemType":"attachment","title":"zhaoNewMethodUsing2023.pdf","parentItem":"Q77M8CTV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zhaoNewMethodUsing2023.pdf","tags":[],"relations":{},"dateAdded":"2023-08-30T13:33:11Z","dateModified":"2023-08-30T13:33:11Z","uri":"http://zotero.org/users/11367251/items/2FWN4WDM","localPath":"/Users/reyvababtista/Projects/Papers/zhaoNewMethodUsing2023.pdf","defaultPath":"files/567/zhaoNewMethodUsing2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400589,"version":0},"$loki":222},{"itemID":1691,"item":{"key":"QDJYL7HN","version":2150,"itemType":"journalArticle","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents","abstractNote":"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.","date":"2022","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2204.06125","accessDate":"2023-12-06T23:06:01Z","rights":"Creative Commons Attribution 4.0 International","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.2204.06125","creators":[{"firstName":"Aditya","lastName":"Ramesh","creatorType":"author"},{"firstName":"Prafulla","lastName":"Dhariwal","creatorType":"author"},{"firstName":"Alex","lastName":"Nichol","creatorType":"author"},{"firstName":"Casey","lastName":"Chu","creatorType":"author"},{"firstName":"Mark","lastName":"Chen","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Computer Vision and Pattern Recognition (cs.CV)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:06:01Z","dateModified":"2023-12-06T23:06:01Z","uri":"http://zotero.org/users/11367251/items/QDJYL7HN","itemID":1691,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400590,"version":0},"$loki":223},{"itemID":1500,"item":{"key":"QELYR39N","version":1670,"itemType":"blogPost","title":"Properties of a Blockchain","date":"2023","url":"https://weteachblockchain.org/courses/blockchain-101/01/properties-of-a-blockchain","accessDate":"2023-11-09","blogTitle":"Properties of a Blockchain","creators":[{"name":"We Teach Blockchain","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T20:47:33Z","dateModified":"2023-11-09T20:48:30Z","uri":"http://zotero.org/users/11367251/items/QELYR39N","itemID":1500,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400592,"version":0},"$loki":224},{"itemID":1432,"item":{"key":"QJCLA4SB","version":1569,"itemType":"conferencePaper","title":"ImageNet: A large-scale hierarchical image database","date":"6/2009","shortTitle":"ImageNet","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/5206848/","accessDate":"2023-11-08T22:47:25Z","place":"Miami, FL","publisher":"IEEE","ISBN":"978-1-4244-3992-8","pages":"248-255","proceedingsTitle":"2009 IEEE Conference on Computer Vision and Pattern Recognition","conferenceName":"2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)","DOI":"10.1109/CVPR.2009.5206848","creators":[{"firstName":"Jia","lastName":"Deng","creatorType":"author"},{"firstName":"Wei","lastName":"Dong","creatorType":"author"},{"firstName":"Richard","lastName":"Socher","creatorType":"author"},{"firstName":"Li-Jia","lastName":"Li","creatorType":"author"},{"name":"Kai Li","creatorType":"author"},{"name":"Li Fei-Fei","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:47:25Z","dateModified":"2023-11-08T22:47:25Z","uri":"http://zotero.org/users/11367251/items/QJCLA4SB","itemID":1432,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400594,"version":0},"$loki":225},{"itemID":520,"item":{"key":"QUIKD9D2","version":678,"itemType":"journalArticle","title":"Artificial intelligence bot ChatGPT in medical research: the potential game changer as a double-edged sword","date":"04/2023","language":"en","shortTitle":"Artificial intelligence bot ChatGPT in medical research","libraryCatalog":"DOI.org (Crossref)","url":"https://link.springer.com/10.1007/s00167-023-07355-6","accessDate":"2023-06-22T15:10:40Z","volume":"31","pages":"1187-1189","publicationTitle":"Knee Surgery, Sports Traumatology, Arthroscopy","DOI":"10.1007/s00167-023-07355-6","issue":"4","journalAbbreviation":"Knee Surg Sports Traumatol Arthrosc","ISSN":"0942-2056, 1433-7347","creators":[{"firstName":"Jari","lastName":"Dahmen","creatorType":"author"},{"firstName":"M. Enes","lastName":"Kayaalp","creatorType":"author"},{"firstName":"Matthieu","lastName":"Ollivier","creatorType":"author"},{"firstName":"Ayoosh","lastName":"Pareek","creatorType":"author"},{"firstName":"Michael T.","lastName":"Hirschmann","creatorType":"author"},{"firstName":"Jon","lastName":"Karlsson","creatorType":"author"},{"firstName":"Philipp W.","lastName":"Winkler","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-22T15:10:40Z","dateModified":"2023-06-22T15:10:41Z","uri":"http://zotero.org/users/11367251/items/QUIKD9D2","itemID":520,"attachments":[{"key":"PAHGN6JU","version":680,"itemType":"attachment","title":"dahmenArtificialintelligencebot2023.pdf","parentItem":"QUIKD9D2","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/dahmenArtificialintelligencebot2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_PAHGN6JU/2\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_PAHGN6JU/2\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-22T15:10:46Z","dateModified":"2023-06-22T15:10:46Z","uri":"http://zotero.org/users/11367251/items/PAHGN6JU","localPath":"/Users/reyvababtista/Projects/Papers/dahmenArtificialintelligencebot2023.pdf","defaultPath":"files/521/dahmenArtificialintelligencebot2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400595,"version":0},"$loki":226},{"itemID":287,"item":{"key":"QWFDEVII","version":289,"itemType":"conferencePaper","title":"DEMONS: an integrated framework for examining associations between physiology and self-reported affect tied to depressive symptoms","abstractNote":"Depression is a prevalent and debilitating disorder among college students. Advances in mobile technology afford the opportunity to collect heterogeneous data while people are in their natural settings. The aim of the current paper is to propose an integrated framework, DEMONS (DEpression MONitoring Study), for combining passive and active data sources using a wearable sensor and a smartphone application. The ability to combine passive and active longitudinal data with mobile devices allows for better understanding of the temporal relations between self-reported affect and physiological variables (e.g., heart rate variability) linked to depressive symptoms. Adoption of the proposed framework will provide crucial information regarding the development and maintenance of depression in college students, as well as increased opportunities for early detection and intervention.","date":"2016-09-12","language":"en","shortTitle":"DEMONS","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/2968219.2968300","accessDate":"2023-03-26T21:22:44Z","place":"Heidelberg Germany","publisher":"ACM","ISBN":"978-1-4503-4462-3","pages":"1139-1143","proceedingsTitle":"Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct","conferenceName":"UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing","DOI":"10.1145/2968219.2968300","creators":[{"firstName":"Philip","lastName":"Chow","creatorType":"author"},{"firstName":"Wesley","lastName":"Bonelli","creatorType":"author"},{"firstName":"Yu","lastName":"Huang","creatorType":"author"},{"firstName":"Karl","lastName":"Fua","creatorType":"author"},{"firstName":"Bethany A.","lastName":"Teachman","creatorType":"author"},{"firstName":"Laura E.","lastName":"Barnes","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-26T21:22:44Z","dateModified":"2023-03-26T21:22:44Z","uri":"http://zotero.org/users/11367251/items/QWFDEVII","itemID":287,"attachments":[{"key":"7YLFRCNJ","version":315,"itemType":"attachment","title":"chowDEMONSintegratedframework2016.pdf","parentItem":"QWFDEVII","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chowDEMONSintegratedframework2016.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_7YLFRCNJ/2\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/3\">Active and Passive Monitoring Framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/3\">Sensus</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/3\">Wearable Sensors</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/4\">DEMONS Case Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/5\">Conclusions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/5\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_7YLFRCNJ/5\">REFERENCES </a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-26T21:29:26Z","dateModified":"2023-03-26T21:29:27Z","uri":"http://zotero.org/users/11367251/items/7YLFRCNJ","localPath":"/Users/reyvababtista/Projects/Papers/chowDEMONSintegratedframework2016.pdf","defaultPath":"files/311/chowDEMONSintegratedframework2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400596,"version":0},"$loki":227},{"itemID":529,"item":{"key":"QXYMVWJ5","version":699,"itemType":"conferencePaper","title":"Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding","abstractNote":"Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However, this process is often labor-intensive, particularly when working with large datasets. While recent AI-based tools demonstrate utility, researchers may not have readily available AI resources and expertise, let alone be challenged by the limited generalizability of those task-specific models. In this study, we explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes. Instead of training task-specific models, a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning. Using a curiosity-driven questions coding task as a case study, we found, by combining GPT-3 with expert-drafted codebooks, our proposed approach achieved fair to substantial agreements with expert-coded results. We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.","date":"2023-03-27","language":"en","shortTitle":"Supporting Qualitative Analysis with Large Language Models","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2304.10548","accessDate":"2023-06-28T17:31:27Z","extra":"arXiv:2304.10548 [cs]","pages":"75-78","proceedingsTitle":"28th International Conference on Intelligent User Interfaces","DOI":"10.1145/3581754.3584136","creators":[{"firstName":"Ziang","lastName":"Xiao","creatorType":"author"},{"firstName":"Xingdi","lastName":"Yuan","creatorType":"author"},{"firstName":"Q. Vera","lastName":"Liao","creatorType":"author"},{"firstName":"Rania","lastName":"Abdelghani","creatorType":"author"},{"firstName":"Pierre-Yves","lastName":"Oudeyer","creatorType":"author"}],"tags":[{"tag":"Computer Science - Human-Computer Interaction","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-28T17:31:27Z","dateModified":"2023-06-28T17:31:27Z","uri":"http://zotero.org/users/11367251/items/QXYMVWJ5","itemID":529,"attachments":[{"key":"B8QUIC7U","version":701,"itemType":"attachment","title":"xiaoSupportingQualitativeAnalysis2023.pdf","parentItem":"QXYMVWJ5","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/xiaoSupportingQualitativeAnalysis2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_B8QUIC7U/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/2\">2 Deductive Coding Task</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/3\">2.1 Case Study: Curiosity-driven questions Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/3\">3 GPT-3 Setup and Prompt Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/4\">4 Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/4\">5 Opportunities and Challenges</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_B8QUIC7U/5\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-28T17:31:32Z","dateModified":"2023-06-28T17:31:33Z","uri":"http://zotero.org/users/11367251/items/B8QUIC7U","localPath":"/Users/reyvababtista/Projects/Papers/xiaoSupportingQualitativeAnalysis2023.pdf","defaultPath":"files/531/xiaoSupportingQualitativeAnalysis2023.pdf"}],"notes":[{"key":"E723YDX5","version":699,"itemType":"note","parentItem":"QXYMVWJ5","note":"Comment: 28th International Conference on Intelligent User Interfaces (IUI '23 Companion), March 27--31, 2023, Sydney, NSW, Australia","tags":[],"relations":{},"dateAdded":"2023-06-28T17:31:27Z","dateModified":"2023-06-28T17:31:27Z","uri":"http://zotero.org/users/11367251/items/E723YDX5"}]},"meta":{"revision":0,"created":1709832400597,"version":0},"$loki":228},{"itemID":1819,"item":{"key":"QYNTKCDD","version":2261,"itemType":"conferencePaper","title":"Chatbot for university related FAQs","date":"9/2017","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/8126057/","accessDate":"2024-01-19T03:02:14Z","place":"Udupi","publisher":"IEEE","ISBN":"978-1-5090-6367-3","pages":"1525-1530","proceedingsTitle":"2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","conferenceName":"2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","DOI":"10.1109/ICACCI.2017.8126057","creators":[{"firstName":"Bhavika R.","lastName":"Ranoliya","creatorType":"author"},{"firstName":"Nidhi","lastName":"Raghuwanshi","creatorType":"author"},{"firstName":"Sanjay","lastName":"Singh","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T03:02:14Z","dateModified":"2024-01-19T03:02:14Z","uri":"http://zotero.org/users/11367251/items/QYNTKCDD","itemID":1819,"attachments":[{"key":"ANTX8QHR","version":2262,"itemType":"attachment","title":"ranoliyaChatbotuniversityrelated2017.pdf","parentItem":"QYNTKCDD","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/ranoliyaChatbotuniversityrelated2017.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T03:02:54Z","dateModified":"2024-01-19T03:02:54Z","uri":"http://zotero.org/users/11367251/items/ANTX8QHR","localPath":"/Users/reyvababtista/Projects/Papers/ranoliyaChatbotuniversityrelated2017.pdf","defaultPath":"files/1821/ranoliyaChatbotuniversityrelated2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400597,"version":0},"$loki":229},{"itemID":1879,"item":{"key":"QYWYABIM","version":2416,"itemType":"journalArticle","title":"A review on wearable photoplethysmography sensors and their potential future applications in health care","date":"2018","libraryCatalog":"DOI.org (Crossref)","url":"https://medcraveonline.com/IJBSBE/a-review-on-wearable-photoplethysmography-sensors-and-their-potential-future-applications-in-health-care.html","accessDate":"2024-02-06T20:23:30Z","volume":"4","publicationTitle":"International Journal of Biosensors & Bioelectronics","DOI":"10.15406/ijbsbe.2018.04.00125","issue":"4","journalAbbreviation":"IJBSBE","ISSN":"25732838","creators":[{"firstName":"Mohammad","lastName":"Ghamari","creatorType":"author"}],"tags":[],"collections":["TAUT9NML"],"relations":{},"dateAdded":"2024-02-06T20:23:30Z","dateModified":"2024-02-06T20:23:30Z","uri":"http://zotero.org/users/11367251/items/QYWYABIM","itemID":1879,"attachments":[{"key":"GPWGP2WK","version":2416,"itemType":"attachment","title":"ghamarireviewwearablephotoplethysmography2018.pdf","parentItem":"QYWYABIM","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/ghamarireviewwearablephotoplethysmography2018.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_GPWGP2WK/1\">Title</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/1\">Abstract</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/1\">Introduction</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/2\">PPG-based monitoring devices </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/2\">Wristband-type PPG-based Devices </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/2\">Forehead-type PPG-based devices </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/3\">Factors affecting PPG sensor recordings </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/3\">Ear-type PPG-based devices  </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/3\">PPG sensors </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/3\">PPG signal </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/3\">Second derivative wave of PPG signal </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Some PPG applications </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Vascular aging and PPG </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/5\">Respiration rate and PPG </a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/5\">Discussion </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/5\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/5\">Acknowledgements </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/5\">Conflict of interest  </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/5\">References </a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Figure 1</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Figure 2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Figure 3</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Table 1</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Table 2</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_GPWGP2WK/4\">Table 3 </a></li></ul>","tags":[],"relations":{},"dateAdded":"2024-02-06T20:23:31Z","dateModified":"2024-02-06T20:23:32Z","uri":"http://zotero.org/users/11367251/items/GPWGP2WK","localPath":"/Users/reyvababtista/Projects/Papers/ghamarireviewwearablephotoplethysmography2018.pdf","defaultPath":"files/1881/ghamarireviewwearablephotoplethysmography2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400602,"version":0},"$loki":230},{"itemID":941,"item":{"key":"R72SACPH","version":1284,"itemType":"blogPost","title":"Bing delivers its largest improvement in search experience using Azure GPUs","date":"11/18/2019","url":"https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/","accessDate":"2023-11-06","blogTitle":"Bing delivers its largest improvement in search experience using Azure GPUs","creators":[{"name":"Jeffrey Zhu","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T01:59:33Z","dateModified":"2023-11-07T02:00:07Z","uri":"http://zotero.org/users/11367251/items/R72SACPH","itemID":941,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400602,"version":0},"$loki":231},{"itemID":953,"item":{"key":"R8M5NX6N","version":1327,"itemType":"blogPost","title":"How Microsoft has (so far) avoided tough scrutiny over privacy issues","date":"01/10/2019","url":"https://www.fastcompany.com/90290137/how-microsoft-has-avoided-tough-scrutiny-over-privacy-issues","accessDate":"2023-11-06","blogTitle":"How Microsoft has (so far) avoided tough scrutiny over privacy issues","creators":[{"name":"Daria Solovieva","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:18:12Z","dateModified":"2023-11-07T02:18:55Z","uri":"http://zotero.org/users/11367251/items/R8M5NX6N","itemID":953,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400602,"version":0},"$loki":232},{"itemID":1611,"item":{"key":"RAC8GKA3","version":1961,"itemType":"journalArticle","title":"Scientific workflows for process mining: building blocks, scenarios, and implementation","date":"11/2016","language":"en","shortTitle":"Scientific workflows for process mining","libraryCatalog":"DOI.org (Crossref)","url":"http://link.springer.com/10.1007/s10009-015-0399-5","accessDate":"2023-11-29T04:13:00Z","volume":"18","pages":"607-628","publicationTitle":"International Journal on Software Tools for Technology Transfer","DOI":"10.1007/s10009-015-0399-5","issue":"6","journalAbbreviation":"Int J Softw Tools Technol Transfer","ISSN":"1433-2779, 1433-2787","creators":[{"firstName":"Alfredo","lastName":"Bolt","creatorType":"author"},{"firstName":"Massimiliano","lastName":"De Leoni","creatorType":"author"},{"firstName":"Wil M. P.","lastName":"Van Der Aalst","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T04:13:00Z","dateModified":"2023-11-29T04:13:00Z","uri":"http://zotero.org/users/11367251/items/RAC8GKA3","itemID":1611,"attachments":[{"key":"NRH49REP","version":1961,"itemType":"attachment","title":"boltScientificworkflowsprocess2016.pdf","parentItem":"RAC8GKA3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/boltScientificworkflowsprocess2016.pdf","tags":[],"relations":{},"dateAdded":"2023-11-29T04:13:04Z","dateModified":"2023-11-29T04:13:04Z","uri":"http://zotero.org/users/11367251/items/NRH49REP","localPath":"/Users/reyvababtista/Projects/Papers/boltScientificworkflowsprocess2016.pdf","defaultPath":"files/1613/boltScientificworkflowsprocess2016.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400603,"version":0},"$loki":233},{"itemID":1697,"item":{"key":"RAWYMPRW","version":2154,"itemType":"journalArticle","title":"A Generalist Agent","abstractNote":"Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.","date":"2022","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2205.06175","accessDate":"2023-12-06T23:10:45Z","rights":"Creative Commons Attribution 4.0 International","extra":"Publisher: arXiv\nVersion Number: 3","DOI":"10.48550/ARXIV.2205.06175","creators":[{"firstName":"Scott","lastName":"Reed","creatorType":"author"},{"firstName":"Konrad","lastName":"Zolna","creatorType":"author"},{"firstName":"Emilio","lastName":"Parisotto","creatorType":"author"},{"firstName":"Sergio Gomez","lastName":"Colmenarejo","creatorType":"author"},{"firstName":"Alexander","lastName":"Novikov","creatorType":"author"},{"firstName":"Gabriel","lastName":"Barth-Maron","creatorType":"author"},{"firstName":"Mai","lastName":"Gimenez","creatorType":"author"},{"firstName":"Yury","lastName":"Sulsky","creatorType":"author"},{"firstName":"Jackie","lastName":"Kay","creatorType":"author"},{"firstName":"Jost Tobias","lastName":"Springenberg","creatorType":"author"},{"firstName":"Tom","lastName":"Eccles","creatorType":"author"},{"firstName":"Jake","lastName":"Bruce","creatorType":"author"},{"firstName":"Ali","lastName":"Razavi","creatorType":"author"},{"firstName":"Ashley","lastName":"Edwards","creatorType":"author"},{"firstName":"Nicolas","lastName":"Heess","creatorType":"author"},{"firstName":"Yutian","lastName":"Chen","creatorType":"author"},{"firstName":"Raia","lastName":"Hadsell","creatorType":"author"},{"firstName":"Oriol","lastName":"Vinyals","creatorType":"author"},{"firstName":"Mahyar","lastName":"Bordbar","creatorType":"author"},{"firstName":"Nando","lastName":"de Freitas","creatorType":"author"}],"tags":[{"tag":"Artificial Intelligence (cs.AI)","type":1},{"tag":"Computation and Language (cs.CL)","type":1},{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1},{"tag":"Robotics (cs.RO)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-06T23:10:45Z","dateModified":"2023-12-06T23:10:45Z","uri":"http://zotero.org/users/11367251/items/RAWYMPRW","itemID":1697,"attachments":[],"notes":[{"key":"U58CEPS7","version":2154,"itemType":"note","parentItem":"RAWYMPRW","note":"<h2>Other</h2>\nPublished at TMLR, 42 pages","tags":[],"relations":{},"dateAdded":"2023-12-06T23:10:45Z","dateModified":"2023-12-06T23:10:45Z","uri":"http://zotero.org/users/11367251/items/U58CEPS7"}]},"meta":{"revision":0,"created":1709832400603,"version":0},"$loki":234},{"itemID":8,"item":{"key":"RMMIC6IV","version":199,"itemType":"journalArticle","title":"Yolo V4 for Advanced Traffic Sign Recognition With Synthetic Training Data Generated by Various GAN","abstractNote":"Convolutional Neural Networks (CNN) achieves perfection in trafﬁc sign identiﬁcation with enough annotated training data. The dataset determines the quality of the complete visual system based on CNN. Unfortunately, databases for trafﬁc signs from the majority of the world’s nations are few. In this scenario, Generative Adversarial Networks (GAN) may be employed to produce more realistic and varied training pictures to supplement the actual arrangement of images. The purpose of this research is to describe how the quality of synthetic pictures created by DCGAN, LSGAN, and WGAN is determined. Our work combines synthetic images with original images to enhance datasets and verify the effectiveness of synthetic datasets. We use different numbers and sizes of images for training. Likewise, the Structural Similarity Index (SSIM) and Mean Square Error (MSE) were employed to assess picture quality. Our study quantiﬁes the SSIM difference between the synthetic and actual images. When additional images are used for training, the synthetic image exhibits a high degree of resemblance to the genuine image. The highest SSIM value was achieved when using 200 total images as input and 32 × 32 image size. Further, we augment the original picture dataset with synthetic pictures and compare the original image model to the synthesis image model. For this experiment, we are using the latest iterations of Yolo, Yolo V3, and Yolo V4. After mixing the real image with the synthesized image produced by LSGAN, the recognition performance has been improved, achieving an accuracy of 84.9% on Yolo V3 and an accuracy of 89.33% on Yolo V4.","date":"2021","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9471877/","accessDate":"2023-03-20T14:20:07Z","volume":"9","pages":"97228-97242","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2021.3094201","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Christine","lastName":"Dewi","creatorType":"author"},{"firstName":"Rung-Ching","lastName":"Chen","creatorType":"author"},{"firstName":"Yan-Ting","lastName":"Liu","creatorType":"author"},{"firstName":"Xiaoyi","lastName":"Jiang","creatorType":"author"},{"firstName":"Kristoko Dwi","lastName":"Hartomo","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/3K2K3W4Y","http://zotero.org/users/11367251/items/GJ5BVZMA"]},"dateAdded":"2023-03-20T14:20:07Z","dateModified":"2023-03-22T17:03:36Z","uri":"http://zotero.org/users/11367251/items/RMMIC6IV","itemID":8,"attachments":[{"key":"UHQ2UYWP","version":240,"itemType":"attachment","title":"dewiYoloV4Advanced2021.pdf","parentItem":"RMMIC6IV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/dewiYoloV4Advanced2021.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:01Z","dateModified":"2023-03-22T18:52:22Z","uri":"http://zotero.org/users/11367251/items/UHQ2UYWP","localPath":"/Users/reyvababtista/Projects/Papers/dewiYoloV4Advanced2021.pdf","defaultPath":"files/244/dewiYoloV4Advanced2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400606,"version":0},"$loki":235},{"itemID":511,"item":{"key":"RPFYVKRH","version":668,"itemType":"preprint","title":"ChatGPT as your Personal Data Scientist","abstractNote":"The rise of big data has amplified the need for efficient, user-friendly automated machine learning (AutoML) tools. However, the intricacy of understanding domain-specific data and defining prediction tasks necessitates human intervention making the process time-consuming while preventing full automation. Instead, envision an intelligent agent capable of assisting users in conducting AutoML tasks through intuitive, natural conversations without requiring in-depth knowledge of the underlying machine learning (ML) processes. This agent's key challenge is to accurately comprehend the user's prediction goals and, consequently, formulate precise ML tasks, adjust data sets and model parameters accordingly, and articulate results effectively. In this paper, we take a pioneering step towards this ambitious goal by introducing a ChatGPT-based conversational data-science framework to act as a \"personal data scientist\". Precisely, we utilize Large Language Models (ChatGPT) to build a natural interface between the users and the ML models (Scikit-Learn), which in turn, allows us to approach this ambitious problem with a realistic solution. Our model pivots around four dialogue states: Data Visualization, Task Formulation, Prediction Engineering, and Result Summary and Recommendation. Each state marks a unique conversation phase, impacting the overall user-system interaction. Multiple LLM instances, serving as \"micro-agents\", ensure a cohesive conversation flow, granting us granular control over the conversation's progression. In summary, we developed an end-to-end system that not only proves the viability of the novel concept of conversational data science but also underscores the potency of LLMs in solving complex tasks. Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement.","date":"2023-05-23","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2305.13657","accessDate":"2023-06-21T22:09:55Z","extra":"arXiv:2305.13657 [cs]","repository":"arXiv","archiveID":"arXiv:2305.13657","creators":[{"firstName":"Md Mahadi","lastName":"Hassan","creatorType":"author"},{"firstName":"Alex","lastName":"Knipper","creatorType":"author"},{"firstName":"Shubhra Kanti Karmaker","lastName":"Santu","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-21T22:09:55Z","dateModified":"2023-06-21T22:09:55Z","uri":"http://zotero.org/users/11367251/items/RPFYVKRH","itemID":511,"attachments":[{"key":"RIZJGNRB","version":668,"itemType":"attachment","title":"hassanChatGPTyourPersonal2023.pdf","parentItem":"RPFYVKRH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hassanChatGPTyourPersonal2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_RIZJGNRB/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/3\">2 Related Works</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/3\">2.1 Large Language Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/4\">2.2 Dialog Systems</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/4\">2.3 AutoML Research</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/5\">3 Model Architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/5\">3.1 Global Micro-agents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/9\">3.2 Data Visualization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/10\">3.3 Task Formulation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/19\">3.4 Prediction Engineering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/19\">3.5 Result Summary and Recommendation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/20\">4 Qualitative Examples</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/20\">4.1 Overall Chat Cycle</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/21\">4.2 Interaction Between Micro-Agents</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/27\">4.3 Prompt Engineering Taxonomy</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/30\">5 Discussion</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/31\">5.1 Fail cases</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/31\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_RIZJGNRB/32\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-21T22:09:59Z","dateModified":"2023-06-21T22:09:59Z","uri":"http://zotero.org/users/11367251/items/RIZJGNRB","localPath":"/Users/reyvababtista/Projects/Papers/hassanChatGPTyourPersonal2023.pdf","defaultPath":"files/512/hassanChatGPTyourPersonal2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400607,"version":0},"$loki":236},{"itemID":918,"item":{"key":"RUJRWNYH","version":1159,"itemType":"blogPost","title":"Top 5 Important Things About Google MUM Algorithm in 2023","date":"02/21/2023","url":"https://www.serpple.com/blog/google-mum-algorithm/","accessDate":"2023-11-04","blogTitle":"Top 5 Important Things About Google MUM Algorithm in 2023","creators":[{"name":"Sesharaja","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T21:49:15Z","dateModified":"2023-11-04T21:55:50Z","uri":"http://zotero.org/users/11367251/items/RUJRWNYH","itemID":918,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400608,"version":0},"$loki":237},{"itemID":1705,"item":{"key":"RZNBBYQ3","version":2168,"itemType":"journalArticle","title":"Attention Is All You Need","abstractNote":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","date":"2017","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/1706.03762","accessDate":"2023-12-07T15:06:22Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 7","DOI":"10.48550/ARXIV.1706.03762","creators":[{"firstName":"Ashish","lastName":"Vaswani","creatorType":"author"},{"firstName":"Noam","lastName":"Shazeer","creatorType":"author"},{"firstName":"Niki","lastName":"Parmar","creatorType":"author"},{"firstName":"Jakob","lastName":"Uszkoreit","creatorType":"author"},{"firstName":"Llion","lastName":"Jones","creatorType":"author"},{"firstName":"Aidan N.","lastName":"Gomez","creatorType":"author"},{"firstName":"Lukasz","lastName":"Kaiser","creatorType":"author"},{"firstName":"Illia","lastName":"Polosukhin","creatorType":"author"}],"tags":[{"tag":"Computation and Language (cs.CL)","type":1},{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-07T15:06:22Z","dateModified":"2023-12-07T15:06:22Z","uri":"http://zotero.org/users/11367251/items/RZNBBYQ3","itemID":1705,"attachments":[],"notes":[{"key":"2D9SD3ES","version":2168,"itemType":"note","parentItem":"RZNBBYQ3","note":"<h2>Other</h2>\n15 pages, 5 figures","tags":[],"relations":{},"dateAdded":"2023-12-07T15:06:22Z","dateModified":"2023-12-07T15:06:22Z","uri":"http://zotero.org/users/11367251/items/2D9SD3ES"}]},"meta":{"revision":0,"created":1709832400608,"version":0},"$loki":238},{"itemID":1746,"item":{"key":"S28ERPPU","version":2193,"itemType":"journalArticle","title":"PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding","abstractNote":"We are now witnessing significant progress of deep learning methods in a variety of tasks (or datasets) of proteins. However, there is a lack of a standard benchmark to evaluate the performance of different methods, which hinders the progress of deep learning in this field. In this paper, we propose such a benchmark called PEER, a comprehensive and multi-task benchmark for Protein sEquence undERstanding. PEER provides a set of diverse protein understanding tasks including protein function prediction, protein localization prediction, protein structure prediction, protein-protein interaction prediction, and protein-ligand interaction prediction. We evaluate different types of sequence-based methods for each task including traditional feature engineering approaches, different sequence encoding methods as well as large-scale pre-trained protein language models. In addition, we also investigate the performance of these methods under the multi-task learning setting. Experimental results show that large-scale pre-trained protein language models achieve the best performance for most individual tasks, and jointly training multiple tasks further boosts the performance. The datasets and source codes of this benchmark are all available at https://github.com/DeepGraphLearning/PEER_Benchmark","date":"2022","shortTitle":"PEER","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2206.02096","accessDate":"2023-12-10T00:45:27Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 2","DOI":"10.48550/ARXIV.2206.02096","creators":[{"firstName":"Minghao","lastName":"Xu","creatorType":"author"},{"firstName":"Zuobai","lastName":"Zhang","creatorType":"author"},{"firstName":"Jiarui","lastName":"Lu","creatorType":"author"},{"firstName":"Zhaocheng","lastName":"Zhu","creatorType":"author"},{"firstName":"Yangtian","lastName":"Zhang","creatorType":"author"},{"firstName":"Chang","lastName":"Ma","creatorType":"author"},{"firstName":"Runcheng","lastName":"Liu","creatorType":"author"},{"firstName":"Jian","lastName":"Tang","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-10T00:45:27Z","dateModified":"2023-12-10T00:45:27Z","uri":"http://zotero.org/users/11367251/items/S28ERPPU","itemID":1746,"attachments":[{"key":"MIY6IXFP","version":2194,"itemType":"attachment","title":"xuPEERComprehensiveMultiTask2022.pdf","parentItem":"S28ERPPU","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/xuPEERComprehensiveMultiTask2022.pdf","tags":[],"relations":{},"dateAdded":"2023-12-10T00:47:51Z","dateModified":"2023-12-10T00:47:51Z","uri":"http://zotero.org/users/11367251/items/MIY6IXFP","localPath":"/Users/reyvababtista/Projects/Papers/xuPEERComprehensiveMultiTask2022.pdf","defaultPath":"files/1749/xuPEERComprehensiveMultiTask2022.pdf"}],"notes":[{"key":"LQRFJEYZ","version":2193,"itemType":"note","parentItem":"S28ERPPU","note":"<h2>Other</h2>\nAccepted by NeurIPS 2022 Dataset and Benchmark Track. arXiv v2: source code released; arXiv v1: release all benchmark results","tags":[],"relations":{},"dateAdded":"2023-12-10T00:45:27Z","dateModified":"2023-12-10T00:45:27Z","uri":"http://zotero.org/users/11367251/items/LQRFJEYZ"}]},"meta":{"revision":0,"created":1709832400609,"version":0},"$loki":239},{"itemID":1657,"item":{"key":"S2I98ENG","version":2079,"itemType":"journalArticle","title":"LightGBM: A Highly Efficient Gradient Boosting Decision Tree","abstractNote":"Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efﬁciency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a signiﬁcant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that ﬁnding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Guolin","lastName":"Ke","creatorType":"author"},{"firstName":"Qi","lastName":"Meng","creatorType":"author"},{"firstName":"Thomas","lastName":"Finley","creatorType":"author"},{"firstName":"Taifeng","lastName":"Wang","creatorType":"author"},{"firstName":"Wei","lastName":"Chen","creatorType":"author"},{"firstName":"Weidong","lastName":"Ma","creatorType":"author"},{"firstName":"Qiwei","lastName":"Ye","creatorType":"author"},{"firstName":"Tie-Yan","lastName":"Liu","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T00:33:06Z","dateModified":"2023-12-04T00:33:07Z","uri":"http://zotero.org/users/11367251/items/S2I98ENG","itemID":1657,"attachments":[{"key":"ZVA453EK","version":2079,"itemType":"attachment","title":"keLightGBMHighlyEfficient.pdf","parentItem":"S2I98ENG","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/keLightGBMHighlyEfficient.pdf","tags":[],"relations":{},"dateAdded":"2023-12-04T00:33:10Z","dateModified":"2023-12-04T00:33:10Z","uri":"http://zotero.org/users/11367251/items/ZVA453EK","localPath":"/Users/reyvababtista/Projects/Papers/keLightGBMHighlyEfficient.pdf","defaultPath":"files/1658/keLightGBMHighlyEfficient.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400610,"version":0},"$loki":240},{"itemID":353,"item":{"key":"S3BRP4HV","version":388,"itemType":"conferencePaper","title":"AndWellness: an open mobile system for activity and experience sampling","abstractNote":"Advances in mobile phone technology have allowed phones to become a convenient platform for real-time assessment of a participants health and behavior. AndWellness, a personal data collection system, uses mobile phones to collect and analyze data from both active, triggered user experience samples and passive logging of onboard environmental sensors. The system includes an application that runs on Android based mobile phones, server software that manages deployments and acts as a central repository for data, and a dashboard front end for both participants and researchers to visualize incoming data in real-time. Our system has gone through testing by researchers in preparation for deployment with participants, and we describe an initial qualitative study plus several planned future studies to demonstrate how our system can be used to better understand a user’s health related habits and observations.","date":"2010-10-05","language":"en","shortTitle":"AndWellness","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/1921081.1921087","accessDate":"2023-03-29T07:26:29Z","place":"San Diego California","publisher":"ACM","ISBN":"978-1-60558-989-3","pages":"34-43","proceedingsTitle":"Wireless Health 2010","conferenceName":"WH '10: Wireless Health 2010","DOI":"10.1145/1921081.1921087","creators":[{"firstName":"John","lastName":"Hicks","creatorType":"author"},{"firstName":"Nithya","lastName":"Ramanathan","creatorType":"author"},{"firstName":"Donnie","lastName":"Kim","creatorType":"author"},{"firstName":"Mohamad","lastName":"Monibi","creatorType":"author"},{"firstName":"Joshua","lastName":"Selsky","creatorType":"author"},{"firstName":"Mark","lastName":"Hansen","creatorType":"author"},{"firstName":"Deborah","lastName":"Estrin","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-29T07:26:29Z","dateModified":"2023-03-29T07:26:30Z","uri":"http://zotero.org/users/11367251/items/S3BRP4HV","itemID":353,"attachments":[{"key":"H53CXSCQ","version":390,"itemType":"attachment","title":"hicksAndWellnessopenmobile2010.pdf","parentItem":"S3BRP4HV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hicksAndWellnessopenmobile22.pdf","tags":[],"relations":{},"dateAdded":"2023-03-29T07:26:34Z","dateModified":"2023-03-29T07:26:35Z","uri":"http://zotero.org/users/11367251/items/H53CXSCQ","localPath":"/Users/reyvababtista/Projects/Papers/hicksAndWellnessopenmobile22.pdf","defaultPath":"files/354/hicksAndWellnessopenmobile22.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400610,"version":0},"$loki":241},{"itemID":1662,"item":{"key":"S9EFDEEH","version":2084,"itemType":"journalArticle","title":"The perceptron: A probabilistic model for information storage and organization in the brain.","date":"1958","language":"en","shortTitle":"The perceptron","libraryCatalog":"DOI.org (Crossref)","url":"http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519","accessDate":"2023-12-04T02:51:13Z","volume":"65","pages":"386-408","publicationTitle":"Psychological Review","DOI":"10.1037/h0042519","issue":"6","journalAbbreviation":"Psychological Review","ISSN":"1939-1471, 0033-295X","creators":[{"firstName":"F.","lastName":"Rosenblatt","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T02:51:13Z","dateModified":"2023-12-04T02:51:13Z","uri":"http://zotero.org/users/11367251/items/S9EFDEEH","itemID":1662,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400611,"version":0},"$loki":242},{"itemID":580,"item":{"key":"SAH8K82U","version":775,"itemType":"preprint","title":"Generative Adversarial Networks","abstractNote":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.","date":"2014-06-10","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1406.2661","accessDate":"2023-09-05T13:35:35Z","extra":"arXiv:1406.2661 [cs, stat]","repository":"arXiv","archiveID":"arXiv:1406.2661","creators":[{"firstName":"Ian J.","lastName":"Goodfellow","creatorType":"author"},{"firstName":"Jean","lastName":"Pouget-Abadie","creatorType":"author"},{"firstName":"Mehdi","lastName":"Mirza","creatorType":"author"},{"firstName":"Bing","lastName":"Xu","creatorType":"author"},{"firstName":"David","lastName":"Warde-Farley","creatorType":"author"},{"firstName":"Sherjil","lastName":"Ozair","creatorType":"author"},{"firstName":"Aaron","lastName":"Courville","creatorType":"author"},{"firstName":"Yoshua","lastName":"Bengio","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:35:35Z","dateModified":"2023-09-05T13:35:36Z","uri":"http://zotero.org/users/11367251/items/SAH8K82U","itemID":580,"attachments":[{"key":"EI547VFA","version":817,"itemType":"attachment","title":"goodfellowGenerativeAdversarialNetworks2014.pdf","parentItem":"SAH8K82U","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/goodfellowGenerativeAdversarialNetworks2014.pdf","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:29Z","dateModified":"2023-09-05T13:47:29Z","uri":"http://zotero.org/users/11367251/items/EI547VFA","localPath":"/Users/reyvababtista/Projects/Papers/goodfellowGenerativeAdversarialNetworks2014.pdf","defaultPath":"files/619/goodfellowGenerativeAdversarialNetworks2014.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400611,"version":0},"$loki":243},{"itemID":1633,"item":{"key":"SAU6IN8Z","version":2184,"itemType":"webpage","title":"Pricing","date":"2023","url":"https://openai.com/pricing","accessDate":"2023-11-29","websiteTitle":"Pricing","creators":[{"name":"OpenAI","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T07:21:30Z","dateModified":"2023-12-08T17:17:53Z","uri":"http://zotero.org/users/11367251/items/SAU6IN8Z","itemID":1633,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400611,"version":0},"$loki":244},{"itemID":1843,"item":{"key":"SUJEBGNH","version":2319,"itemType":"journalArticle","title":"Indonesian Chatbot of University Admission Using a Question Answering System Based on Sequence-to-Sequence Model","date":"2019","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S187705091931097X","accessDate":"2024-01-19T15:30:20Z","volume":"157","pages":"367-374","publicationTitle":"Procedia Computer Science","DOI":"10.1016/j.procs.2019.08.179","journalAbbreviation":"Procedia Computer Science","ISSN":"18770509","creators":[{"firstName":"Yogi Wisesa","lastName":"Chandra","creatorType":"author"},{"firstName":"Suyanto","lastName":"Suyanto","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:30:20Z","dateModified":"2024-01-19T15:30:20Z","uri":"http://zotero.org/users/11367251/items/SUJEBGNH","itemID":1843,"attachments":[{"key":"NQ3EMTS6","version":2324,"itemType":"attachment","title":"chandraIndonesianChatbotUniversity2019.pdf","parentItem":"SUJEBGNH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chandraIndonesianChatbotUniversity2019.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:36:51Z","dateModified":"2024-01-19T15:36:51Z","uri":"http://zotero.org/users/11367251/items/NQ3EMTS6","localPath":"/Users/reyvababtista/Projects/Papers/chandraIndonesianChatbotUniversity2019.pdf","defaultPath":"files/1845/chandraIndonesianChatbotUniversity2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400612,"version":0},"$loki":245},{"itemID":937,"item":{"key":"SUR2KXM7","version":1262,"itemType":"blogPost","title":"The safer way to search","date":"11/05/2023","url":"https://safety.google/search/","accessDate":"2023-11-05","blogTitle":"The safer way to search","creators":[{"name":"Google","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T05:09:48Z","dateModified":"2023-11-06T05:10:22Z","uri":"http://zotero.org/users/11367251/items/SUR2KXM7","itemID":937,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400612,"version":0},"$loki":246},{"itemID":916,"item":{"key":"SUVCJ9KP","version":1166,"itemType":"blogPost","title":"A brief history of Google algorithm changes","date":"02/27/2023","url":"https://www.techtarget.com/searchcontentmanagement/infographic/A-brief-history-of-Google-algorithm-changes","accessDate":"2023-11-04","blogTitle":"A brief history of Google algorithm changes","creators":[{"name":"Tim Murphy","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T21:43:55Z","dateModified":"2023-11-04T22:10:22Z","uri":"http://zotero.org/users/11367251/items/SUVCJ9KP","itemID":916,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400612,"version":0},"$loki":247},{"itemID":1419,"item":{"key":"SW2CQ55D","version":1556,"itemType":"conferencePaper","title":"ActivityNet: A large-scale video benchmark for human activity understanding","date":"6/2015","shortTitle":"ActivityNet","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/7298698/","accessDate":"2023-11-08T22:44:47Z","place":"Boston, MA, USA","publisher":"IEEE","ISBN":"978-1-4673-6964-0","pages":"961-970","proceedingsTitle":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","conferenceName":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2015.7298698","creators":[{"firstName":"Fabian Caba","lastName":"Heilbron","creatorType":"author"},{"firstName":"Victor","lastName":"Escorcia","creatorType":"author"},{"firstName":"Bernard","lastName":"Ghanem","creatorType":"author"},{"firstName":"Juan Carlos","lastName":"Niebles","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T22:44:47Z","dateModified":"2023-11-08T22:44:47Z","uri":"http://zotero.org/users/11367251/items/SW2CQ55D","itemID":1419,"attachments":[{"key":"53NM9C33","version":1557,"itemType":"attachment","title":"heilbronActivityNetlargescalevideo2015.pdf","parentItem":"SW2CQ55D","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/heilbronActivityNetlargescalevideo2015.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T22:44:52Z","dateModified":"2023-11-08T22:44:52Z","uri":"http://zotero.org/users/11367251/items/53NM9C33","localPath":"/Users/reyvababtista/Projects/Papers/heilbronActivityNetlargescalevideo2015.pdf","defaultPath":"files/1421/heilbronActivityNetlargescalevideo2015.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400613,"version":0},"$loki":248},{"itemID":950,"item":{"key":"SXBJD8XZ","version":1308,"itemType":"blogPost","title":"Privacy Policy","date":"05/11/2023","url":"https://duckduckgo.com/privacy","accessDate":"2023-11-06","blogTitle":"Privacy Policy","creators":[{"name":"DuckDuckGo","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-07T02:08:50Z","dateModified":"2023-11-07T02:09:33Z","uri":"http://zotero.org/users/11367251/items/SXBJD8XZ","itemID":950,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400613,"version":0},"$loki":249},{"itemID":1496,"item":{"key":"T82SAXR7","version":1653,"itemType":"conferencePaper","title":"nocaps: novel object captioning at scale","abstractNote":"Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed 'nocaps', for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets. The associated training data consists of COCO image-caption pairs, plus OpenImages image-level labels and object bounding boxes. Since OpenImages contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work on this task.","date":"10/2019","shortTitle":"nocaps","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1812.08658","accessDate":"2023-11-08T23:06:32Z","extra":"arXiv:1812.08658 [cs]","pages":"8947-8956","proceedingsTitle":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00904","creators":[{"firstName":"Harsh","lastName":"Agrawal","creatorType":"author"},{"firstName":"Karan","lastName":"Desai","creatorType":"author"},{"firstName":"Yufei","lastName":"Wang","creatorType":"author"},{"firstName":"Xinlei","lastName":"Chen","creatorType":"author"},{"firstName":"Rishabh","lastName":"Jain","creatorType":"author"},{"firstName":"Mark","lastName":"Johnson","creatorType":"author"},{"firstName":"Dhruv","lastName":"Batra","creatorType":"author"},{"firstName":"Devi","lastName":"Parikh","creatorType":"author"},{"firstName":"Stefan","lastName":"Lee","creatorType":"author"},{"firstName":"Peter","lastName":"Anderson","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:06:32Z","dateModified":"2023-11-08T23:06:32Z","uri":"http://zotero.org/users/11367251/items/T82SAXR7","itemID":1496,"attachments":[{"key":"ACVH9TNA","version":1654,"itemType":"attachment","title":"agrawalnocapsnovelobject2019.pdf","parentItem":"T82SAXR7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/agrawalnocapsnovelobject2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:06:38Z","dateModified":"2023-11-08T23:06:38Z","uri":"http://zotero.org/users/11367251/items/ACVH9TNA","localPath":"/Users/reyvababtista/Projects/Papers/agrawalnocapsnovelobject2019.pdf","defaultPath":"files/1498/agrawalnocapsnovelobject2019.pdf"},{"key":"NRZL7GWV","version":1662,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1812.08658","accessDate":"2023-11-08T23:06:43Z","parentItem":"T82SAXR7","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1812.html","tags":[],"relations":{},"dateAdded":"2023-11-08T23:06:43Z","dateModified":"2023-11-08T23:06:43Z","uri":"http://zotero.org/users/11367251/items/NRZL7GWV","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/NRZL7GWV/1812.html","defaultPath":"files/1499/1812.html"}],"notes":[]},"meta":{"revision":0,"created":1709832400614,"version":0},"$loki":250},{"itemID":1813,"item":{"key":"TD88QFM3","version":2252,"itemType":"attachment","title":"2005.11401.pdf","linkMode":"imported_file","contentType":"application/pdf","charset":"","filename":"2005.11401.pdf","tags":[],"collections":[],"relations":{},"dateAdded":"2024-01-19T02:53:09Z","dateModified":"2024-01-19T02:53:09Z","uri":"http://zotero.org/users/11367251/items/TD88QFM3","itemID":1813,"localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/TD88QFM3/2005.11401.pdf","defaultPath":"files/1813/2005.11401.pdf"},"meta":{"revision":0,"created":1709832400614,"version":0},"$loki":251},{"itemID":517,"item":{"key":"TGLIKV84","version":676,"itemType":"conferencePaper","title":"The Impact of ChatGPT on Streaming Media: A Crowdsourced and Data-Driven Analysis using Twitter and Reddit","abstractNote":"ChatGPT, a general-purpose text generation AI model, is reshaping various domains ranging from education and software development to legal defense and novel writing. Despite its potential impact, there is a lack of research on how ChatGPT might inﬂuence streaming media, which is an essential part of everyday entertainment. As a result, it remains unclear how ChatGPT is changing the future of streaming media. To bridge such a research gap, in this paper, we propose a crowdsourced, data-driven framework that leverages two social media platforms, Twitter and Reddit, to explore the impact of ChatGPT on streaming media. Through extensive analysis of social media data collected from Twitter and Reddit, we reveal how ChatGPT is transforming streaming media from diverse perspectives. Our data analytics demonstrates that ChatGPT is sparking both fear and excitement in the context of the streaming media and enhancing the downstream visual generative models, such as DALLE-2 and Stable Diffusion Videos. To the best of our knowledge, this study is the ﬁrst large-scale and systematical investigation into the effects of ChatGPT on streaming media. Hope our ﬁndings will inspire further research and discussions on this topic across academia and industry.","date":"5/2023","language":"en","shortTitle":"The Impact of ChatGPT on Streaming Media","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/10132120/","accessDate":"2023-06-22T15:10:21Z","place":"New York, NY, USA","publisher":"IEEE","ISBN":"9798350312935","pages":"222-227","proceedingsTitle":"2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)","conferenceName":"2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)","DOI":"10.1109/BigDataSecurity-HPSC-IDS58521.2023.00046","creators":[{"firstName":"Yunhe","lastName":"Feng","creatorType":"author"},{"firstName":"Pradhyumna","lastName":"Poralla","creatorType":"author"},{"firstName":"Swagatika","lastName":"Dash","creatorType":"author"},{"firstName":"Kaicheng","lastName":"Li","creatorType":"author"},{"firstName":"Vrushabh","lastName":"Desai","creatorType":"author"},{"firstName":"Meikang","lastName":"Qiu","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-22T15:10:21Z","dateModified":"2023-06-22T15:10:21Z","uri":"http://zotero.org/users/11367251/items/TGLIKV84","itemID":517,"attachments":[{"key":"X5XATQUB","version":676,"itemType":"attachment","title":"fengImpactChatGPTStreaming2023.pdf","parentItem":"TGLIKV84","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/fengImpactChatGPTStreaming2023.pdf","tags":[],"relations":{},"dateAdded":"2023-06-22T15:10:25Z","dateModified":"2023-06-22T15:10:25Z","uri":"http://zotero.org/users/11367251/items/X5XATQUB","localPath":"/Users/reyvababtista/Projects/Papers/fengImpactChatGPTStreaming2023.pdf","defaultPath":"files/518/fengImpactChatGPTStreaming2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400615,"version":0},"$loki":252},{"itemID":945,"item":{"key":"THM5NTBW","version":1299,"itemType":"book","title":"Inventing the cloud century: how cloudiness keeps changing our life, economy and technology","date":"2018","language":"eng","shortTitle":"Inventing the cloud century","libraryCatalog":"K10plus ISBN","place":"Cham","publisher":"Springer International Publishing","ISBN":"978-3-319-61161-7 978-3-319-61160-0","numPages":"609","creators":[{"firstName":"Marcus","lastName":"Oppitz","creatorType":"author"},{"firstName":"Peter","lastName":"Tomsu","creatorType":"author"}],"tags":[],"collections":[],"relations":{},"dateAdded":"2023-11-07T02:06:14Z","dateModified":"2023-11-07T02:06:14Z","uri":"http://zotero.org/users/11367251/items/THM5NTBW","itemID":945,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400615,"version":0},"$loki":253},{"itemID":750,"item":{"key":"TSHNTWJQ","version":1004,"itemType":"journalArticle","title":"Improving Language Understanding by Generative Pre-Training","abstractNote":"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Alec","lastName":"Radford","creatorType":"author"},{"firstName":"Karthik","lastName":"Narasimhan","creatorType":"author"},{"firstName":"Tim","lastName":"Salimans","creatorType":"author"},{"firstName":"Ilya","lastName":"Sutskever","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-10-04T15:37:44Z","dateModified":"2023-10-04T15:37:44Z","uri":"http://zotero.org/users/11367251/items/TSHNTWJQ","itemID":750,"attachments":[{"key":"H2IGIT49","version":1004,"itemType":"attachment","title":"radfordImprovingLanguageUnderstanding.pdf","parentItem":"TSHNTWJQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/radfordImprovingLanguageUnderstanding.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_H2IGIT49/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/2\">Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2IGIT49/3\">Framework</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/3\">Unsupervised pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/3\">Supervised fine-tuning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/4\">Task-specific input transformations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_H2IGIT49/4\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/4\">Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/5\">Supervised fine-tuning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/7\">Analysis</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_H2IGIT49/8\">Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-10-04T15:37:48Z","dateModified":"2023-10-04T15:37:48Z","uri":"http://zotero.org/users/11367251/items/H2IGIT49","localPath":"/Users/reyvababtista/Projects/Papers/radfordImprovingLanguageUnderstanding.pdf","defaultPath":"files/751/radfordImprovingLanguageUnderstanding.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400616,"version":0},"$loki":254},{"itemID":431,"item":{"key":"TW4UTB4X","version":532,"itemType":"webpage","title":"Handwriting Recognition","date":"2023-05-04","url":"https://paperswithcode.com/task/handwriting-recognition","accessDate":"2023-05-04","websiteTitle":"Handwriting Recognition","creators":[{"name":"Papers with Code","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T03:52:01Z","dateModified":"2023-05-05T03:52:20Z","uri":"http://zotero.org/users/11367251/items/TW4UTB4X","itemID":431,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400618,"version":0},"$loki":255},{"itemID":803,"item":{"key":"U5PPXUAZ","version":1064,"itemType":"preprint","title":"X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks","abstractNote":"Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multigrained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X2-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X2-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X2-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X2-VLM results in high transferability for it to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X2-VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training. The code and pre-trained models are available at github.com/zengyan-97/X2-VLM.","date":"2023-07-30","language":"en","shortTitle":"X$^2$-VLM","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2211.12402","accessDate":"2023-10-28T15:31:09Z","extra":"arXiv:2211.12402 [cs]","repository":"arXiv","archiveID":"arXiv:2211.12402","creators":[{"firstName":"Yan","lastName":"Zeng","creatorType":"author"},{"firstName":"Xinsong","lastName":"Zhang","creatorType":"author"},{"firstName":"Hang","lastName":"Li","creatorType":"author"},{"firstName":"Jiawei","lastName":"Wang","creatorType":"author"},{"firstName":"Jipeng","lastName":"Zhang","creatorType":"author"},{"firstName":"Wangchunshu","lastName":"Zhou","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-28T15:31:09Z","dateModified":"2023-10-28T15:31:09Z","uri":"http://zotero.org/users/11367251/items/U5PPXUAZ","itemID":803,"attachments":[{"key":"EGIMHGHA","version":1066,"itemType":"attachment","title":"zengVLMAllInOnePretrained2023.pdf","parentItem":"U5PPXUAZ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/zengVLMAllInOnePretrained2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_EGIMHGHA/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/3\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/3\">Image-Text Pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/4\">Video-Text Pre-training</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/4\">Multilingual Multi-modal Pre-training</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/4\">Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/4\">Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/4\">Unified Vision Encoding</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/5\">Multi-Grained Vision Language Pre-training</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/5\">Multi-Grained Aligning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/7\">Multi-Grained Localization</a></li></ul></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/7\">Experiment</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/7\">Pre-training Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/7\">Implementation Details</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/8\">Image-Text Downstream Tasks</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/8\">Image-Text Retrieval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/9\">Visual Question Answering</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/10\">Visual Reasoning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/10\">Visual Grounding</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/11\">Image Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/11\">Winoground</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/12\">Open-vocabulary Attribute Detection</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/12\">Video-Text Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/12\">Multilingual Multi-modal Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/13\">Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/14\">Qualitative Study of Multi-Grained Alignments</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/15\">Conclusion and Discussion</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/22\">Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/22\">Pre-training Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/22\">Implementation Details</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/22\">Ablation Study</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_EGIMHGHA/23\">Qualitative Study of Multi-Grained Alignments</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-10-28T15:31:20Z","dateModified":"2023-10-28T15:31:20Z","uri":"http://zotero.org/users/11367251/items/EGIMHGHA","localPath":"/Users/reyvababtista/Projects/Papers/zengVLMAllInOnePretrained2023.pdf","defaultPath":"files/805/zengVLMAllInOnePretrained2023.pdf"}],"notes":[{"key":"IFT3W2DL","version":1064,"itemType":"note","parentItem":"U5PPXUAZ","note":"Comment: 25 pages","tags":[],"relations":{},"dateAdded":"2023-10-28T15:31:09Z","dateModified":"2023-10-28T15:31:09Z","uri":"http://zotero.org/users/11367251/items/IFT3W2DL"}]},"meta":{"revision":0,"created":1709832400619,"version":0},"$loki":256},{"itemID":267,"item":{"key":"U63Y45LB","version":258,"itemType":"journalArticle","title":"Beiwe: A data collection platform for high-throughput digital phenotyping","abstractNote":"Beiwe is a high-throughput data collection platform for smartphone-based digital phenotyping. It has been in development and use since 2013. Beiwe consists of two native front-end applications: one for Android (written in Java and Kotlin) and one for iOS (written in Swift and Objective-C). The Beiwe back-end, which is based on Amazon Web Services (AWS), has been implemented primarily in Python 3.6, but it also makes use of Django for ORM and Flask for API and web servers. It uses several AWS services, such as S3 for flat file storage, EC2 virtual servers for data processing, Elastic Beanstalk for orchestration, and RDS for PostgreSQL database engine. Most smartphone applications use software development kits (SDKs) that generate unvalidated behavioral summary measures using closed proprietary algorithms. These applications do not meet the high standards of reproducible science, and often require researchers to modify their scientific questions based on what data happens to be available. In contrast, Beiwe collects raw sensor and phone use data, and its data collection parameters can be customized to address specific scientific questions of interest. Collection of raw data also improves reproducibility of studies and enables re-analyses of data and pooling of data across studies. Every aspect of Beiwe data collection is fully customizable, including which sensors to sample, how frequently to sample them, whether to add Gaussian noise to GPS location, whether to use Wi-Fi or cellular data for uploads, how frequently to upload data, specification of surveys and their response options, and skip logic. All study settings are captured in a JSON-formatted configuration file, which can be exported from and imported to Beiwe to enhance transparency and reproducibility of studies.","date":"2021-12-15","language":"en","shortTitle":"Beiwe","libraryCatalog":"DOI.org (Crossref)","url":"https://joss.theoj.org/papers/10.21105/joss.03417","accessDate":"2023-03-23T22:21:03Z","volume":"6","pages":"3417","publicationTitle":"Journal of Open Source Software","DOI":"10.21105/joss.03417","issue":"68","journalAbbreviation":"JOSS","ISSN":"2475-9066","creators":[{"firstName":"Jukka-Pekka","lastName":"Onnela","creatorType":"author"},{"firstName":"Caleb","lastName":"Dixon","creatorType":"author"},{"firstName":"Keary","lastName":"Griffin","creatorType":"author"},{"firstName":"Tucker","lastName":"Jaenicke","creatorType":"author"},{"firstName":"Leila","lastName":"Minowada","creatorType":"author"},{"firstName":"Sean","lastName":"Esterkin","creatorType":"author"},{"firstName":"Alvin","lastName":"Siu","creatorType":"author"},{"firstName":"Josh","lastName":"Zagorsky","creatorType":"author"},{"firstName":"Eli","lastName":"Jones","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-23T22:21:03Z","dateModified":"2023-03-23T22:21:03Z","uri":"http://zotero.org/users/11367251/items/U63Y45LB","itemID":267,"attachments":[{"key":"CIN7I8RQ","version":261,"itemType":"attachment","title":"onnelaBeiwedatacollection2021.pdf","parentItem":"U63Y45LB","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/onnelaBeiwedatacollection2021.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_CIN7I8RQ/1\">Summary</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CIN7I8RQ/1\">Statement of need</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CIN7I8RQ/4\">Acknowledgements</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_CIN7I8RQ/4\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-23T22:21:09Z","dateModified":"2023-03-23T22:21:10Z","uri":"http://zotero.org/users/11367251/items/CIN7I8RQ","localPath":"/Users/reyvababtista/Projects/Papers/onnelaBeiwedatacollection2021.pdf","defaultPath":"files/268/onnelaBeiwedatacollection2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400620,"version":0},"$loki":257},{"itemID":1678,"item":{"key":"UECZDZ2S","version":2121,"itemType":"blogPost","title":"Neural network","date":"2023","url":"https://en.wikipedia.org/w/index.php?title=Neural_network&oldid=1182490931","accessDate":"2023-12-05","blogTitle":"Neural network","creators":[{"name":"Wikipedia contributors","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T21:03:29Z","dateModified":"2023-12-05T21:04:26Z","uri":"http://zotero.org/users/11367251/items/UECZDZ2S","itemID":1678,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400620,"version":0},"$loki":258},{"itemID":1492,"item":{"key":"UED6QEHK","version":1646,"itemType":"preprint","title":"Moments in Time Dataset: one million videos for event understanding","abstractNote":"We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical in time (\"opening\" is \"closing\" in reverse), and either transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.","date":"2019-02-16","shortTitle":"Moments in Time Dataset","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1801.03150","accessDate":"2023-11-08T23:05:46Z","extra":"arXiv:1801.03150 [cs]","repository":"arXiv","archiveID":"arXiv:1801.03150","creators":[{"firstName":"Mathew","lastName":"Monfort","creatorType":"author"},{"firstName":"Alex","lastName":"Andonian","creatorType":"author"},{"firstName":"Bolei","lastName":"Zhou","creatorType":"author"},{"firstName":"Kandan","lastName":"Ramakrishnan","creatorType":"author"},{"firstName":"Sarah Adel","lastName":"Bargal","creatorType":"author"},{"firstName":"Tom","lastName":"Yan","creatorType":"author"},{"firstName":"Lisa","lastName":"Brown","creatorType":"author"},{"firstName":"Quanfu","lastName":"Fan","creatorType":"author"},{"firstName":"Dan","lastName":"Gutfruend","creatorType":"author"},{"firstName":"Carl","lastName":"Vondrick","creatorType":"author"},{"firstName":"Aude","lastName":"Oliva","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:05:46Z","dateModified":"2023-11-08T23:05:46Z","uri":"http://zotero.org/users/11367251/items/UED6QEHK","itemID":1492,"attachments":[{"key":"S3C4HDJI","version":1650,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1801.03150","accessDate":"2023-11-08T23:05:57Z","parentItem":"UED6QEHK","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1801.html","tags":[],"relations":{},"dateAdded":"2023-11-08T23:05:57Z","dateModified":"2023-11-08T23:05:57Z","uri":"http://zotero.org/users/11367251/items/S3C4HDJI","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/S3C4HDJI/1801.html","defaultPath":"files/1495/1801.html"},{"key":"WX8CLKTW","version":1647,"itemType":"attachment","title":"monfortMomentsTimeDataset2019.pdf","parentItem":"UED6QEHK","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/monfortMomentsTimeDataset2019.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:05:52Z","dateModified":"2023-11-08T23:05:52Z","uri":"http://zotero.org/users/11367251/items/WX8CLKTW","localPath":"/Users/reyvababtista/Projects/Papers/monfortMomentsTimeDataset2019.pdf","defaultPath":"files/1494/monfortMomentsTimeDataset2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400621,"version":0},"$loki":259},{"itemID":1487,"item":{"key":"UIESKWXJ","version":1639,"itemType":"preprint","title":"A Short Note on the Kinetics-700 Human Action Dataset","abstractNote":"We describe an extension of the DeepMind Kinetics human action dataset from 600 classes to 700 classes, where for each class there are at least 600 video clips from different YouTube videos. This paper details the changes introduced for this new release of the dataset, and includes a comprehensive set of statistics as well as baseline results using the I3D neural network architecture.","date":"2022-10-17","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1907.06987","accessDate":"2023-11-08T23:04:47Z","extra":"arXiv:1907.06987 [cs]","repository":"arXiv","archiveID":"arXiv:1907.06987","creators":[{"firstName":"Joao","lastName":"Carreira","creatorType":"author"},{"firstName":"Eric","lastName":"Noland","creatorType":"author"},{"firstName":"Chloe","lastName":"Hillier","creatorType":"author"},{"firstName":"Andrew","lastName":"Zisserman","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:04:47Z","dateModified":"2023-11-08T23:04:47Z","uri":"http://zotero.org/users/11367251/items/UIESKWXJ","itemID":1487,"attachments":[{"key":"TCVAWNM7","version":1642,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1907.06987","accessDate":"2023-11-08T23:04:55Z","parentItem":"UIESKWXJ","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1907.html","tags":[],"relations":{},"dateAdded":"2023-11-08T23:04:55Z","dateModified":"2023-11-08T23:04:55Z","uri":"http://zotero.org/users/11367251/items/TCVAWNM7","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/TCVAWNM7/1907.html","defaultPath":"files/1491/1907.html"},{"key":"97YLH8T5","version":1639,"itemType":"attachment","title":"carreiraShortNoteKinetics7002022.pdf","parentItem":"UIESKWXJ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/carreiraShortNoteKinetics7002022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:04:49Z","dateModified":"2023-11-08T23:04:49Z","uri":"http://zotero.org/users/11367251/items/97YLH8T5","localPath":"/Users/reyvababtista/Projects/Papers/carreiraShortNoteKinetics7002022.pdf","defaultPath":"files/1490/carreiraShortNoteKinetics7002022.pdf"}],"notes":[{"key":"TCG38QCH","version":1639,"itemType":"note","parentItem":"UIESKWXJ","note":"Comment: added note about dangers of training on k700 and evaluating on k400/k600. arXiv admin note: text overlap with arXiv:1808.01340","tags":[],"relations":{},"dateAdded":"2023-11-08T23:04:47Z","dateModified":"2023-11-08T23:04:47Z","uri":"http://zotero.org/users/11367251/items/TCG38QCH"}]},"meta":{"revision":0,"created":1709832400621,"version":0},"$loki":260},{"itemID":1840,"item":{"key":"UIXJDRQS","version":2309,"itemType":"bookSection","title":"Jooka: A Bilingual Chatbot for University Admission","date":"2021","language":"en","shortTitle":"Jooka","libraryCatalog":"DOI.org (Crossref)","url":"http://link.springer.com/10.1007/978-3-030-72660-7_64","accessDate":"2024-01-19T15:26:33Z","extra":"Series Title: Advances in Intelligent Systems and Computing\nDOI: 10.1007/978-3-030-72660-7_64","volume":"1367","place":"Cham","publisher":"Springer International Publishing","ISBN":"978-3-030-72659-1 978-3-030-72660-7","pages":"671-681","bookTitle":"Trends and Applications in Information Systems and Technologies","creators":[{"firstName":"Álvaro","lastName":"Rocha","creatorType":"editor"},{"firstName":"Hojjat","lastName":"Adeli","creatorType":"editor"},{"firstName":"Gintautas","lastName":"Dzemyda","creatorType":"editor"},{"firstName":"Fernando","lastName":"Moreira","creatorType":"editor"},{"firstName":"Ana Maria","lastName":"Ramalho Correia","creatorType":"editor"},{"firstName":"Walid","lastName":"El Hefny","creatorType":"author"},{"firstName":"Yasmin","lastName":"Mansy","creatorType":"author"},{"firstName":"Mina","lastName":"Abdallah","creatorType":"author"},{"firstName":"Slim","lastName":"Abdennadher","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T15:26:33Z","dateModified":"2024-01-19T15:26:33Z","uri":"http://zotero.org/users/11367251/items/UIXJDRQS","itemID":1840,"attachments":[{"key":"8B4KNT92","version":2310,"itemType":"attachment","title":"elhefnyJookaBilingualChatbot2021.pdf","parentItem":"UIXJDRQS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/elhefnyJookaBilingualChatbot2021.pdf","tags":[],"relations":{},"dateAdded":"2024-01-19T15:27:02Z","dateModified":"2024-01-19T15:27:02Z","uri":"http://zotero.org/users/11367251/items/8B4KNT92","localPath":"/Users/reyvababtista/Projects/Papers/elhefnyJookaBilingualChatbot2021.pdf","defaultPath":"files/1842/elhefnyJookaBilingualChatbot2021.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400622,"version":0},"$loki":261},{"itemID":993,"item":{"key":"V29ZNPUC","version":1380,"itemType":"journalArticle","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization","abstractNote":"We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.","date":"02/2020","language":"en","shortTitle":"Grad-CAM","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1610.02391","accessDate":"2023-11-07T23:28:55Z","extra":"arXiv:1610.02391 [cs]","volume":"128","pages":"336-359","publicationTitle":"International Journal of Computer Vision","DOI":"10.1007/s11263-019-01228-7","issue":"2","journalAbbreviation":"Int J Comput Vis","ISSN":"0920-5691, 1573-1405","creators":[{"firstName":"Ramprasaath R.","lastName":"Selvaraju","creatorType":"author"},{"firstName":"Michael","lastName":"Cogswell","creatorType":"author"},{"firstName":"Abhishek","lastName":"Das","creatorType":"author"},{"firstName":"Ramakrishna","lastName":"Vedantam","creatorType":"author"},{"firstName":"Devi","lastName":"Parikh","creatorType":"author"},{"firstName":"Dhruv","lastName":"Batra","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T23:28:55Z","dateModified":"2023-11-07T23:28:56Z","uri":"http://zotero.org/users/11367251/items/V29ZNPUC","itemID":993,"attachments":[{"key":"I62S5H3R","version":1381,"itemType":"attachment","title":"selvarajuGradCAMVisualExplanations2020.pdf","parentItem":"V29ZNPUC","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/selvarajuGradCAMVisualExplanations2020.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T23:28:58Z","dateModified":"2023-11-07T23:28:58Z","uri":"http://zotero.org/users/11367251/items/I62S5H3R","localPath":"/Users/reyvababtista/Projects/Papers/selvarajuGradCAMVisualExplanations2020.pdf","defaultPath":"files/995/selvarajuGradCAMVisualExplanations2020.pdf"}],"notes":[{"key":"97I7TG85","version":1380,"itemType":"note","parentItem":"V29ZNPUC","note":"Comment: This version was published in International Journal of Computer Vision (IJCV) in 2019; A previous version of the paper was published at International Conference on Computer Vision (ICCV'17)","tags":[],"relations":{},"dateAdded":"2023-11-07T23:28:55Z","dateModified":"2023-11-07T23:28:55Z","uri":"http://zotero.org/users/11367251/items/97I7TG85"}]},"meta":{"revision":0,"created":1709832400622,"version":0},"$loki":262},{"itemID":1123,"item":{"key":"V9B69TVE","version":1430,"itemType":"preprint","title":"Microsoft COCO Captions: Data Collection and Evaluation Server","abstractNote":"In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, ﬁve independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.","date":"2015-04-03","language":"en","shortTitle":"Microsoft COCO Captions","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1504.00325","accessDate":"2023-11-08T15:35:48Z","extra":"arXiv:1504.00325 [cs]","repository":"arXiv","archiveID":"arXiv:1504.00325","creators":[{"firstName":"Xinlei","lastName":"Chen","creatorType":"author"},{"firstName":"Hao","lastName":"Fang","creatorType":"author"},{"firstName":"Tsung-Yi","lastName":"Lin","creatorType":"author"},{"firstName":"Ramakrishna","lastName":"Vedantam","creatorType":"author"},{"firstName":"Saurabh","lastName":"Gupta","creatorType":"author"},{"firstName":"Piotr","lastName":"Dollar","creatorType":"author"},{"firstName":"C. Lawrence","lastName":"Zitnick","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T15:35:48Z","dateModified":"2023-11-08T15:35:48Z","uri":"http://zotero.org/users/11367251/items/V9B69TVE","itemID":1123,"attachments":[{"key":"C93EE2W7","version":1432,"itemType":"attachment","title":"chenMicrosoftCOCOCaptions2015.pdf","parentItem":"V9B69TVE","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chenMicrosoftCOCOCaptions2015.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T15:35:53Z","dateModified":"2023-11-08T15:35:53Z","uri":"http://zotero.org/users/11367251/items/C93EE2W7","localPath":"/Users/reyvababtista/Projects/Papers/chenMicrosoftCOCOCaptions2015.pdf","defaultPath":"files/1125/chenMicrosoftCOCOCaptions2015.pdf"}],"notes":[{"key":"R5HQXWUX","version":1430,"itemType":"note","parentItem":"V9B69TVE","note":"Comment: arXiv admin note: text overlap with arXiv:1411.4952","tags":[],"relations":{},"dateAdded":"2023-11-08T15:35:48Z","dateModified":"2023-11-08T15:35:48Z","uri":"http://zotero.org/users/11367251/items/R5HQXWUX"}]},"meta":{"revision":0,"created":1709832400623,"version":0},"$loki":263},{"itemID":1625,"item":{"key":"V9EGX53P","version":1982,"itemType":"journalArticle","title":"Visualizing Data using t-SNE","abstractNote":"We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each\ndatapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic\nNeighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces\nsignificantly better visualizations by reducing the tendency to crowd points together in the center\nof the map. t-SNE is better than existing techniques at creating a single map that reveals structure\nat many different scales. This is particularly important for high-dimensional data that lie on several\ndifferent, but related, low-dimensional manifolds, such as images of objects from multiple classes\nseen from multiple viewpoints. For visualizing the structure of very large data sets, we show how\nt-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the\ndata to influence the way in which a subset of the data is displayed. We illustrate the performance of\nt-SNE on a wide variety of data sets and compare it with many other non-parametric visualization\ntechniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-\ntions produced by t-SNE are significantly better than those produced by the other techniques on\nalmost all of the data sets.","creators":[{"name":"Laurens van der Maaten","creatorType":"author"}],"tags":[],"collections":[],"relations":{},"dateAdded":"2023-11-29T05:06:02Z","dateModified":"2023-11-29T05:06:23Z","uri":"http://zotero.org/users/11367251/items/V9EGX53P","itemID":1625,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400623,"version":0},"$loki":264},{"itemID":714,"item":{"key":"VBFHYRC9","version":949,"itemType":"preprint","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","abstractNote":"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence 2.","date":"2023-05-25","language":"en","shortTitle":"HuggingGPT","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2303.17580","accessDate":"2023-09-21T21:51:01Z","extra":"arXiv:2303.17580 [cs]","repository":"arXiv","archiveID":"arXiv:2303.17580","creators":[{"firstName":"Yongliang","lastName":"Shen","creatorType":"author"},{"firstName":"Kaitao","lastName":"Song","creatorType":"author"},{"firstName":"Xu","lastName":"Tan","creatorType":"author"},{"firstName":"Dongsheng","lastName":"Li","creatorType":"author"},{"firstName":"Weiming","lastName":"Lu","creatorType":"author"},{"firstName":"Yueting","lastName":"Zhuang","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-21T21:51:01Z","dateModified":"2023-09-21T21:51:01Z","uri":"http://zotero.org/users/11367251/items/VBFHYRC9","itemID":714,"attachments":[{"key":"F7IFV7K7","version":951,"itemType":"attachment","title":"shenHuggingGPTSolvingAI2023.pdf","parentItem":"VBFHYRC9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/shenHuggingGPTSolvingAI2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_F7IFV7K7/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/4\">Related Works</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/4\">HuggingGPT</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/4\">Task Planning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/6\">Model Selection</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/6\">Task Execution</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/7\">Response Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/7\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/7\">Settings</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/7\">Qualitative Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/8\">Quantitative Evaluation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/9\">Limitations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/9\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/10\">Acknowledgement</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">Appendix</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">More details</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">Template for Task Planning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">Model Descriptions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">Hybrid Endpoint in System Deployment</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">Task List</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/13\">GPT-4 Score</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/14\">Datasets for Task Planning Evaluation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/14\">Case Study</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/14\">Case Study on Various Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/14\">Case Study on Complex Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/15\">Case Study on More Scenarios</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_F7IFV7K7/15\">More Discussion about Related Works</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-21T21:51:05Z","dateModified":"2023-09-21T21:51:06Z","uri":"http://zotero.org/users/11367251/items/F7IFV7K7","localPath":"/Users/reyvababtista/Projects/Papers/shenHuggingGPTSolvingAI2023.pdf","defaultPath":"files/715/shenHuggingGPTSolvingAI2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400624,"version":0},"$loki":265},{"itemID":508,"item":{"key":"VGCLPAYH","version":661,"itemType":"journalArticle","title":"This new conversational AI model can be your friend, philosopher, and guide ... and even your worst enemy","date":"01/2023","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S2666389922003233","accessDate":"2023-06-21T15:18:48Z","volume":"4","pages":"100676","publicationTitle":"Patterns","DOI":"10.1016/j.patter.2022.100676","issue":"1","journalAbbreviation":"Patterns","ISSN":"26663899","creators":[{"firstName":"Joyjit","lastName":"Chatterjee","creatorType":"author"},{"firstName":"Nina","lastName":"Dethlefs","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-21T15:18:49Z","dateModified":"2023-06-21T15:18:49Z","uri":"http://zotero.org/users/11367251/items/VGCLPAYH","itemID":508,"attachments":[{"key":"IYZRR5AS","version":664,"itemType":"attachment","title":"chatterjeeThisnewconversational2023.pdf","parentItem":"VGCLPAYH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/chatterjeeThisnewconversational2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_IYZRR5AS/1\">Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IYZRR5AS/1\">Our experimental interactions with the ChatGPT</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IYZRR5AS/3\">Call to action</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IYZRR5AS/3\">Acknowledgments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IYZRR5AS/3\">Declaration of interests</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IYZRR5AS/3\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-21T15:19:10Z","dateModified":"2023-06-21T15:19:10Z","uri":"http://zotero.org/users/11367251/items/IYZRR5AS","localPath":"/Users/reyvababtista/Projects/Papers/chatterjeeThisnewconversational2023.pdf","defaultPath":"files/509/chatterjeeThisnewconversational2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400625,"version":0},"$loki":266},{"itemID":556,"item":{"key":"VLMU6SXQ","version":742,"itemType":"journalArticle","title":"Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis","abstractNote":"Background: The COVID-19 pandemic has had a significant impact on health care workers worldwide. Vaccination has been identified as a key strategy to reduce the risk of severe illness and death among health care workers, but the effectiveness of vaccination in preventing hospitalization among health care workers is not well understood.\nMethods: We used a simulated data set of 100 000 health care workers to estimate the effect of vaccination on the risk of hospitalization among health care workers during the COVID-19 pandemic. The data set included information on the health care workers' ages, body mass index (BMI), and number of comorbidities. We simulated infections with a small probability of hospitalization due to COVID-19 and a subset of individuals were vaccinated with a fictional vaccine that reduced the risk of hospitalization after infection. We performed a survival analysis using the Cox proportional hazards model to estimate the hazard ratio of hospitalization among vaccinated health care workers compared with unvaccinated health care workers.\nResults: The hazard ratio (HR) for vaccination status was 0.48 (95% confidence interval [CI] 0.28-0.86), indicating that the risk of hospitalization among vaccinated health care workers was about half that of unvaccinated health care workers. Other covariates in the model did not have a statistically significant effect on the hazard of hospitalization.\nConclusion: Our study indicates that vaccination is associated with a significantly reduced risk of hospitalization among health care workers during the COVID-19 pandemic. These results support the use of vaccination as a key strategy to reduce the risk of severe illness and death among health care workers.","date":"2023-02-17","language":"en","shortTitle":"Can ChatGPT draft a research article?","libraryCatalog":"DOI.org (Crossref)","url":"https://jogh.org/2023/jogh-13-01003","accessDate":"2023-08-21T15:17:02Z","volume":"13","pages":"01003","publicationTitle":"Journal of Global Health","DOI":"10.7189/jogh.13.01003","journalAbbreviation":"J Glob Health","ISSN":"2047-2978, 2047-2986","creators":[{"firstName":"Calum","lastName":"Macdonald","creatorType":"author"},{"firstName":"Davies","lastName":"Adeloye","creatorType":"author"},{"firstName":"Aziz","lastName":"Sheikh","creatorType":"author"},{"firstName":"Igor","lastName":"Rudan","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-08-21T15:17:02Z","dateModified":"2023-08-21T15:17:02Z","uri":"http://zotero.org/users/11367251/items/VLMU6SXQ","itemID":556,"attachments":[{"key":"7K2DJ4SB","version":744,"itemType":"attachment","title":"macdonaldCanChatGPTdraft2023.pdf","parentItem":"VLMU6SXQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/macdonaldCanChatGPTdraft2023.pdf","tags":[],"relations":{},"dateAdded":"2023-08-21T15:17:07Z","dateModified":"2023-08-21T15:17:07Z","uri":"http://zotero.org/users/11367251/items/7K2DJ4SB","localPath":"/Users/reyvababtista/Projects/Papers/macdonaldCanChatGPTdraft2023.pdf","defaultPath":"files/557/macdonaldCanChatGPTdraft2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400625,"version":0},"$loki":267},{"itemID":262,"item":{"key":"VLYEPWW9","version":252,"itemType":"journalArticle","title":"AWARE: Mobile Context Instrumentation Framework","date":"2015-04-20","language":"en","shortTitle":"AWARE","libraryCatalog":"DOI.org (Crossref)","url":"http://journal.frontiersin.org/article/10.3389/fict.2015.00006/abstract","accessDate":"2023-03-22T19:30:46Z","volume":"2","publicationTitle":"Frontiers in ICT","DOI":"10.3389/fict.2015.00006","journalAbbreviation":"Front. ICT","ISSN":"2297-198X","creators":[{"firstName":"Denzil","lastName":"Ferreira","creatorType":"author"},{"firstName":"Vassilis","lastName":"Kostakos","creatorType":"author"},{"firstName":"Anind K.","lastName":"Dey","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-03-22T19:30:46Z","dateModified":"2023-03-22T19:33:10Z","uri":"http://zotero.org/users/11367251/items/VLYEPWW9","itemID":262,"attachments":[{"key":"9QRHDZ9I","version":250,"itemType":"attachment","title":"ferreiraAWAREMobileContext2015.pdf","parentItem":"VLYEPWW9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/ferreiraAWAREMobileContext2015.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T19:30:44Z","dateModified":"2023-03-22T19:30:54Z","uri":"http://zotero.org/users/11367251/items/9QRHDZ9I","localPath":"/Users/reyvababtista/Projects/Papers/ferreiraAWAREMobileContext2015.pdf","defaultPath":"files/261/ferreiraAWAREMobileContext2015.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400626,"version":0},"$loki":268},{"itemID":424,"item":{"key":"VNP4FTAA","version":510,"itemType":"webpage","title":"The R Project for Statistical Computing","date":"2023-05-04","url":"https://www.r-project.org","accessDate":"2023-05-04","websiteTitle":"The R Project for Statistical Computing","creators":[{"name":"The R Foundation","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T01:54:01Z","dateModified":"2023-05-05T01:54:27Z","uri":"http://zotero.org/users/11367251/items/VNP4FTAA","itemID":424,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400626,"version":0},"$loki":269},{"itemID":1670,"item":{"key":"VSLMSIXH","version":2092,"itemType":"journalArticle","title":"Backpropagation through time: what it does and how to do it","date":"Oct./1990","shortTitle":"Backpropagation through time","libraryCatalog":"DOI.org (Crossref)","url":"http://ieeexplore.ieee.org/document/58337/","accessDate":"2023-12-04T04:44:35Z","volume":"78","pages":"1550-1560","publicationTitle":"Proceedings of the IEEE","DOI":"10.1109/5.58337","issue":"10","journalAbbreviation":"Proc. IEEE","ISSN":"00189219","creators":[{"firstName":"P.J.","lastName":"Werbos","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T04:44:35Z","dateModified":"2023-12-04T04:44:35Z","uri":"http://zotero.org/users/11367251/items/VSLMSIXH","itemID":1670,"attachments":[{"key":"R47VB9J7","version":2093,"itemType":"attachment","title":"werbosBackpropagationtimewhat1990.pdf","parentItem":"VSLMSIXH","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/werbosBackpropagationtimewhat1990.pdf","tags":[],"relations":{},"dateAdded":"2023-12-04T04:44:40Z","dateModified":"2023-12-04T04:44:40Z","uri":"http://zotero.org/users/11367251/items/R47VB9J7","localPath":"/Users/reyvababtista/Projects/Papers/werbosBackpropagationtimewhat1990.pdf","defaultPath":"files/1672/werbosBackpropagationtimewhat1990.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400626,"version":0},"$loki":270},{"itemID":925,"item":{"key":"VSY8LXMF","version":1189,"itemType":"blogPost","title":"Improving Search for the next 20 years","date":"08/24/2018","url":"https://blog.google/products/search/improving-search-next-20-years/","accessDate":"2023-11-04","blogTitle":"Improving Search for the next 20 years","creators":[{"name":"Ben Gomes","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-04T23:10:07Z","dateModified":"2023-11-04T23:10:46Z","uri":"http://zotero.org/users/11367251/items/VSY8LXMF","itemID":925,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400626,"version":0},"$loki":271},{"itemID":592,"item":{"key":"VV7YM9WL","version":791,"itemType":"preprint","title":"Dynamic Routing Between Capsules","abstractNote":"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a speciﬁc type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.","date":"2017-11-07","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1710.09829","accessDate":"2023-09-05T13:41:53Z","extra":"arXiv:1710.09829 [cs]","repository":"arXiv","archiveID":"arXiv:1710.09829","creators":[{"firstName":"Sara","lastName":"Sabour","creatorType":"author"},{"firstName":"Nicholas","lastName":"Frosst","creatorType":"author"},{"firstName":"Geoffrey E.","lastName":"Hinton","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:41:53Z","dateModified":"2023-09-05T13:41:54Z","uri":"http://zotero.org/users/11367251/items/VV7YM9WL","itemID":592,"attachments":[{"key":"MF9RA9RG","version":817,"itemType":"attachment","title":"sabourDynamicRoutingCapsules2017.pdf","parentItem":"VV7YM9WL","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/sabourDynamicRoutingCapsules2017.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_MF9RA9RG/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/2\">2 How the vector inputs and outputs of a capsule are computed</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/3\">3 Margin loss for digit existence</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/3\">4 CapsNet architecture</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/4\">4.1 Reconstruction as a regularization method</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/5\">5 Capsules on MNIST</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/5\">5.1 What the individual dimensions of a capsule represent</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/6\">5.2 Robustness to Affine Transformations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/6\">6 Segmenting highly overlapping digits</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/6\">6.1 MultiMNIST dataset</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/7\">6.2 MultiMNIST results</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/8\">7 Other datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/8\">8 Discussion and previous work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_MF9RA9RG/11\">A How many routing iterations to use?</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:28Z","dateModified":"2023-09-05T13:47:29Z","uri":"http://zotero.org/users/11367251/items/MF9RA9RG","localPath":"/Users/reyvababtista/Projects/Papers/sabourDynamicRoutingCapsules2017.pdf","defaultPath":"files/618/sabourDynamicRoutingCapsules2017.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400627,"version":0},"$loki":272},{"itemID":1608,"item":{"key":"VXDH3G52","version":1879,"itemType":"blogPost","title":"New models and developer products announced at DevDay","date":"11/06/2023","url":"https://openai.com/blog/new-models-and-developer-products-announced-at-devday","accessDate":"2023-11-25","blogTitle":"New models and developer products announced at DevDay","creators":[{"name":"OpenAI","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-11-25T15:04:00Z","dateModified":"2023-11-25T15:04:57Z","uri":"http://zotero.org/users/11367251/items/VXDH3G52","itemID":1608,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400627,"version":0},"$loki":273},{"itemID":1659,"item":{"key":"VXPS82WM","version":2081,"itemType":"journalArticle","title":"CatBoost: unbiased boosting with categorical features","abstractNote":"This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.","date":"2017","shortTitle":"CatBoost","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/1706.09516","accessDate":"2023-12-04T00:34:08Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 5","DOI":"10.48550/ARXIV.1706.09516","creators":[{"firstName":"Liudmila","lastName":"Prokhorenkova","creatorType":"author"},{"firstName":"Gleb","lastName":"Gusev","creatorType":"author"},{"firstName":"Aleksandr","lastName":"Vorobev","creatorType":"author"},{"firstName":"Anna Veronika","lastName":"Dorogush","creatorType":"author"},{"firstName":"Andrey","lastName":"Gulin","creatorType":"author"}],"tags":[{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1}],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T00:34:08Z","dateModified":"2023-12-04T00:34:08Z","uri":"http://zotero.org/users/11367251/items/VXPS82WM","itemID":1659,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400627,"version":0},"$loki":274},{"itemID":1699,"item":{"key":"VY7WQMJP","version":2164,"itemType":"journalArticle","title":"Backpropagation Applied to Handwritten Zip Code Recognition","abstractNote":"The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.","date":"12/1989","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://direct.mit.edu/neco/article/1/4/541-551/5515","accessDate":"2023-12-07T15:03:11Z","volume":"1","pages":"541-551","publicationTitle":"Neural Computation","DOI":"10.1162/neco.1989.1.4.541","issue":"4","journalAbbreviation":"Neural Computation","ISSN":"0899-7667, 1530-888X","creators":[{"firstName":"Y.","lastName":"LeCun","creatorType":"author"},{"firstName":"B.","lastName":"Boser","creatorType":"author"},{"firstName":"J. S.","lastName":"Denker","creatorType":"author"},{"firstName":"D.","lastName":"Henderson","creatorType":"author"},{"firstName":"R. E.","lastName":"Howard","creatorType":"author"},{"firstName":"W.","lastName":"Hubbard","creatorType":"author"},{"firstName":"L. D.","lastName":"Jackel","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-07T15:03:11Z","dateModified":"2023-12-07T15:03:11Z","uri":"http://zotero.org/users/11367251/items/VY7WQMJP","itemID":1699,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400628,"version":0},"$loki":275},{"itemID":577,"item":{"key":"W37B8PCF","version":772,"itemType":"preprint","title":"Auto-Encoding Variational Bayes","abstractNote":"How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efﬁcient by ﬁtting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.","date":"2022-12-10","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1312.6114","accessDate":"2023-09-05T13:35:08Z","extra":"arXiv:1312.6114 [cs, stat]","repository":"arXiv","archiveID":"arXiv:1312.6114","creators":[{"firstName":"Diederik P.","lastName":"Kingma","creatorType":"author"},{"firstName":"Max","lastName":"Welling","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Statistics - Machine Learning","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:35:08Z","dateModified":"2023-09-05T13:35:08Z","uri":"http://zotero.org/users/11367251/items/W37B8PCF","itemID":577,"attachments":[{"key":"SSJNIIL4","version":817,"itemType":"attachment","title":"kingmaAutoEncodingVariationalBayes2022.pdf","parentItem":"W37B8PCF","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/kingmaAutoEncodingVariationalBayes2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_SSJNIIL4/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/1\">2 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/2\">2.1 Problem scenario</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/3\">2.2 The variational bound</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/3\">2.3 The SGVB estimator and AEVB algorithm</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/4\">2.4 The reparameterization trick</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/5\">3 Example: Variational Auto-Encoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/6\">4 Related work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/6\">5 Experiments</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/8\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/8\">7 Future work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/9\">A Visualisations</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/10\">B Solution of - DKL(qbold0mu mumu 2005/06/28 ver: 1.3 subfig package(z) || pbold0mu mumu 2005/06/28 ver: 1.3 subfig package(z)), Gaussian case</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/11\">C MLP's as probabilistic encoders and decoders</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/11\">C.1 Bernoulli MLP as decoder</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/11\">C.2 Gaussian MLP as encoder or decoder</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/11\">D Marginal likelihood estimator</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/12\">E Monte Carlo EM</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/12\">F Full VB</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_SSJNIIL4/13\">F.1 Example</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:26Z","dateModified":"2023-09-05T13:47:26Z","uri":"http://zotero.org/users/11367251/items/SSJNIIL4","localPath":"/Users/reyvababtista/Projects/Papers/kingmaAutoEncodingVariationalBayes2022.pdf","defaultPath":"files/614/kingmaAutoEncodingVariationalBayes2022.pdf"}],"notes":[{"key":"3698HGWE","version":772,"itemType":"note","parentItem":"W37B8PCF","note":"Comment: Fixes a typo in the abstract, no other changes","tags":[],"relations":{},"dateAdded":"2023-09-05T13:35:08Z","dateModified":"2023-09-05T13:35:08Z","uri":"http://zotero.org/users/11367251/items/3698HGWE"}]},"meta":{"revision":0,"created":1709832400628,"version":0},"$loki":276},{"itemID":1629,"item":{"key":"W7S5IDJX","version":2188,"itemType":"webpage","title":"K-means","date":"2023/11/28","url":"https://scikit-learn.org/stable/modules/clustering.html#k-means","accessDate":"2023-11-28","websiteTitle":"K-means","creators":[{"name":"Skicit-learn","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T05:19:35Z","dateModified":"2023-12-08T17:18:51Z","uri":"http://zotero.org/users/11367251/items/W7S5IDJX","itemID":1629,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400629,"version":0},"$loki":277},{"itemID":20,"item":{"key":"W7Y6K2HQ","version":199,"itemType":"journalArticle","title":"Machine Vision Based Traffic Sign Detection Methods: Review, Analyses and Perspectives","abstractNote":"Trafﬁc signs recognition (TSR) is an important part of some advanced driver-assistance systems (ADASs) and auto driving systems (ADSs). As the ﬁrst key step of TSR, trafﬁc sign detection (TSD) is a challenging problem because of different types, small sizes, complex driving scenes, and occlusions. In recent years, there have been a large number of TSD algorithms based on machine vision and pattern recognition. In this paper, a comprehensive review of the literature on TSD is presented. We divide the reviewed detection methods into ﬁve main categories: color-based methods, shape-based methods, color- and shape-based methods, machine-learning-based methods, and LIDAR-based methods. The methods in each category are also classiﬁed into different subcategories for understanding and summarizing the mechanisms of different methods. For some reviewed methods that lack comparisons on public datasets, we reimplemented part of these methods for comparison. The experimental comparisons and analyses are presented on the reported performance and the performance of our reimplemented methods. Furthermore, future directions and recommendations of the TSD research are given to promote the development of the TSD.","date":"2019","language":"en","shortTitle":"Machine Vision Based Traffic Sign Detection Methods","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8746141/","accessDate":"2023-03-20T14:25:02Z","volume":"7","pages":"86578-86596","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2019.2924947","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Chunsheng","lastName":"Liu","creatorType":"author"},{"firstName":"Shuang","lastName":"Li","creatorType":"author"},{"firstName":"Faliang","lastName":"Chang","creatorType":"author"},{"firstName":"Yinhai","lastName":"Wang","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/BTKFN2B9"]},"dateAdded":"2023-03-20T14:25:02Z","dateModified":"2023-03-22T17:03:42Z","uri":"http://zotero.org/users/11367251/items/W7Y6K2HQ","itemID":20,"attachments":[{"key":"UHQ9A3UE","version":241,"itemType":"attachment","title":"liuMachineVisionBased2019.pdf","parentItem":"W7Y6K2HQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/liuMachineVisionBased2019.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UHQ9A3UE/1\">INTRODUCTION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/2\">TRAFFIC SIGN</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/3\">TRAFFIC SIGNS FOR HUMAN DRIVING SAFETY</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/3\">MACHINE VISION BASED TSR SYSTEMS AND THEIR APPLICATIONS</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">BENCHMARKS FOR TSR</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">GERMAN TRAFFIC SIGN BENCHMARK (GTSB)</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">BELGIUMTS (BTS) DATASET [25]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">TSINGHUA-TENCENT 100K (TT100K) [10]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">LISA DATASET [11]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">SWEDISH TRAFFIC SIGNS (STS) DATASET [26]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/4\">RUG DATASET [27]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">STEREOPOLIS DATABASE [28]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">FLEYEH TRAFFIC SIGNS DATASET (FTSD) [29]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">MAPPING AND ASSESSING THE STATE OF TRAFFIC INFRASTRUCTURE (MASTIF) DATASET [18]</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">EUROPEAN TRAFFIC SIGN DATASET (ETSD) [91]</a></li></ul></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">OVERVIEW OF TRAFFIC SIGN DETECTION</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">COLOR BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">REVIEW OF COLOR BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/5\">RGB BASED THRESHOLDING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/6\">HUE AND SATURATION THRESHOLDING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/6\">THRESHOLDING ON OTHER SPACES</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/6\">CHROMATIC/ACHROMATIC DECOMPOSITION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/6\">PIXEL CLASSIFICATION</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/7\">ANALYSIS OF THE COLOR BASED METHODS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/7\">SHAPE BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/7\">REVIEW OF SHAPE BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/7\">SHAPE DETECTION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">SHAPE ANALYSIS AND MATCHING</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">FOURIER TRANSFORMATION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">KEY POINTS DETECTION</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">ANALYSIS OF THE SHAPE BASED METHODS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">COLOR AND SHAPE BASED METHODS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">REVIEW OF COLOR AND SHAPE BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/8\">EXTREME REGIONS BASED DETECTION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/10\">HIGH CONTRASTED MARGIN REGIONS BASED DETECTION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/10\">SALIENCY DETECTION</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/10\">ANALYSIS OF THE COLOR AND SHAPE BASED DETECTION METHODS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/11\">MACHINE LEARNING BASED METHODS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/11\">REVIEW OF MACHINE LEARNING BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/11\">ADABOOST BASED METHODS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/12\">SVM BASED METHODS</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/13\">CNN BASED METHODS</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/14\">ANALYSIS OF THE MACHINE LEARNING BASED METHODS</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/15\">LIDAR BASED METHODS</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/16\">REVIEW OF LIDAR BASED DETECTION METHODS</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/16\">DATA CLOUD BASED DETECTION</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/16\">DATA CLOUD AND RGB IMAGE BASED DETECTION</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/16\">ANALYSIS OF THE LIDAR BASED TSD METHODS</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/17\">CONCLUSIONS AND PERSPECTIVES</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UHQ9A3UE/17\">REFERENCES</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:04Z","dateModified":"2023-03-22T18:52:23Z","uri":"http://zotero.org/users/11367251/items/UHQ9A3UE","localPath":"/Users/reyvababtista/Projects/Papers/liuMachineVisionBased2019.pdf","defaultPath":"files/250/liuMachineVisionBased2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400630,"version":0},"$loki":278},{"itemID":971,"item":{"key":"W8QJ72GD","version":1352,"itemType":"preprint","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision","abstractNote":"With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-speciﬁc objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single preﬁx language modeling objective. Without utilizing extra data or task-speciﬁc customization, the resulting model signiﬁcantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.","date":"2022-05-15","language":"en","shortTitle":"SimVLM","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2108.10904","accessDate":"2023-11-07T22:33:56Z","extra":"arXiv:2108.10904 [cs]","repository":"arXiv","archiveID":"arXiv:2108.10904","creators":[{"firstName":"Zirui","lastName":"Wang","creatorType":"author"},{"firstName":"Jiahui","lastName":"Yu","creatorType":"author"},{"firstName":"Adams Wei","lastName":"Yu","creatorType":"author"},{"firstName":"Zihang","lastName":"Dai","creatorType":"author"},{"firstName":"Yulia","lastName":"Tsvetkov","creatorType":"author"},{"firstName":"Yuan","lastName":"Cao","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T22:33:56Z","dateModified":"2023-11-07T22:33:56Z","uri":"http://zotero.org/users/11367251/items/W8QJ72GD","itemID":971,"attachments":[{"key":"5UJQP6J8","version":1356,"itemType":"attachment","title":"wangSimVLMSimpleVisual2022.pdf","parentItem":"W8QJ72GD","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/wangSimVLMSimpleVisual2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_5UJQP6J8/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/2\">2 Related Work</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/3\">3 SimVLM</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/3\">3.1 Background</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/4\">3.2 Proposed Objective: Prefix Language Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/4\">3.3 Architecture</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/4\">3.4 Datasets</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/5\">4 Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/5\">4.1 Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/5\">4.2 Comparison with existing approaches</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/6\">4.3 Zero-Shot Generalization</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/6\">4.3.1 Zero-shot/Few-shot Image Captioning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/7\">4.3.2 Zero-shot cross-modality Transfer</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/7\">4.3.3 Open-ended VQA</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/9\">4.4 Analysis</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/9\">5 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/15\">A Generated Examples</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/16\">B Experimental Details</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/16\">B.1 Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/16\">B.2 Finetuning</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/17\">C Model Performance on Language-only Task</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_5UJQP6J8/17\">D Erratum</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-07T22:34:00Z","dateModified":"2023-11-07T22:34:01Z","uri":"http://zotero.org/users/11367251/items/5UJQP6J8","localPath":"/Users/reyvababtista/Projects/Papers/wangSimVLMSimpleVisual2022.pdf","defaultPath":"files/973/wangSimVLMSimpleVisual2022.pdf"}],"notes":[{"key":"EU4TDALE","version":1352,"itemType":"note","parentItem":"W8QJ72GD","note":"Comment: Published at ICLR 2022","tags":[],"relations":{},"dateAdded":"2023-11-07T22:33:56Z","dateModified":"2023-11-07T22:33:56Z","uri":"http://zotero.org/users/11367251/items/EU4TDALE"}]},"meta":{"revision":0,"created":1709832400630,"version":0},"$loki":279},{"itemID":546,"item":{"key":"WBV2IFYP","version":721,"itemType":"conferencePaper","title":"Few-shot training LLMs for project-specific code-summarization","abstractNote":"Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Fewshotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pretrained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for codesummarization, leveraging project-specific training.","date":"2022-10-10","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://dl.acm.org/doi/10.1145/3551349.3559555","accessDate":"2023-06-30T14:19:24Z","place":"Rochester MI USA","publisher":"ACM","ISBN":"978-1-4503-9475-8","pages":"1-5","proceedingsTitle":"Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","conferenceName":"ASE '22: 37th IEEE/ACM International Conference on Automated Software Engineering","DOI":"10.1145/3551349.3559555","creators":[{"firstName":"Toufique","lastName":"Ahmed","creatorType":"author"},{"firstName":"Premkumar","lastName":"Devanbu","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-30T14:19:24Z","dateModified":"2023-06-30T14:19:24Z","uri":"http://zotero.org/users/11367251/items/WBV2IFYP","itemID":546,"attachments":[{"key":"UDF7YJY4","version":721,"itemType":"attachment","title":"ahmedFewshottrainingLLMs2022.pdf","parentItem":"WBV2IFYP","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/ahmedFewshottrainingLLMs2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_UDF7YJY4/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/2\">2 Background and Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/2\">3 Methodology</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/3\">4 Result</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/3\">4.1 Cross-project few-shot</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/3\">4.2 Same-project few-shot</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/3\">4.3 Are we getting statistically significant result?</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/3\">4.4 Zero-shot and one-shot training</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/4\">5 Threats</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/4\">6 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_UDF7YJY4/5\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-06-30T14:19:27Z","dateModified":"2023-06-30T14:19:28Z","uri":"http://zotero.org/users/11367251/items/UDF7YJY4","localPath":"/Users/reyvababtista/Projects/Papers/ahmedFewshottrainingLLMs2022.pdf","defaultPath":"files/547/ahmedFewshottrainingLLMs2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400631,"version":0},"$loki":280},{"itemID":1896,"item":{"key":"WIDCJ3KQ","version":2448,"itemType":"journalArticle","title":"Non-Invasive Blood Pressure Estimation from ECG Using Machine Learning Techniques","date":"2018-04-11","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://www.mdpi.com/1424-8220/18/4/1160","accessDate":"2024-02-06T22:34:26Z","volume":"18","pages":"1160","publicationTitle":"Sensors","DOI":"10.3390/s18041160","issue":"4","journalAbbreviation":"Sensors","ISSN":"1424-8220","creators":[{"firstName":"Monika","lastName":"Simjanoska","creatorType":"author"},{"firstName":"Martin","lastName":"Gjoreski","creatorType":"author"},{"firstName":"Matjaž","lastName":"Gams","creatorType":"author"},{"firstName":"Ana","lastName":"Madevska Bogdanova","creatorType":"author"}],"tags":[],"collections":["TAUT9NML"],"relations":{},"dateAdded":"2024-02-06T22:34:26Z","dateModified":"2024-02-06T22:34:26Z","uri":"http://zotero.org/users/11367251/items/WIDCJ3KQ","itemID":1896,"attachments":[{"key":"Q57Z67PN","version":2448,"itemType":"attachment","title":"simjanoskaNonInvasiveBloodPressure2018.pdf","parentItem":"WIDCJ3KQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/simjanoskaNonInvasiveBloodPressure2018.pdf","tags":[],"relations":{},"dateAdded":"2024-02-06T22:34:28Z","dateModified":"2024-02-06T22:34:28Z","uri":"http://zotero.org/users/11367251/items/Q57Z67PN","localPath":"/Users/reyvababtista/Projects/Papers/simjanoskaNonInvasiveBloodPressure2018.pdf","defaultPath":"files/1898/simjanoskaNonInvasiveBloodPressure2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400631,"version":0},"$loki":281},{"itemID":792,"item":{"key":"WJRM73KQ","version":1046,"itemType":"preprint","title":"Perceiver: General Perception with Iterative Attention","abstractNote":"Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspeciﬁc assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classiﬁcation tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.","date":"2021-06-22","language":"en","shortTitle":"Perceiver","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2103.03206","accessDate":"2023-10-17T21:46:32Z","extra":"arXiv:2103.03206 [cs, eess]","repository":"arXiv","archiveID":"arXiv:2103.03206","creators":[{"firstName":"Andrew","lastName":"Jaegle","creatorType":"author"},{"firstName":"Felix","lastName":"Gimeno","creatorType":"author"},{"firstName":"Andrew","lastName":"Brock","creatorType":"author"},{"firstName":"Andrew","lastName":"Zisserman","creatorType":"author"},{"firstName":"Oriol","lastName":"Vinyals","creatorType":"author"},{"firstName":"Joao","lastName":"Carreira","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-17T21:46:32Z","dateModified":"2023-10-17T21:46:33Z","uri":"http://zotero.org/users/11367251/items/WJRM73KQ","itemID":792,"attachments":[{"key":"GLRHYJU6","version":1049,"itemType":"attachment","title":"jaeglePerceiverGeneralPerception2021.pdf","parentItem":"WJRM73KQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/jaeglePerceiverGeneralPerception2021.pdf","tags":[],"relations":{},"dateAdded":"2023-10-17T21:46:37Z","dateModified":"2023-10-17T21:46:37Z","uri":"http://zotero.org/users/11367251/items/GLRHYJU6","localPath":"/Users/reyvababtista/Projects/Papers/jaeglePerceiverGeneralPerception2021.pdf","defaultPath":"files/794/jaeglePerceiverGeneralPerception2021.pdf"}],"notes":[{"key":"B3IB5Z2D","version":1046,"itemType":"note","parentItem":"WJRM73KQ","note":"Comment: ICML 2021","tags":[],"relations":{},"dateAdded":"2023-10-17T21:46:32Z","dateModified":"2023-10-17T21:46:32Z","uri":"http://zotero.org/users/11367251/items/B3IB5Z2D"}]},"meta":{"revision":0,"created":1709832400631,"version":0},"$loki":282},{"itemID":446,"item":{"key":"WK6CBMB2","version":564,"itemType":"blogPost","title":"YOLOv5","date":"5/9/2023","url":"https://github.com/ultralytics/yolov5","accessDate":"2023-05-09","blogTitle":"YOLOv5","creators":[{"name":"Ultralytics","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-05-10T01:10:10Z","dateModified":"2023-05-10T01:10:24Z","uri":"http://zotero.org/users/11367251/items/WK6CBMB2","itemID":446,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400632,"version":0},"$loki":283},{"itemID":1755,"item":{"key":"WRSBBKVQ","version":2206,"itemType":"report","title":"Language models generalize beyond natural proteins","abstractNote":"Abstract\n          \n            Learning the design patterns of proteins from sequences across evolution may have promise toward generative protein design. However it is unknown whether language models, trained on sequences of natural proteins, will be capable of more than memorization of existing protein families. Here we show that language models generalize beyond natural proteins to generate\n            de novo\n            proteins. We focus on two protein design tasks: fixed backbone design where the structure is specified, and unconstrained generation where the structure is sampled from the model. Remarkably although the models are trained only on sequences, we find that they are capable of designing structure. A total of 228 generated proteins are evaluated experimentally with high overall success rates (152/228 or 67%) in producing a soluble and monomeric species by size exclusion chromatography. Out of 152 experimentally successful designs, 35 have no significant sequence match to known natural proteins. Of the remaining 117, sequence identity to the nearest sequence match is at median 27%, below 20% for 6 designs, and as low as 18% for 3 designs. For fixed backbone design, the language model generates successful designs for each of eight experimentally evaluated artificially created fixed backbone targets. For unconstrained generation, sampled proteins cover diverse topologies and secondary structure compositions, and have high experimental success rate (71/129 or 55%). The designs reflect deep patterns linking sequence and structure, including motifs that occur in related natural structures, and motifs that are not observed in similar structural contexts in known protein families. The results show that language models, though only trained on sequences, learn a deep grammar that enables the design of protein structure, extending beyond natural proteins.","date":"2022-12-22","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"http://biorxiv.org/lookup/doi/10.1101/2022.12.21.521521","accessDate":"2023-12-11T02:11:42Z","extra":"DOI: 10.1101/2022.12.21.521521","reportType":"preprint","institution":"Synthetic Biology","creators":[{"firstName":"Robert","lastName":"Verkuil","creatorType":"author"},{"firstName":"Ori","lastName":"Kabeli","creatorType":"author"},{"firstName":"Yilun","lastName":"Du","creatorType":"author"},{"firstName":"Basile I. M.","lastName":"Wicky","creatorType":"author"},{"firstName":"Lukas F.","lastName":"Milles","creatorType":"author"},{"firstName":"Justas","lastName":"Dauparas","creatorType":"author"},{"firstName":"David","lastName":"Baker","creatorType":"author"},{"firstName":"Sergey","lastName":"Ovchinnikov","creatorType":"author"},{"firstName":"Tom","lastName":"Sercu","creatorType":"author"},{"firstName":"Alexander","lastName":"Rives","creatorType":"author"}],"tags":[],"collections":["6HQNAPTR"],"relations":{},"dateAdded":"2023-12-11T02:11:42Z","dateModified":"2023-12-11T02:11:42Z","uri":"http://zotero.org/users/11367251/items/WRSBBKVQ","itemID":1755,"attachments":[{"key":"IMVC598E","version":2207,"itemType":"attachment","title":"verkuilLanguagemodelsgeneralize2022.pdf","parentItem":"WRSBBKVQ","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/verkuilLanguagemodelsgeneralize2022.pdf","tags":[],"relations":{},"dateAdded":"2023-12-11T02:11:49Z","dateModified":"2023-12-11T02:11:49Z","uri":"http://zotero.org/users/11367251/items/IMVC598E","localPath":"/Users/reyvababtista/Projects/Papers/verkuilLanguagemodelsgeneralize2022.pdf","defaultPath":"files/1757/verkuilLanguagemodelsgeneralize2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400633,"version":0},"$loki":284},{"itemID":26,"item":{"key":"WTTFTVH5","version":200,"itemType":"journalArticle","title":"Hyperspectral Image Classification Using Random Occlusion Data Augmentation","abstractNote":"Convolutional neural networks (CNNs) have become a powerful tool for remotely sensed hyperspectral image (HSI) classiﬁcation due to their great generalization ability and high accuracy. However, owing to the huge amount of parameters that need to be learned and to the complex nature of HSI data itself, these approaches must deal with the important problem of overﬁtting, which can lead to inadequate generalization and loss of accuracy. In order to mitigate this problem, in this letter, we adopt random occlusion, a recently developed data augmentation (DA) method for training CNNs, in which the pixels of different rectangular spatial regions in the HSI are randomly occluded, generating training images with various levels of occlusion and reducing the risk of overﬁtting. Our results with two well-known HSIs reveal that the proposed method helps to achieve better classiﬁcation accuracy with low computational cost.","date":"11/2019","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8694852/","accessDate":"2023-03-20T14:25:11Z","volume":"16","pages":"1751-1755","publicationTitle":"IEEE Geoscience and Remote Sensing Letters","DOI":"10.1109/LGRS.2019.2909495","issue":"11","journalAbbreviation":"IEEE Geosci. Remote Sensing Lett.","ISSN":"1545-598X, 1558-0571","creators":[{"firstName":"Juan Mario","lastName":"Haut","creatorType":"author"},{"firstName":"Mercedes E.","lastName":"Paoletti","creatorType":"author"},{"firstName":"Javier","lastName":"Plaza","creatorType":"author"},{"firstName":"Antonio","lastName":"Plaza","creatorType":"author"},{"firstName":"Li","lastName":"Plaza","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/KCY8UWBM"]},"dateAdded":"2023-03-20T14:25:11Z","dateModified":"2023-03-22T17:03:42Z","uri":"http://zotero.org/users/11367251/items/WTTFTVH5","itemID":26,"attachments":[{"key":"V8TSNHBP","version":241,"itemType":"attachment","title":"hautHyperspectralImageClassification2019.pdf","parentItem":"WTTFTVH5","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hautHyperspectralImageClassification2019.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:05Z","dateModified":"2023-03-22T18:52:23Z","uri":"http://zotero.org/users/11367251/items/V8TSNHBP","localPath":"/Users/reyvababtista/Projects/Papers/hautHyperspectralImageClassification2019.pdf","defaultPath":"files/251/hautHyperspectralImageClassification2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400634,"version":0},"$loki":285},{"itemID":589,"item":{"key":"WURQNQR4","version":787,"itemType":"journalArticle","title":"Centre for Cognitive Science, University of Edinburgh, 2, Buccleuch Place, Edinburgh EH8 9LW, Scotland April 1996","abstractNote":"This document is an introduction to radial basis function (RBF) networks, a type of arti cial neural network for application to problems of supervised learning (e.g. regression, classi cation and time series prediction). It is now only available in PostScript2 (an older and now unsupported hyper-text version3 may be available for a while longer).","language":"en","libraryCatalog":"Zotero","creators":[{"firstName":"Mark J L","lastName":"Orr","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-09-05T13:41:20Z","dateModified":"2023-09-05T13:41:20Z","uri":"http://zotero.org/users/11367251/items/WURQNQR4","itemID":589,"attachments":[{"key":"RXUSD8J8","version":817,"itemType":"attachment","title":"orrCentreCognitiveScience.pdf","parentItem":"WURQNQR4","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/orrCentreCognitiveScience.pdf","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:27Z","dateModified":"2023-09-05T13:47:27Z","uri":"http://zotero.org/users/11367251/items/RXUSD8J8","localPath":"/Users/reyvababtista/Projects/Papers/orrCentreCognitiveScience.pdf","defaultPath":"files/617/orrCentreCognitiveScience.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400635,"version":0},"$loki":286},{"itemID":987,"item":{"key":"WV5NR9IT","version":1367,"itemType":"preprint","title":"Pre-training image-language transformers for open-vocabulary tasks","abstractNote":"We present a pre-training approach for vision and language transformer models, which is based on a mixture of diverse tasks. We explore both the use of image-text captioning data in pre-training, which does not need additional supervision, as well as object-aware strategies to pre-train the model. We evaluate the method on a number of textgenerative vision+language tasks, such as Visual Question Answering, visual entailment and captioning, and demonstrate large gains over standard pre-training methods.","date":"2022-09-09","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2209.04372","accessDate":"2023-11-07T22:49:08Z","extra":"arXiv:2209.04372 [cs]","repository":"arXiv","archiveID":"arXiv:2209.04372","creators":[{"firstName":"A. J.","lastName":"Piergiovanni","creatorType":"author"},{"firstName":"Weicheng","lastName":"Kuo","creatorType":"author"},{"firstName":"Anelia","lastName":"Angelova","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T22:49:08Z","dateModified":"2023-11-07T22:49:08Z","uri":"http://zotero.org/users/11367251/items/WV5NR9IT","itemID":987,"attachments":[{"key":"WE739QJ8","version":1371,"itemType":"attachment","title":"piergiovanniPretrainingimagelanguagetransformers2022.pdf","parentItem":"WV5NR9IT","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/piergiovanniPretrainingimagelanguagetransformers2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T22:49:13Z","dateModified":"2023-11-07T22:49:13Z","uri":"http://zotero.org/users/11367251/items/WE739QJ8","localPath":"/Users/reyvababtista/Projects/Papers/piergiovanniPretrainingimagelanguagetransformers2022.pdf","defaultPath":"files/988/piergiovanniPretrainingimagelanguagetransformers2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400657,"version":0},"$loki":287},{"itemID":1615,"item":{"key":"WWZK6PWS","version":1970,"itemType":"journalArticle","title":"Principal Component Analysis: A Natural Approach to Data Exploration","abstractNote":"Principal component analysis (PCA) is often used for analyzing data in the most diverse areas. In this work, we report an integrated approach to several theoretical and practical aspects of PCA. We start by providing, in an intuitive and accessible manner, the basic principles underlying PCA and its applications. Next, we present a systematic, though no exclusive, survey of some representative works illustrating the potential of PCA applications to a wide range of areas. An experimental investigation of the ability of PCA for variance explanation and dimensionality reduction is also developed, which confirms the efficacy of PCA and also shows that standardizing or not the original data can have important effects on the obtained results. Overall, we believe the several covered issues can assist researchers from the most diverse areas in using and interpreting PCA.","date":"2022-05-31","shortTitle":"Principal Component Analysis","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1804.02502","accessDate":"2023-11-29T04:59:26Z","extra":"arXiv:1804.02502 [cs, stat]","volume":"54","pages":"1-34","publicationTitle":"ACM Computing Surveys","DOI":"10.1145/3447755","issue":"4","journalAbbreviation":"ACM Comput. Surv.","ISSN":"0360-0300, 1557-7341","creators":[{"firstName":"Felipe L.","lastName":"Gewers","creatorType":"author"},{"firstName":"Gustavo R.","lastName":"Ferreira","creatorType":"author"},{"firstName":"Henrique F.","lastName":"de Arruda","creatorType":"author"},{"firstName":"Filipi N.","lastName":"Silva","creatorType":"author"},{"firstName":"Cesar H.","lastName":"Comin","creatorType":"author"},{"firstName":"Diego R.","lastName":"Amancio","creatorType":"author"},{"firstName":"Luciano da F.","lastName":"Costa","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computational Engineering, Finance, and Science","type":1},{"tag":"Statistics - Computation","type":1},{"tag":"Statistics - Methodology","type":1}],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T04:59:26Z","dateModified":"2023-11-29T04:59:26Z","uri":"http://zotero.org/users/11367251/items/WWZK6PWS","itemID":1615,"attachments":[{"key":"Y98A2MQU","version":1971,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1804.02502","accessDate":"2023-11-29T04:59:35Z","parentItem":"WWZK6PWS","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1804.html","tags":[],"relations":{},"dateAdded":"2023-11-29T04:59:35Z","dateModified":"2023-11-29T04:59:35Z","uri":"http://zotero.org/users/11367251/items/Y98A2MQU","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/Y98A2MQU/1804.html","defaultPath":"files/1618/1804.html"},{"key":"8I7GEEWB","version":1968,"itemType":"attachment","title":"gewersPrincipalComponentAnalysis2022.pdf","parentItem":"WWZK6PWS","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/gewersPrincipalComponentAnalysis2022.pdf","tags":[],"relations":{},"dateAdded":"2023-11-29T04:59:29Z","dateModified":"2023-11-29T04:59:29Z","uri":"http://zotero.org/users/11367251/items/8I7GEEWB","localPath":"/Users/reyvababtista/Projects/Papers/gewersPrincipalComponentAnalysis2022.pdf","defaultPath":"files/1617/gewersPrincipalComponentAnalysis2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400658,"version":0},"$loki":288},{"itemID":675,"item":{"key":"XDLFL4ZA","version":898,"itemType":"preprint","title":"Large Language Models as Optimizers","abstractNote":"Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.","date":"2023-09-06","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2309.03409","accessDate":"2023-09-13T01:59:44Z","extra":"arXiv:2309.03409 [cs]","repository":"arXiv","archiveID":"arXiv:2309.03409","creators":[{"firstName":"Chengrun","lastName":"Yang","creatorType":"author"},{"firstName":"Xuezhi","lastName":"Wang","creatorType":"author"},{"firstName":"Yifeng","lastName":"Lu","creatorType":"author"},{"firstName":"Hanxiao","lastName":"Liu","creatorType":"author"},{"firstName":"Quoc V.","lastName":"Le","creatorType":"author"},{"firstName":"Denny","lastName":"Zhou","creatorType":"author"},{"firstName":"Xinyun","lastName":"Chen","creatorType":"author"}],"tags":[{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Computation and Language","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-09-13T01:59:44Z","dateModified":"2023-09-13T01:59:44Z","uri":"http://zotero.org/users/11367251/items/XDLFL4ZA","itemID":675,"attachments":[{"key":"T8WBM8WT","version":901,"itemType":"attachment","title":"yangLargeLanguageModels2023.pdf","parentItem":"XDLFL4ZA","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/yangLargeLanguageModels2023.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_T8WBM8WT/2\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/3\">OPRO: LLM as the Optimizer</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/3\">Desirables of Optimization by LLMs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/4\">Meta-prompt Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/4\">Solution Generation</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/4\">Motivating Example: Mathematical Optimization</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/4\">Linear Regression</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/5\">Traveling Salesman Problem (TSP)</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/7\">Application: Prompt Optimization</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/7\">Problem Setup</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/8\">Meta-Prompt Design</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/8\">Prompt Optimization Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/8\">Evaluation Setup</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/9\">Main Results</a><ul style=\"list-style-type: none; padding-left:24px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/9\">GSM8K</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/11\">BBH</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/13\">Semantically similar instructions may achieve drastically different accuracies</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/13\">Transferability of found instructions</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/13\">Ablation Studies</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/18\">Related Work</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/19\">Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/24\">Some Failure Cases</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/25\">Prompting Formats for Scorer LLM</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/26\">Meta-Prompts</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/26\">Meta-Prompt for Math Optimization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/27\">Meta-Prompt for Prompt Optimization</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/28\">Prompt Optimization Curves on the Remaining BBH Tasks</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/29\">Prompt Optimization on BBH Tasks – Tabulated Accuracies and Found Instructions</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/29\">PaLM 2-L-IT as optimizer, optimization starting from the empty string</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/33\">gpt-3.5-turbo as optimizer, optimization starting from the empty string</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_T8WBM8WT/38\">PaLM 2-L as scorer, gpt-3.5-turbo as optimizer, optimization starting from ``Let's solve the problem.''</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-09-13T01:59:51Z","dateModified":"2023-09-13T01:59:52Z","uri":"http://zotero.org/users/11367251/items/T8WBM8WT","localPath":"/Users/reyvababtista/Projects/Papers/yangLargeLanguageModels2023.pdf","defaultPath":"files/676/yangLargeLanguageModels2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400659,"version":0},"$loki":289},{"itemID":1822,"item":{"key":"XGK6E4R3","version":2264,"itemType":"bookSection","title":"Chatbot in a Campus Environment: Design of LiSA, a Virtual Assistant to Help Students in Their University Life","date":"2018","language":"en","shortTitle":"Chatbot in a Campus Environment","libraryCatalog":"DOI.org (Crossref)","url":"https://link.springer.com/10.1007/978-3-319-91250-9_9","accessDate":"2024-01-19T03:03:27Z","extra":"Series Title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-319-91250-9_9","volume":"10903","place":"Cham","publisher":"Springer International Publishing","ISBN":"978-3-319-91249-3 978-3-319-91250-9","pages":"103-116","bookTitle":"Human-Computer Interaction. Interaction Technologies","creators":[{"firstName":"Masaaki","lastName":"Kurosu","creatorType":"editor"},{"firstName":"Massimiliano","lastName":"Dibitonto","creatorType":"author"},{"firstName":"Katarzyna","lastName":"Leszczynska","creatorType":"author"},{"firstName":"Federica","lastName":"Tazzi","creatorType":"author"},{"firstName":"Carlo M.","lastName":"Medaglia","creatorType":"author"}],"tags":[],"collections":["A49KF992"],"relations":{},"dateAdded":"2024-01-19T03:03:27Z","dateModified":"2024-01-19T03:03:27Z","uri":"http://zotero.org/users/11367251/items/XGK6E4R3","itemID":1822,"attachments":[{"key":"IPCRS6C8","version":2265,"itemType":"attachment","title":"dibitontoChatbotCampusEnvironment2018.pdf","parentItem":"XGK6E4R3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/dibitontoChatbotCampusEnvironment2018.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_IPCRS6C8/1\">Abstract</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/1\">1 Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/2\">2 Background</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/2\">2.1 Chatbots: A Short Overview</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/3\">2.2 Conversational Interfaces and User Experience</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/5\">2.3 Users’ Reactions During Human-Chatbot Conversations</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/6\">3 Designing a Chatbot for Universities</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/6\">3.1 State of the Art</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/7\">3.2 The Design Process for the LiSA Chatbot</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/8\">4 Method</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/8\">4.1 Survey Bot Design</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/9\">4.2 Participants</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/9\">5 Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/9\">5.1 User Needs and Habits in Information Request</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/11\">5.2 User Behaviour During the Conversation</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/12\">6 Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/13\">7 Future Works</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_IPCRS6C8/13\">References</a></li></ul>","tags":[],"relations":{},"dateAdded":"2024-01-19T03:03:50Z","dateModified":"2024-01-19T03:03:51Z","uri":"http://zotero.org/users/11367251/items/IPCRS6C8","localPath":"/Users/reyvababtista/Projects/Papers/dibitontoChatbotCampusEnvironment2018.pdf","defaultPath":"files/1824/dibitontoChatbotCampusEnvironment2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400660,"version":0},"$loki":290},{"itemID":1510,"item":{"key":"XLE8AYHW","version":1723,"itemType":"blogPost","title":"Understanding searches better than ever before","date":"2019/10/25","url":"https://blog.google/products/search/search-language-understanding-bert/","accessDate":"2023-11-09","blogTitle":"Understanding searches better than ever before","creators":[{"name":"Pandu Nayak","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-09T21:33:00Z","dateModified":"2023-11-09T21:33:38Z","uri":"http://zotero.org/users/11367251/items/XLE8AYHW","itemID":1510,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400660,"version":0},"$loki":291},{"itemID":1686,"item":{"key":"XLNUUET5","version":2146,"itemType":"journalArticle","title":"LLaMA: Open and Efficient Foundation Language Models","abstractNote":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.","date":"2023","shortTitle":"LLaMA","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/2302.13971","accessDate":"2023-12-06T20:31:04Z","rights":"Creative Commons Attribution 4.0 International","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.2302.13971","creators":[{"firstName":"Hugo","lastName":"Touvron","creatorType":"author"},{"firstName":"Thibaut","lastName":"Lavril","creatorType":"author"},{"firstName":"Gautier","lastName":"Izacard","creatorType":"author"},{"firstName":"Xavier","lastName":"Martinet","creatorType":"author"},{"firstName":"Marie-Anne","lastName":"Lachaux","creatorType":"author"},{"firstName":"Timothée","lastName":"Lacroix","creatorType":"author"},{"firstName":"Baptiste","lastName":"Rozière","creatorType":"author"},{"firstName":"Naman","lastName":"Goyal","creatorType":"author"},{"firstName":"Eric","lastName":"Hambro","creatorType":"author"},{"firstName":"Faisal","lastName":"Azhar","creatorType":"author"},{"firstName":"Aurelien","lastName":"Rodriguez","creatorType":"author"},{"firstName":"Armand","lastName":"Joulin","creatorType":"author"},{"firstName":"Edouard","lastName":"Grave","creatorType":"author"},{"firstName":"Guillaume","lastName":"Lample","creatorType":"author"}],"tags":[{"tag":"Computation and Language (cs.CL)","type":1},{"tag":"FOS: Computer and information sciences","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-12-06T20:31:04Z","dateModified":"2023-12-06T20:31:04Z","uri":"http://zotero.org/users/11367251/items/XLNUUET5","itemID":1686,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400660,"version":0},"$loki":292},{"itemID":7,"item":{"key":"XLQ2KDK4","version":199,"itemType":"conferencePaper","title":"Traffic Sign Classifiers Under Physical World Realistic Sticker Occlusions: A Cross Analysis Study","abstractNote":"Recent adversarial attacks with real world applications are capable of deceiving deep neural networks (DNN), which often appear as printed stickers applied to objects in physical world. Though achieving high success rate in lab tests and limited field tests, such attacks have not been tested on multiple DNN architectures with a standard setup to unveil the common robustness and weakness points of both the DNNs and the attacks. Furthermore, realistic looking stickers applied by normal people as acts of vandalism are not studied to discover their potential risks as well the risk of optimizing the location of such realistic stickers to achieve the maximum performance drop. In this paper, (a) we study the case of realistic looking sticker application effects on traffic sign detectors performance; (b) we use traffic sign image classification as our use case and train and attack 11 of the modern architectures for our analysis; (c) by considering different factors like brightness, blurriness and contrast of the train images in our sticker application procedure, we show that simple image processing techniques can help realistic looking stickers fit into their background to mimic real world tests; (d) by performing structured synthetic and real-world evaluations, we study the difference of various traffic sign classes in terms of their crucial distinctive features among the tested DNNs.","date":"2022-6-5","language":"en","shortTitle":"Traffic Sign Classifiers Under Physical World Realistic Sticker Occlusions","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9827143/","accessDate":"2023-03-20T14:20:05Z","place":"Aachen, Germany","publisher":"IEEE","ISBN":"978-1-66548-821-1","pages":"644-650","proceedingsTitle":"2022 IEEE Intelligent Vehicles Symposium (IV)","conferenceName":"2022 IEEE Intelligent Vehicles Symposium (IV)","DOI":"10.1109/IV51971.2022.9827143","creators":[{"firstName":"Yasin","lastName":"Bayzidi","creatorType":"author"},{"firstName":"Alen","lastName":"Smajic","creatorType":"author"},{"firstName":"Fabian","lastName":"Huger","creatorType":"author"},{"firstName":"Ruby","lastName":"Moritz","creatorType":"author"},{"firstName":"Serin","lastName":"Varghese","creatorType":"author"},{"firstName":"Peter","lastName":"Schlicht","creatorType":"author"},{"firstName":"Alois","lastName":"Knoll","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/63GCHLGA","http://zotero.org/users/11367251/items/U4TBG9FZ"]},"dateAdded":"2023-03-20T14:20:05Z","dateModified":"2023-03-22T17:03:33Z","uri":"http://zotero.org/users/11367251/items/XLQ2KDK4","itemID":7,"attachments":[{"key":"8U6QA2A8","version":240,"itemType":"attachment","title":"bayzidiTrafficSignClassifiers2022.pdf","parentItem":"XLQ2KDK4","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/bayzidiTrafficSignClassifiers2022.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:51:59Z","dateModified":"2023-03-22T18:52:21Z","uri":"http://zotero.org/users/11367251/items/8U6QA2A8","localPath":"/Users/reyvababtista/Projects/Papers/bayzidiTrafficSignClassifiers2022.pdf","defaultPath":"files/241/bayzidiTrafficSignClassifiers2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400661,"version":0},"$loki":293},{"itemID":1888,"item":{"key":"XV75RNZI","version":2439,"itemType":"journalArticle","title":"Wearables measuring electrodermal activity to assess perceived stress in care: a scoping review","abstractNote":"Abstract\n            \n              Background:\n              Chronic stress responses can lead to physical and behavioural health problems, often experienced and observed in the care of people with intellectual disabilities or people with dementia. Electrodermal activity (EDA) is a bio-signal for stress, which can be measured by wearables and thereby support stress management. However, the how, when and to what extent patients and healthcare providers can benefit is unclear. This study aims to create an overview of available wearables enabling the detection of perceived stress by using EDA.\n            \n            \n              Methods:\n              Following the PRISMA-SCR protocol for scoping reviews, four databases were included in the search of peer-reviewed studies published between 2012 and 2022, reporting detection of EDA in relation to self-reported stress or stress-related behaviours. Type of wearable, bodily location, research population, context, stressor type and the reported relationship between EDA and perceived stress were extracted.\n            \n            \n              Results:\n              Of the 74 included studies, the majority included healthy subjects in laboratory situations. Field studies and studies using machine learning (ML) to predict stress have increased in the last years. EDA is most often measured on the wrist, with offline data processing. Studies predicting perceived stress or stress-related behaviour using EDA features, reported accuracies between 42% and 100% with an average of 82.6%. Of these studies, the majority used ML.\n            \n            \n              Conclusion:\n              Wearable EDA sensors are promising in detecting perceived stress. Field studies with relevant populations in a health or care context are lacking. Future studies should focus on the application of EDA-measuring wearables in real-life situations to support stress management.","date":"2023-03-24","language":"en","shortTitle":"Wearables measuring electrodermal activity to assess perceived stress in care","libraryCatalog":"DOI.org (Crossref)","url":"https://www.cambridge.org/core/product/identifier/S0924270823000194/type/journal_article","accessDate":"2024-02-06T20:49:53Z","pages":"1-11","publicationTitle":"Acta Neuropsychiatrica","DOI":"10.1017/neu.2023.19","journalAbbreviation":"Acta Neuropsychiatr.","ISSN":"0924-2708, 1601-5215","creators":[{"firstName":"Agata","lastName":"Klimek","creatorType":"author"},{"firstName":"Ittay","lastName":"Mannheim","creatorType":"author"},{"firstName":"Gerard","lastName":"Schouten","creatorType":"author"},{"firstName":"Eveline J. M.","lastName":"Wouters","creatorType":"author"},{"firstName":"Manon W. H.","lastName":"Peeters","creatorType":"author"}],"tags":[],"collections":["TAUT9NML"],"relations":{},"dateAdded":"2024-02-06T20:49:53Z","dateModified":"2024-02-06T20:49:53Z","uri":"http://zotero.org/users/11367251/items/XV75RNZI","itemID":1888,"attachments":[{"key":"2DVNZ54P","version":2439,"itemType":"attachment","title":"klimekWearablesmeasuringelectrodermal2023.pdf","parentItem":"XV75RNZI","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/klimekWearablesmeasuringelectrodermal2023.pdf","tags":[],"relations":{},"dateAdded":"2024-02-06T20:49:55Z","dateModified":"2024-02-06T20:49:55Z","uri":"http://zotero.org/users/11367251/items/2DVNZ54P","localPath":"/Users/reyvababtista/Projects/Papers/klimekWearablesmeasuringelectrodermal2023.pdf","defaultPath":"files/1890/klimekWearablesmeasuringelectrodermal2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400662,"version":0},"$loki":294},{"itemID":796,"item":{"key":"XYCIF6C9","version":1055,"itemType":"preprint","title":"General-purpose, long-context autoregressive modeling with Perceiver AR","abstractNote":"Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this longrange structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms. When trained on images or music, Perceiver AR generates outputs with clear long-term coherence and structure. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 × 64 ImageNet images and PG-19 books.","date":"2022-06-14","language":"en","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2202.07765","accessDate":"2023-10-24T15:31:28Z","extra":"arXiv:2202.07765 [cs, eess]","repository":"arXiv","archiveID":"arXiv:2202.07765","creators":[{"firstName":"Curtis","lastName":"Hawthorne","creatorType":"author"},{"firstName":"Andrew","lastName":"Jaegle","creatorType":"author"},{"firstName":"Cătălina","lastName":"Cangea","creatorType":"author"},{"firstName":"Sebastian","lastName":"Borgeaud","creatorType":"author"},{"firstName":"Charlie","lastName":"Nash","creatorType":"author"},{"firstName":"Mateusz","lastName":"Malinowski","creatorType":"author"},{"firstName":"Sander","lastName":"Dieleman","creatorType":"author"},{"firstName":"Oriol","lastName":"Vinyals","creatorType":"author"},{"firstName":"Matthew","lastName":"Botvinick","creatorType":"author"},{"firstName":"Ian","lastName":"Simon","creatorType":"author"},{"firstName":"Hannah","lastName":"Sheahan","creatorType":"author"},{"firstName":"Neil","lastName":"Zeghidour","creatorType":"author"},{"firstName":"Jean-Baptiste","lastName":"Alayrac","creatorType":"author"},{"firstName":"João","lastName":"Carreira","creatorType":"author"},{"firstName":"Jesse","lastName":"Engel","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Machine Learning","type":1},{"tag":"Computer Science - Artificial Intelligence","type":1},{"tag":"Computer Science - Sound","type":1},{"tag":"Electrical Engineering and Systems Science - Audio and Speech Processing","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-10-24T15:31:28Z","dateModified":"2023-10-24T15:31:29Z","uri":"http://zotero.org/users/11367251/items/XYCIF6C9","itemID":796,"attachments":[{"key":"HPLIQX5U","version":1057,"itemType":"attachment","title":"hawthorneGeneralpurposelongcontextautoregressive2022.pdf","parentItem":"XYCIF6C9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/hawthorneGeneralpurposelongcontextautoregressive2022.pdf","tags":[],"relations":{},"dateAdded":"2023-10-24T15:31:38Z","dateModified":"2023-10-24T15:31:38Z","uri":"http://zotero.org/users/11367251/items/HPLIQX5U","localPath":"/Users/reyvababtista/Projects/Papers/hawthorneGeneralpurposelongcontextautoregressive2022.pdf","defaultPath":"files/798/hawthorneGeneralpurposelongcontextautoregressive2022.pdf"}],"notes":[{"key":"EN9QUWTQ","version":1055,"itemType":"note","parentItem":"XYCIF6C9","note":"Comment: ICML 2022","tags":[],"relations":{},"dateAdded":"2023-10-24T15:31:28Z","dateModified":"2023-10-24T15:31:28Z","uri":"http://zotero.org/users/11367251/items/EN9QUWTQ"}]},"meta":{"revision":0,"created":1709832400662,"version":0},"$loki":295},{"itemID":6,"item":{"key":"XZY7KPV7","version":199,"itemType":"conferencePaper","title":"Detection of Occluded Road Signs on Autonomous Driving Vehicles","abstractNote":"Autonomous driving vehicle relies heavily on its perception system to sense surrounding environments and make driving decisions. One important task on autonomous driving vehicles is to correctly recognize different trafﬁc signs. However, the trafﬁc signs in the wild can be in various conditions, e.g., occluded, deteriorated, or vandalized, and not all of them are recognizable. In this work, we propose a novel system that leverages the perception system on autonomous vehicle to identify occluded road signs in real time. Based on transfer learning, we propose the occluded sign classiﬁcation network (OSCN) that is able to achieve a precision of 96.34% on a real-world dataset.","date":"7/2019","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/8784797/","accessDate":"2023-03-20T14:20:03Z","place":"Shanghai, China","publisher":"IEEE","ISBN":"978-1-5386-9552-4","pages":"856-861","proceedingsTitle":"2019 IEEE International Conference on Multimedia and Expo (ICME)","conferenceName":"2019 IEEE International Conference on Multimedia and Expo (ICME)","DOI":"10.1109/ICME.2019.00152","creators":[{"firstName":"Jingda","lastName":"Guo","creatorType":"author"},{"firstName":"Xianwei","lastName":"Cheng","creatorType":"author"},{"firstName":"Qi","lastName":"Chen","creatorType":"author"},{"firstName":"Qing","lastName":"Yang","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/9A82LUEP","http://zotero.org/users/11367251/items/LFSY3IKK"]},"dateAdded":"2023-03-20T14:20:03Z","dateModified":"2023-03-22T17:03:43Z","uri":"http://zotero.org/users/11367251/items/XZY7KPV7","itemID":6,"attachments":[{"key":"NULKGSJ2","version":241,"itemType":"attachment","title":"guoDetectionOccludedRoad2019.pdf","parentItem":"XZY7KPV7","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/guoDetectionOccludedRoad2019.pdf","tags":[],"relations":{},"dateAdded":"2023-03-22T18:52:05Z","dateModified":"2023-03-22T18:52:23Z","uri":"http://zotero.org/users/11367251/items/NULKGSJ2","localPath":"/Users/reyvababtista/Projects/Papers/guoDetectionOccludedRoad2019.pdf","defaultPath":"files/252/guoDetectionOccludedRoad2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400663,"version":0},"$loki":296},{"itemID":356,"item":{"key":"Y9RTNJF2","version":392,"itemType":"journalArticle","title":"Faster Light Detection Algorithm of Traffic Signs Based on YOLOv5s-A2","abstractNote":"Trafﬁc sign recognition systems have been applied to advanced driving assistance and automatic driving systems to help drivers obtain important road information accurately. The current mainstream detection methods have high accuracy in this task, but the number of model parameters is large, and the detection speed is slow. Based on YOLOv5s as the basic framework, this paper proposes YOLOv5S-A2, which can improve the detection speed and reduce the model size at the cost of reducing the detection accuracy. Firstly, a data augmentation strategy is proposed by combining various operations to alleviate the problem of unbalanced class instances. Secondly, we proposed a path aggregation module for Feature Pyramid Network (FPN) to make new horizontal connections. It can enhance multi-scale feature representation capability and compensate for the loss of feature information. Thirdly, an attention detection head module is proposed to solve the aliasing effect in cross-scale fusion and enhance the representation of predictive features. Experiments on Tsinghua-Tencent 100K dataset (TT100K) show that our method can achieve more remarkable performance improvement and faster inference speed than other advanced technologies. Our method achieves 87.3% mean average precision (mAP), surpassing the original model’s 7.9%, and the frames per second (FPS) value is maintained at 87.7. To show generality, we tested it on the German Trafﬁc Sign Detection Benchmark (GTSDB) without tuning and obtained an average precision of 94.1%, and the FPS value is maintained at about 105.3. In addition, the number of YOLOv5s-A2 parameters is about 7.9 M.","date":"2023","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/9878318/","accessDate":"2023-04-10T22:15:49Z","volume":"11","pages":"19395-19404","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2022.3204818","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Xu","lastName":"Yuan","creatorType":"author"},{"firstName":"Alifu","lastName":"Kuerban","creatorType":"author"},{"firstName":"Yixiao","lastName":"Chen","creatorType":"author"},{"firstName":"Wenlong","lastName":"Lin","creatorType":"author"}],"tags":[],"collections":["A8KHNGZD"],"relations":{},"dateAdded":"2023-04-10T22:15:49Z","dateModified":"2023-04-10T22:15:50Z","uri":"http://zotero.org/users/11367251/items/Y9RTNJF2","itemID":356,"attachments":[{"key":"MMFMQA9H","version":394,"itemType":"attachment","title":"yuanFasterLightDetection2023.pdf","parentItem":"Y9RTNJF2","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/yuanFasterLightDetection2023.pdf","tags":[],"relations":{},"dateAdded":"2023-04-10T22:15:55Z","dateModified":"2023-04-10T22:15:55Z","uri":"http://zotero.org/users/11367251/items/MMFMQA9H","localPath":"/Users/reyvababtista/Projects/Papers/yuanFasterLightDetection2023.pdf","defaultPath":"files/357/yuanFasterLightDetection2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400663,"version":0},"$loki":297},{"itemID":1143,"item":{"key":"YGT4AHCB","version":1455,"itemType":"preprint","title":"TallyQA: Answering Complex Counting Questions","abstractNote":"Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identiﬁcation, reasoning, and more. To do this, we created TallyQA, the world’s largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efﬁciently used with high-resolution imagery. It yields stateof-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark.","date":"2018-10-31","language":"en","shortTitle":"TallyQA","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1810.12440","accessDate":"2023-11-08T16:01:44Z","extra":"arXiv:1810.12440 [cs]","repository":"arXiv","archiveID":"arXiv:1810.12440","creators":[{"firstName":"Manoj","lastName":"Acharya","creatorType":"author"},{"firstName":"Kushal","lastName":"Kafle","creatorType":"author"},{"firstName":"Christopher","lastName":"Kanan","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T16:01:44Z","dateModified":"2023-11-08T16:01:45Z","uri":"http://zotero.org/users/11367251/items/YGT4AHCB","itemID":1143,"attachments":[{"key":"4GIUUHDB","version":1463,"itemType":"attachment","title":"acharyaTallyQAAnsweringComplex2018.pdf","parentItem":"YGT4AHCB","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/acharyaTallyQAAnsweringComplex2018.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_4GIUUHDB/1\">Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/2\">Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/2\">VQA Datasets &amp;amp; Counting</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/2\">Algorithms for Open-Ended Counting</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/3\">The TallyQA Dataset</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/3\">Collecting New Complex Questions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/3\">Importing Questions from Other Datasets</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/4\">Classifying Simple and Complex Questions</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/4\">Dataset Splits &amp;amp; Statistics</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/4\">A New Framework for Complex Counting</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/5\">Training &amp;amp; Implementation Details</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/5\">Experiments</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/6\">Models Evaluated</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/6\">Results</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/7\">Performance Without Location Features</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/7\">Comparison with the Original RN</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/7\">Visualizing RCN</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/7\">Discussion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_4GIUUHDB/8\">Conclusions</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-08T16:03:54Z","dateModified":"2023-11-08T16:03:55Z","uri":"http://zotero.org/users/11367251/items/4GIUUHDB","localPath":"/Users/reyvababtista/Projects/Papers/acharyaTallyQAAnsweringComplex2018.pdf","defaultPath":"files/1150/acharyaTallyQAAnsweringComplex2018.pdf"}],"notes":[{"key":"WECCQ2GD","version":1455,"itemType":"note","parentItem":"YGT4AHCB","note":"Comment: To appear in AAAI 2019 ( To download the dataset please go to http://www.manojacharya.com/ )","tags":[],"relations":{},"dateAdded":"2023-11-08T16:01:44Z","dateModified":"2023-11-08T16:01:44Z","uri":"http://zotero.org/users/11367251/items/WECCQ2GD"}]},"meta":{"revision":0,"created":1709832400664,"version":0},"$loki":298},{"itemID":1675,"item":{"key":"YH783TNB","version":2109,"itemType":"blogPost","title":"Multiple Linear Regression","date":"2023","url":"https://corporatefinanceinstitute.com/resources/data-science/multiple-linear-regression/","accessDate":"2023-12-05","blogTitle":"Multiple Linear Regression","creators":[{"name":"Sebastian Taylor","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-05T20:58:22Z","dateModified":"2023-12-05T20:59:58Z","uri":"http://zotero.org/users/11367251/items/YH783TNB","itemID":1675,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400664,"version":0},"$loki":299},{"itemID":975,"item":{"key":"YT94QH7D","version":1358,"itemType":"conferencePaper","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning","abstractNote":"We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and ﬁltering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.","date":"2018","language":"en","shortTitle":"Conceptual Captions","libraryCatalog":"DOI.org (Crossref)","url":"http://aclweb.org/anthology/P18-1238","accessDate":"2023-11-07T22:37:56Z","place":"Melbourne, Australia","publisher":"Association for Computational Linguistics","pages":"2556-2565","proceedingsTitle":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","conferenceName":"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","DOI":"10.18653/v1/P18-1238","creators":[{"firstName":"Piyush","lastName":"Sharma","creatorType":"author"},{"firstName":"Nan","lastName":"Ding","creatorType":"author"},{"firstName":"Sebastian","lastName":"Goodman","creatorType":"author"},{"firstName":"Radu","lastName":"Soricut","creatorType":"author"}],"tags":[],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-07T22:37:56Z","dateModified":"2023-11-07T22:37:56Z","uri":"http://zotero.org/users/11367251/items/YT94QH7D","itemID":975,"attachments":[{"key":"J5I7BG4F","version":1358,"itemType":"attachment","title":"sharmaConceptualCaptionsCleaned2018.pdf","parentItem":"YT94QH7D","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/sharmaConceptualCaptionsCleaned2018.pdf","tags":[],"relations":{},"dateAdded":"2023-11-07T22:38:00Z","dateModified":"2023-11-07T22:38:00Z","uri":"http://zotero.org/users/11367251/items/J5I7BG4F","localPath":"/Users/reyvababtista/Projects/Papers/sharmaConceptualCaptionsCleaned2018.pdf","defaultPath":"files/976/sharmaConceptualCaptionsCleaned2018.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400665,"version":0},"$loki":300},{"itemID":1666,"item":{"key":"YTCJMGSW","version":2088,"itemType":"journalArticle","title":"Backpropagation Applied to Handwritten Zip Code Recognition","abstractNote":"The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.","date":"12/1989","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://direct.mit.edu/neco/article/1/4/541-551/5515","accessDate":"2023-12-04T04:04:29Z","volume":"1","pages":"541-551","publicationTitle":"Neural Computation","DOI":"10.1162/neco.1989.1.4.541","issue":"4","journalAbbreviation":"Neural Computation","ISSN":"0899-7667, 1530-888X","creators":[{"firstName":"Y.","lastName":"LeCun","creatorType":"author"},{"firstName":"B.","lastName":"Boser","creatorType":"author"},{"firstName":"J. S.","lastName":"Denker","creatorType":"author"},{"firstName":"D.","lastName":"Henderson","creatorType":"author"},{"firstName":"R. E.","lastName":"Howard","creatorType":"author"},{"firstName":"W.","lastName":"Hubbard","creatorType":"author"},{"firstName":"L. D.","lastName":"Jackel","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T04:04:29Z","dateModified":"2023-12-04T04:04:29Z","uri":"http://zotero.org/users/11367251/items/YTCJMGSW","itemID":1666,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400665,"version":0},"$loki":301},{"itemID":533,"item":{"key":"YTR2YLT9","version":705,"itemType":"preprint","title":"Toolformer: Language Models Can Teach Themselves to Use Tools","abstractNote":"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacriﬁcing its core language modeling abilities.","date":"2023-02-09","language":"en","shortTitle":"Toolformer","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2302.04761","accessDate":"2023-06-29T19:00:04Z","extra":"arXiv:2302.04761 [cs]","repository":"arXiv","archiveID":"arXiv:2302.04761","creators":[{"firstName":"Timo","lastName":"Schick","creatorType":"author"},{"firstName":"Jane","lastName":"Dwivedi-Yu","creatorType":"author"},{"firstName":"Roberto","lastName":"Dessì","creatorType":"author"},{"firstName":"Roberta","lastName":"Raileanu","creatorType":"author"},{"firstName":"Maria","lastName":"Lomeli","creatorType":"author"},{"firstName":"Luke","lastName":"Zettlemoyer","creatorType":"author"},{"firstName":"Nicola","lastName":"Cancedda","creatorType":"author"},{"firstName":"Thomas","lastName":"Scialom","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-29T19:00:04Z","dateModified":"2023-06-29T19:00:04Z","uri":"http://zotero.org/users/11367251/items/YTR2YLT9","itemID":533,"attachments":[{"key":"MYVZADGW","version":707,"itemType":"attachment","title":"schickToolformerLanguageModels2023.pdf","parentItem":"YTR2YLT9","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/schickToolformerLanguageModels2023.pdf","tags":[],"relations":{},"dateAdded":"2023-06-29T19:00:11Z","dateModified":"2023-06-29T19:00:11Z","uri":"http://zotero.org/users/11367251/items/MYVZADGW","localPath":"/Users/reyvababtista/Projects/Papers/schickToolformerLanguageModels2023.pdf","defaultPath":"files/534/schickToolformerLanguageModels2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400666,"version":0},"$loki":302},{"itemID":548,"item":{"key":"YV3ENAUF","version":731,"itemType":"webpage","title":"State of GPT","abstractNote":"Learn about the training pipeline of GPT assistants like ChatGPT, from tokenization to pretraining, supervised finetuning, and Reinforcement Learning from Human Feedback (RLHF). Dive deeper into practical techniques and mental models for the effective use of these models, including prompting strategies, finetuning, the rapidly growing ecosystem of tools, and their future extensions.","date":"05/23/2023","url":"https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2","accessDate":"2023-06-29","websiteTitle":"State of GPT","creators":[{"name":"Andrej Karpathy","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-06-30T14:22:58Z","dateModified":"2023-06-30T14:25:39Z","uri":"http://zotero.org/users/11367251/items/YV3ENAUF","itemID":548,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400666,"version":0},"$loki":303},{"itemID":77,"item":{"key":"Z2NPUTJ3","version":202,"itemType":"conferencePaper","title":"Reproducible Workflows for Exploring and Modeling EMA Data","abstractNote":"Improper use of substances like cannabis may lead to physical, emotional, economic, and social problems. Therefore, it is significant to elucidate the inter-individual and intraindividual influences along with contextual influences that predict the use of cannabis. TigerAware is a mobile survey data collection platform that holds unique promise to advance research in addiction and substance use. This paper presents a novel method to support Ecological Momentary Assessment (EMA) studies. We propose to extract useful information from TigerAware survey data using data mining and machine learning methods, and structure customizable survey analyses into reproducible workflows. Through our analysis pipeline for EMA, researchers are able to discover meaningful information from survey data with minimal duplication of effort and improve the efficiency and rigor of the process.","date":"12/2022","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/10061712/","accessDate":"2023-03-22T15:34:13Z","place":"Atlanta, GA, USA","publisher":"IEEE","ISBN":"978-1-66547-300-2","pages":"117-124","proceedingsTitle":"2022 IEEE 8th International Conference on Collaboration and Internet Computing (CIC)","conferenceName":"2022 IEEE 8th International Conference on Collaboration and Internet Computing (CIC)","DOI":"10.1109/CIC56439.2022.00026","creators":[{"firstName":"Ching-Yun","lastName":"Yu","creatorType":"author"},{"firstName":"Yi","lastName":"Shang","creatorType":"author"},{"firstName":"Timothy","lastName":"Trull","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{"dc:replaces":["http://zotero.org/users/11367251/items/T62NBFQ2"]},"dateAdded":"2023-03-22T15:34:14Z","dateModified":"2023-03-22T17:03:30Z","uri":"http://zotero.org/users/11367251/items/Z2NPUTJ3","itemID":77,"attachments":[{"key":"ZUL9882T","version":240,"itemType":"attachment","title":"yuReproducibleWorkflowsExploring2022.pdf","parentItem":"Z2NPUTJ3","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/yuReproducibleWorkflowsExploring2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_ZUL9882T/1\">I. Introduction</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZUL9882T/1\">II. Related Work</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/1\">A. TigerAware</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/1\">B. Reproducible Analysis Pipeline for Data Streams</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/2\">C. Sample Dataset</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZUL9882T/2\">III. Methods and Implementation</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/2\">A. Compute Clusters of Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/2\">B. Identify Outliers among Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/2\">C. Identify Strongly Correlated Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/3\">D. Build Predictive Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/4\">E. Customizable Data Analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">IV. Preliminary Results</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">1) Cannabis Use survey data, which contains fifty-one participants with one hundred and twenty-two questions each.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">2) Random Prompt survey data, which contains fifty-three participants with one hundred and twenty-one questions each.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">3) Morning Report survey data, which contains fifty-three participants with fifty-four questions each.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">4) Cannabis Use and Random Prompt survey data, which contains fifty-three participants with forty-three specific questions each.</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">A. Compute Clusters of Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/5\">B. Identify Outliers among Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/6\">C. Identify Strongly Correlated Participants</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/6\">D. Build Predictive Models</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/7\">E. Customizable Data Analysis</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_ZUL9882T/8\">V. Summary</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_ZUL9882T/8\">References</a></li></ul></li></ul>","tags":[],"relations":{},"dateAdded":"2023-03-22T18:51:57Z","dateModified":"2023-03-22T18:52:21Z","uri":"http://zotero.org/users/11367251/items/ZUL9882T","localPath":"/Users/reyvababtista/Projects/Papers/yuReproducibleWorkflowsExploring2022.pdf","defaultPath":"files/239/yuReproducibleWorkflowsExploring2022.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400667,"version":0},"$loki":304},{"itemID":1135,"item":{"key":"Z3RSHR9H","version":1445,"itemType":"preprint","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks","abstractNote":"A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEIT-3, which achieves state-of-the-art transfer performance on both vision and visionlanguage tasks. Speciﬁcally, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-speciﬁc encoding. Based on the shared backbone, we perform masked “language” modeling on images (Imglish), texts (English), and image-text pairs (“parallel sentences”) in a uniﬁed manner. Experimental results show that BEIT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classiﬁcation (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).","date":"2022-08-30","language":"en","shortTitle":"Image as a Foreign Language","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/2208.10442","accessDate":"2023-11-08T15:40:53Z","extra":"arXiv:2208.10442 [cs]","repository":"arXiv","archiveID":"arXiv:2208.10442","creators":[{"firstName":"Wenhui","lastName":"Wang","creatorType":"author"},{"firstName":"Hangbo","lastName":"Bao","creatorType":"author"},{"firstName":"Li","lastName":"Dong","creatorType":"author"},{"firstName":"Johan","lastName":"Bjorck","creatorType":"author"},{"firstName":"Zhiliang","lastName":"Peng","creatorType":"author"},{"firstName":"Qiang","lastName":"Liu","creatorType":"author"},{"firstName":"Kriti","lastName":"Aggarwal","creatorType":"author"},{"firstName":"Owais Khan","lastName":"Mohammed","creatorType":"author"},{"firstName":"Saksham","lastName":"Singhal","creatorType":"author"},{"firstName":"Subhojit","lastName":"Som","creatorType":"author"},{"firstName":"Furu","lastName":"Wei","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1},{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T15:40:53Z","dateModified":"2023-11-08T15:40:53Z","uri":"http://zotero.org/users/11367251/items/Z3RSHR9H","itemID":1135,"attachments":[{"key":"K2VT9FSN","version":1448,"itemType":"attachment","title":"wangImageForeignLanguage2022.pdf","parentItem":"Z3RSHR9H","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/wangImageForeignLanguage2022.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_K2VT9FSN/2\">1 Introduction: The Big Convergence</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/3\">2 BEiT-3: A General-Purpose Multimodal Foundation Model</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/3\">2.1 Backbone Network: Multiway Transformers</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/4\">2.2 Pretraining Task: Masked Data Modeling</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/5\">2.3 Scaling Up: BEiT-3 Pretraining</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/5\">3 Experiments on Vision and Vision-Language Tasks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/6\">3.1 Vision-Language Downstream Tasks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/8\">3.2 Vision Downstream Tasks</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/9\">4 Conclusion</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/16\">A Effects of Intermediate Finetuning for Retrieval</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/16\">B Hyperparameters Used for Pretraining</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_K2VT9FSN/17\">C Hyperparameters Used for Finetuning</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-11-08T15:41:00Z","dateModified":"2023-11-08T15:41:00Z","uri":"http://zotero.org/users/11367251/items/K2VT9FSN","localPath":"/Users/reyvababtista/Projects/Papers/wangImageForeignLanguage2022.pdf","defaultPath":"files/1137/wangImageForeignLanguage2022.pdf"}],"notes":[{"key":"TGS4BG3W","version":1445,"itemType":"note","parentItem":"Z3RSHR9H","note":"Comment: 18 pages","tags":[],"relations":{},"dateAdded":"2023-11-08T15:40:53Z","dateModified":"2023-11-08T15:40:53Z","uri":"http://zotero.org/users/11367251/items/TGS4BG3W"}]},"meta":{"revision":0,"created":1709832400668,"version":0},"$loki":305},{"itemID":1751,"item":{"key":"Z3ZQ33CV","version":2202,"itemType":"journalArticle","title":"An Overview of Multi-Task Learning in Deep Neural Networks","abstractNote":"Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.","date":"2017","libraryCatalog":"DOI.org (Datacite)","url":"https://arxiv.org/abs/1706.05098","accessDate":"2023-12-10T19:46:51Z","rights":"arXiv.org perpetual, non-exclusive license","extra":"Publisher: arXiv\nVersion Number: 1","DOI":"10.48550/ARXIV.1706.05098","creators":[{"firstName":"Sebastian","lastName":"Ruder","creatorType":"author"}],"tags":[{"tag":"Artificial Intelligence (cs.AI)","type":1},{"tag":"FOS: Computer and information sciences","type":1},{"tag":"Machine Learning (cs.LG)","type":1},{"tag":"Machine Learning (stat.ML)","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-12-10T19:46:51Z","dateModified":"2023-12-10T19:46:51Z","uri":"http://zotero.org/users/11367251/items/Z3ZQ33CV","itemID":1751,"attachments":[{"key":"93DPVQMT","version":2203,"itemType":"attachment","title":"ruderOverviewMultiTaskLearning2017.pdf","parentItem":"Z3ZQ33CV","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/ruderOverviewMultiTaskLearning2017.pdf","note":"<p xmlns=\"http://www.w3.org/1999/xhtml\" id=\"title\"><strong>Contents</strong></p><ul xmlns=\"http://www.w3.org/1999/xhtml\" style=\"list-style-type: none; padding-left:0px\" id=\"toc\"><li><a href=\"zotero://open-pdf/0_93DPVQMT/1\">1 Introduction</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/2\">2 Motivation</a></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_93DPVQMT/2\">3 Two MTL methods for Deep Learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/3\">3.1 Hard parameter sharing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/3\">3.2 Soft parameter sharing</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_93DPVQMT/3\">4 Why does MTL work?</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/3\">4.1 Implicit data augmentation</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/3\">4.2 Attention focusing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/4\">4.3 Eavesdropping</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/4\">4.4 Representation bias</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/4\">4.5 Regularization</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_93DPVQMT/4\">5 MTL in non-neural models</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/4\">5.1 Block-sparse regularization</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/5\">5.2 Learning task relationships</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_93DPVQMT/6\">6 Recent work on MTL for Deep Learning</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/7\">6.1 Deep Relationship Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/7\">6.2 Fully-Adaptive Feature Sharing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/7\">6.3 Cross-stitch Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/7\">6.4 Low supervision</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/8\">6.5 A Joint Many-Task Model</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/8\">6.6 Weighting losses with uncertainty</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/9\">6.7 Tensor factorisation for MTL</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/9\">6.8 Sluice Networks</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/9\">6.9 What should I share in my model?</a></li></ul></li><li style=\"padding-top:8px\"><a href=\"zotero://open-pdf/0_93DPVQMT/9\">7 Auxiliary tasks</a><ul style=\"list-style-type: none; padding-left:12px\"><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/10\">7.1 Related task</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/10\">7.2 Adversarial</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/10\">7.3 Hints</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/10\">7.4 Focusing attention</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/10\">7.5 Quantization smoothing</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/10\">7.6 Predicting inputs</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/11\">7.7 Using the future to predict the present</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/11\">7.8 Representation learning</a></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/11\">7.9 What auxiliary tasks are helpful?</a></li></ul></li><li style=\"padding-top:4px\"><a href=\"zotero://open-pdf/0_93DPVQMT/11\">8 Conclusion</a></li></ul>","tags":[],"relations":{},"dateAdded":"2023-12-10T19:47:56Z","dateModified":"2023-12-10T19:47:57Z","uri":"http://zotero.org/users/11367251/items/93DPVQMT","localPath":"/Users/reyvababtista/Projects/Papers/ruderOverviewMultiTaskLearning2017.pdf","defaultPath":"files/1754/ruderOverviewMultiTaskLearning2017.pdf"}],"notes":[{"key":"AMZ2IVLP","version":2202,"itemType":"note","parentItem":"Z3ZQ33CV","note":"<h2>Other</h2>\n14 pages, 8 figures","tags":[],"relations":{},"dateAdded":"2023-12-10T19:46:51Z","dateModified":"2023-12-10T19:46:51Z","uri":"http://zotero.org/users/11367251/items/AMZ2IVLP"}]},"meta":{"revision":0,"created":1709832400668,"version":0},"$loki":306},{"itemID":428,"item":{"key":"Z48G9F5L","version":524,"itemType":"webpage","title":"PaddleOCR","date":"2023-05-04","url":"https://github.com/PaddlePaddle/PaddleOCR","accessDate":"2023-05-04","websiteTitle":"PaddleOCR","creators":[{"name":"PaddlePaddle","creatorType":"author"}],"tags":[],"collections":["BCUNULLU"],"relations":{},"dateAdded":"2023-05-05T03:43:56Z","dateModified":"2023-05-05T03:44:30Z","uri":"http://zotero.org/users/11367251/items/Z48G9F5L","itemID":428,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400668,"version":0},"$loki":307},{"itemID":930,"item":{"key":"Z5AKLXUQ","version":1223,"itemType":"blogPost","title":"Understanding page experience in Google Search results","date":"05/23/2023","url":"https://developers.google.com/search/docs/appearance/page-experience?safe=active","accessDate":"2023-11-05","blogTitle":"Understanding page experience in Google Search results","creators":[{"name":"Google Search Central","creatorType":"author"}],"tags":[],"collections":["6X9VU85H"],"relations":{},"dateAdded":"2023-11-06T04:05:16Z","dateModified":"2023-11-06T04:06:06Z","uri":"http://zotero.org/users/11367251/items/Z5AKLXUQ","itemID":930,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400669,"version":0},"$loki":308},{"itemID":1610,"item":{"key":"ZBAF5BPI","version":2189,"itemType":"webpage","title":"RapidMiner","date":"11/11/2023","url":"https://rapidminer.com/","accessDate":"2023-11-11","websiteTitle":"RapidMiner","creators":[{"name":"RapidMiner","creatorType":"author"}],"tags":[],"collections":["DYJCDLCC"],"relations":{},"dateAdded":"2023-11-29T04:10:38Z","dateModified":"2023-12-08T17:19:04Z","uri":"http://zotero.org/users/11367251/items/ZBAF5BPI","itemID":1610,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400669,"version":0},"$loki":309},{"itemID":1478,"item":{"key":"ZBFBQK6U","version":1629,"itemType":"preprint","title":"A Short Note about Kinetics-600","abstractNote":"We describe an extension of the DeepMind Kinetics human action dataset from 400 classes, each with at least 400 video clips, to 600 classes, each with at least 600 video clips. In order to scale up the dataset we changed the data collection process so it uses multiple queries per class, with some of them in a language other than english -- portuguese. This paper details the changes between the two versions of the dataset and includes a comprehensive set of statistics of the new version as well as baseline results using the I3D neural network architecture. The paper is a companion to the release of the ground truth labels for the public test set.","date":"2018-08-03","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1808.01340","accessDate":"2023-11-08T23:03:06Z","extra":"arXiv:1808.01340 [cs]","repository":"arXiv","archiveID":"arXiv:1808.01340","creators":[{"firstName":"Joao","lastName":"Carreira","creatorType":"author"},{"firstName":"Eric","lastName":"Noland","creatorType":"author"},{"firstName":"Andras","lastName":"Banki-Horvath","creatorType":"author"},{"firstName":"Chloe","lastName":"Hillier","creatorType":"author"},{"firstName":"Andrew","lastName":"Zisserman","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computer Vision and Pattern Recognition","type":1}],"collections":["AXTDM6XB"],"relations":{},"dateAdded":"2023-11-08T23:03:06Z","dateModified":"2023-11-08T23:03:06Z","uri":"http://zotero.org/users/11367251/items/ZBFBQK6U","itemID":1478,"attachments":[{"key":"3WRBP297","version":1632,"itemType":"attachment","title":"arXiv.org Snapshot","url":"https://arxiv.org/abs/1808.01340","accessDate":"2023-11-08T23:03:14Z","parentItem":"ZBFBQK6U","linkMode":"imported_url","contentType":"text/html","charset":"utf-8","filename":"1808.html","tags":[],"relations":{},"dateAdded":"2023-11-08T23:03:14Z","dateModified":"2023-11-08T23:03:14Z","uri":"http://zotero.org/users/11367251/items/3WRBP297","localPath":"/Users/reyvababtista/Projects/papers/Zotero/storage/3WRBP297/1808.html","defaultPath":"files/1482/1808.html"},{"key":"37ESSIIX","version":1629,"itemType":"attachment","title":"carreiraShortNoteKinetics6002018.pdf","parentItem":"ZBFBQK6U","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/carreiraShortNoteKinetics6002018.pdf","tags":[],"relations":{},"dateAdded":"2023-11-08T23:03:08Z","dateModified":"2023-11-08T23:03:08Z","uri":"http://zotero.org/users/11367251/items/37ESSIIX","localPath":"/Users/reyvababtista/Projects/Papers/carreiraShortNoteKinetics6002018.pdf","defaultPath":"files/1481/carreiraShortNoteKinetics6002018.pdf"}],"notes":[{"key":"ICWC2UWA","version":1629,"itemType":"note","parentItem":"ZBFBQK6U","note":"Comment: Companion to public release of kinetics-600 test set labels","tags":[],"relations":{},"dateAdded":"2023-11-08T23:03:06Z","dateModified":"2023-11-08T23:03:06Z","uri":"http://zotero.org/users/11367251/items/ICWC2UWA"}]},"meta":{"revision":0,"created":1709832400670,"version":0},"$loki":310},{"itemID":586,"item":{"key":"ZN4YFHWC","version":1134,"itemType":"preprint","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","abstractNote":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.","date":"2019-05-24","language":"en","shortTitle":"BERT","libraryCatalog":"arXiv.org","url":"http://arxiv.org/abs/1810.04805","accessDate":"2023-09-05T13:38:32Z","extra":"arXiv:1810.04805 [cs]","repository":"arXiv","archiveID":"arXiv:1810.04805","creators":[{"firstName":"Jacob","lastName":"Devlin","creatorType":"author"},{"firstName":"Ming-Wei","lastName":"Chang","creatorType":"author"},{"firstName":"Kenton","lastName":"Lee","creatorType":"author"},{"firstName":"Kristina","lastName":"Toutanova","creatorType":"author"}],"tags":[{"tag":"Computer Science - Computation and Language","type":1}],"collections":["AXTDM6XB","6X9VU85H"],"relations":{},"dateAdded":"2023-09-05T13:38:32Z","dateModified":"2023-09-05T13:38:33Z","uri":"http://zotero.org/users/11367251/items/ZN4YFHWC","itemID":586,"attachments":[{"key":"DUWTFTQK","version":817,"itemType":"attachment","title":"devlinBERTPretrainingDeep2019.pdf","parentItem":"ZN4YFHWC","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/devlinBERTPretrainingDeep2019.pdf","tags":[],"relations":{},"dateAdded":"2023-09-05T13:47:27Z","dateModified":"2023-09-05T13:47:27Z","uri":"http://zotero.org/users/11367251/items/DUWTFTQK","localPath":"/Users/reyvababtista/Projects/Papers/devlinBERTPretrainingDeep2019.pdf","defaultPath":"files/616/devlinBERTPretrainingDeep2019.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400671,"version":0},"$loki":311},{"itemID":1660,"item":{"key":"ZQI4BHE7","version":2082,"itemType":"journalArticle","title":"Stacked generalization","date":"1/1992","language":"en","libraryCatalog":"DOI.org (Crossref)","url":"https://linkinghub.elsevier.com/retrieve/pii/S0893608005800231","accessDate":"2023-12-04T02:14:40Z","volume":"5","pages":"241-259","publicationTitle":"Neural Networks","DOI":"10.1016/S0893-6080(05)80023-1","issue":"2","journalAbbreviation":"Neural Networks","ISSN":"08936080","creators":[{"firstName":"David H.","lastName":"Wolpert","creatorType":"author"}],"tags":[],"collections":["DLE3Q4P4"],"relations":{},"dateAdded":"2023-12-04T02:14:40Z","dateModified":"2023-12-04T02:14:40Z","uri":"http://zotero.org/users/11367251/items/ZQI4BHE7","itemID":1660,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400671,"version":0},"$loki":312},{"itemID":944,"item":{"key":"ZRVFRX6G","version":1298,"itemType":"book","title":"Inventing the Cloud Century","creators":[{"name":"Marcus Oppitz","creatorType":"author"}],"tags":[],"collections":[],"relations":{},"dateAdded":"2023-11-07T02:05:10Z","dateModified":"2023-11-07T02:06:04Z","uri":"http://zotero.org/users/11367251/items/ZRVFRX6G","itemID":944,"attachments":[],"notes":[]},"meta":{"revision":0,"created":1709832400672,"version":0},"$loki":313},{"itemID":563,"item":{"key":"ZYZ8E2XP","version":754,"itemType":"journalArticle","title":"Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models","abstractNote":"The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.","date":"2023","language":"en","shortTitle":"Chat2VIS","libraryCatalog":"DOI.org (Crossref)","url":"https://ieeexplore.ieee.org/document/10121440/","accessDate":"2023-08-30T13:31:06Z","volume":"11","pages":"45181-45193","publicationTitle":"IEEE Access","DOI":"10.1109/ACCESS.2023.3274199","journalAbbreviation":"IEEE Access","ISSN":"2169-3536","creators":[{"firstName":"Paula","lastName":"Maddigan","creatorType":"author"},{"firstName":"Teo","lastName":"Susnjak","creatorType":"author"}],"tags":[],"collections":["L7CVPXN4"],"relations":{},"dateAdded":"2023-08-30T13:31:06Z","dateModified":"2023-08-30T13:31:06Z","uri":"http://zotero.org/users/11367251/items/ZYZ8E2XP","itemID":563,"attachments":[{"key":"9929E2GG","version":755,"itemType":"attachment","title":"maddiganChat2VISGeneratingData2023.pdf","parentItem":"ZYZ8E2XP","linkMode":"linked_file","contentType":"application/pdf","charset":"","path":"/Users/reyvababtista/Projects/Papers/maddiganChat2VISGeneratingData2023.pdf","tags":[],"relations":{},"dateAdded":"2023-08-30T13:31:10Z","dateModified":"2023-08-30T13:31:10Z","uri":"http://zotero.org/users/11367251/items/9929E2GG","localPath":"/Users/reyvababtista/Projects/Papers/maddiganChat2VISGeneratingData2023.pdf","defaultPath":"files/564/maddiganChat2VISGeneratingData2023.pdf"}],"notes":[]},"meta":{"revision":0,"created":1709832400672,"version":0},"$loki":314}],"idIndex":null,"binaryIndices":{"itemID":{"name":"itemID","dirty":false,"values":[295,292,234,277,117,194,181,284,141,86,59,38,3,70,152,173,303,160,267,2,256,61,82,119,226,144,132,183,149,42,197,116,128,91,206,200,89,118,101,162,240,296,75,136,176,198,107,26,120,126,64,88,157,50,71,268,104,99,187,306,210,69,254,28,191,115,46,282,156,178,123,79,265,235,131,251,225,165,227,301,45,84,166,279,302,112,65,266,8,313,221,196,100,19,275,242,94,4,310,285,186,271,145,36,113,13,211,125,31,170,58,205,288,57,264,85,16,32,189,253,93,146,87,97,193,281,294,27,255,155,246,78,236,161,47,43,67,270,184,51,110,127,307,179,12,202,25,139,66,245,98,20,230,96,0,312,252,138,248,80,102,231,151,24,220,217,175,278,299,215,185,286,213,261,182,122,40,5,74,262,44,41,304,133,297,73,135,190,177,218,9,142,18,216,130,54,108,180,76,247,134,17,224,22,219,169,207,48,11,83,56,111,37,309,114,259,258,249,223,163,33,188,35,29,49,150,68,290,272,72,308,232,208,287,204,121,263,209,77,276,7,95,30,243,63,168,137,14,109,199,1,239,273,311,124,241,129,201,159,300,21,269,158,298,153,105,257,147,90,53,203,174,140,291,148,10,106,222,195,34,103,233,274,214,164,237,171,238,305,283,55,52,81,172,6,250,23,228,289,167,212,143,192,92,260,244,39,60,15,154,229,293,62,280]}},"constraints":null,"uniqueNames":[],"transforms":{"Better BibTeX metadata":[{"type":"Better BibTeX metadata","value":{"Zotero":"6.0.35"}}]},"objType":"itemToExportFormat","dirty":true,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":true,"cloneMethod":"parse-stringify","asyncListeners":false,"disableMeta":false,"disableChangesApi":true,"disableDeltaChangesApi":true,"autoupdate":false,"serializableIndices":true,"disableFreeze":true,"ttl":null,"maxId":314,"DynamicViews":[],"events":{"insert":[],"update":[],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[],"dirtyIds":[]}