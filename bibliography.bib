@inproceedings{ahmedFewshottrainingLLMs2022,
  title = {Few-Shot Training {{LLMs}} for Project-Specific Code-Summarization},
  booktitle = {Proceedings of the 37th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Ahmed, Toufique and Devanbu, Premkumar},
  year = {2022},
  month = oct,
  pages = {1--5},
  publisher = {{ACM}},
  address = {{Rochester MI USA}},
  doi = {10.1145/3551349.3559555},
  urldate = {2023-06-30},
  abstract = {Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Fewshotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pretrained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for codesummarization, leveraging project-specific training.},
  isbn = {978-1-4503-9475-8},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/ahmedFewshottrainingLLMs2022.pdf}
}

@misc{andrejkarpathyStateGPT2023,
  title = {State of {{GPT}}},
  author = {{Andrej Karpathy}},
  year = {2023},
  month = may,
  journal = {State of GPT},
  urldate = {2023-06-29},
  abstract = {Learn about the training pipeline of GPT assistants like ChatGPT, from tokenization to pretraining, supervised finetuning, and Reinforcement Learning from Human Feedback (RLHF). Dive deeper into practical techniques and mental models for the effective use of these models, including prompting strategies, finetuning, the rapidly growing ecosystem of tools, and their future extensions.},
  howpublished = {https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2}
}

@article{arulkumaranBriefSurveyDeep2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Process. Mag.},
  volume = {34},
  number = {6},
  eprint = {1708.05866},
  primaryclass = {cs, stat},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  urldate = {2023-09-05},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/arulkumaranBriefSurveyDeep2017.pdf}
}

@article{bangquanRealTimeEmbeddedTraffic2019,
  title = {Real-{{Time Embedded Traffic Sign Recognition Using Efficient Convolutional Neural Network}}},
  author = {Bangquan, Xie and Xiao Xiong, Weng},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {53330--53346},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2912311},
  urldate = {2023-03-20},
  abstract = {Traffic sign recognition(TSR) based on deep learning is rapidly developing. Specifically, TSR contains two technologies, namely, traffic sign classification (TSC) and traffic sign detection (TSD). However, the challenge of TSR is to ensure its efficiency, which means adequate accuracy, generalization, and speed in real-time by a computationally limited platform. In this paper, we will introduce a new efficient TSC network called ENet (efficient network) and a TSD network called EmdNet (efficient network using multiscale operation and depthwise separable convolution). We used data mining and multiscale operation to improve the accuracy and generalization ability and used depthwise separable convolution (DSC) to improve the speed. The resulting ENet possesses 0.9 M parameters (1/15 the parameters of the start-of-the-art method) while still achieving an accuracy of 98.6 \% on the German Traffic Sign Recognition benchmark (GTSRB). In addition, we design EmdNet' s backbone network according to the principles of ENet. The EmdNet with the SDD Framework possesses only 6.3 M parameters, which is similar to MobileNet's scale.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bangquanRealTimeEmbeddedTraffic2019.pdf}
}

@misc{bankAutoencoders2021,
  title = {Autoencoders},
  author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
  year = {2021},
  month = apr,
  number = {arXiv:2003.05991},
  eprint = {2003.05991},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/bankAutoencoders2021.pdf}
}

@inproceedings{bayzidiTrafficSignClassifiers2022,
  title = {Traffic {{Sign Classifiers Under Physical World Realistic Sticker Occlusions}}: {{A Cross Analysis Study}}},
  shorttitle = {Traffic {{Sign Classifiers Under Physical World Realistic Sticker Occlusions}}},
  booktitle = {2022 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Bayzidi, Yasin and Smajic, Alen and Huger, Fabian and Moritz, Ruby and Varghese, Serin and Schlicht, Peter and Knoll, Alois},
  year = {2022},
  month = jun,
  pages = {644--650},
  publisher = {{IEEE}},
  address = {{Aachen, Germany}},
  doi = {10.1109/IV51971.2022.9827143},
  urldate = {2023-03-20},
  abstract = {Recent adversarial attacks with real world applications are capable of deceiving deep neural networks (DNN), which often appear as printed stickers applied to objects in physical world. Though achieving high success rate in lab tests and limited field tests, such attacks have not been tested on multiple DNN architectures with a standard setup to unveil the common robustness and weakness points of both the DNNs and the attacks. Furthermore, realistic looking stickers applied by normal people as acts of vandalism are not studied to discover their potential risks as well the risk of optimizing the location of such realistic stickers to achieve the maximum performance drop. In this paper, (a) we study the case of realistic looking sticker application effects on traffic sign detectors performance; (b) we use traffic sign image classification as our use case and train and attack 11 of the modern architectures for our analysis; (c) by considering different factors like brightness, blurriness and contrast of the train images in our sticker application procedure, we show that simple image processing techniques can help realistic looking stickers fit into their background to mimic real world tests; (d) by performing structured synthetic and real-world evaluations, we study the difference of various traffic sign classes in terms of their crucial distinctive features among the tested DNNs.},
  isbn = {978-1-66548-821-1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bayzidiTrafficSignClassifiers2022.pdf}
}

@article{bentdigitalbiomarkerdiscovery2021,
  title = {The Digital Biomarker Discovery Pipeline: {{An}} Open-Source Software Platform for the Development of Digital Biomarkers Using {{mHealth}} and Wearables Data},
  shorttitle = {The Digital Biomarker Discovery Pipeline},
  author = {Bent, Brinnae and Wang, Ke and Grzesiak, Emilia and Jiang, Chentian and Qi, Yuankai and Jiang, Yihang and Cho, Peter and Zingler, Kyle and Ogbeide, Felix Ikponmwosa and Zhao, Arthur and Runge, Ryan and Sim, Ida and Dunn, Jessilyn},
  year = {2021},
  journal = {J. Clin. Trans. Sci.},
  volume = {5},
  number = {1},
  pages = {e19},
  issn = {2059-8661},
  doi = {10.1017/cts.2020.511},
  urldate = {2023-03-26},
  abstract = {Introduction: Digital health is rapidly expanding due to surging healthcare costs, deteriorating health outcomes, and the growing prevalence and accessibility of mobile health (mHealth) and wearable technology. Data from Biometric Monitoring Technologies (BioMeTs), including mHealth and wearables, can be transformed into digital biomarkers that act as indicators of health outcomes and can be used to diagnose and monitor a number of chronic diseases and conditions. There are many challenges faced by digital biomarker development, including a lack of regulatory oversight, limited funding opportunities, general mistrust of sharing personal data, and a shortage of open-source data and code. Further, the process of transforming data into digital biomarkers is computationally expensive, and standards and validation methods in digital biomarker research are lacking. Methods: In order to provide a collaborative, standardized space for digital biomarker research and validation, we present the first comprehensive, open-source software platform for end-to-end digital biomarker development: The Digital Biomarker Discovery Pipeline (DBDP). Results: Here, we detail the general DBDP framework as well as three robust modules within the DBDP that have been developed for specific digital biomarker discovery use cases. Conclusions: The clear need for such a platform will accelerate the DBDP's adoption as the industry standard for digital biomarker development and will support its role as the epicenter of digital biomarker collaboration and exploration.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bentdigitalbiomarkerdiscovery2021.pdf}
}

@article{bergerFieldEvaluationSmartphonebased2015,
  title = {Field {{Evaluation}} of the {{Smartphone-based Travel Behaviour Data Collection App}} ``{{SmartMo}}''},
  author = {Berger, Martin and Platzer, Mario},
  year = {2015},
  journal = {Transportation Research Procedia},
  volume = {11},
  pages = {263--279},
  issn = {23521465},
  doi = {10.1016/j.trpro.2015.12.023},
  urldate = {2023-03-26},
  abstract = {This paper outlines an innovative approach to the evaluation of a self-administered smartphone-based survey for the collection of travel behaviour data. For this approach, a traditional travel survey is modified to match mobile devices. The smartphone application ``SmartMo'' is designed in a multi-stage iterative development process. It is then implemented and evaluated through a number of field tests involving 97 participants. Results of the field evaluation will be discussed including the technical performance (e.g. secure data transfer and data management, energy consumption, map-matching), usability (e.g. comprehensibility, handling, joy of use) as well as user acceptance (e.g. willingness to participate, data protection and privacy). A brief overview of the SmartMo data collection system will also be provided.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bergerFieldEvaluationSmartphonebased2015.pdf}
}

@misc{brownLanguageModelsare2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \textendash{} something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/brownLanguageModelsare2020.pdf}
}

@article{chatterjeeThisnewconversational2023,
  title = {This New Conversational {{AI}} Model Can Be Your Friend, Philosopher, and Guide ... and Even Your Worst Enemy},
  author = {Chatterjee, Joyjit and Dethlefs, Nina},
  year = {2023},
  month = jan,
  journal = {Patterns},
  volume = {4},
  number = {1},
  pages = {100676},
  issn = {26663899},
  doi = {10.1016/j.patter.2022.100676},
  urldate = {2023-06-21},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/chatterjeeThisnewconversational2023.pdf}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-30},
  abstract = {We introduce Codex, a GPT language model finetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/chenEvaluatingLargeLanguage2021.pdf}
}

@article{chenExaminingeffectsmotives2017,
  title = {Examining the Effects of Motives and Gender Differences on Smartphone Addiction},
  author = {Chen, Chongyang and Zhang, Kem Z.K. and Gong, Xiang and Zhao, Sesia J. and Lee, Matthew K.O. and Liang, Liang},
  year = {2017},
  month = oct,
  journal = {Computers in Human Behavior},
  volume = {75},
  pages = {891--902},
  issn = {07475632},
  doi = {10.1016/j.chb.2017.07.002},
  urldate = {2023-05-06},
  abstract = {Smartphones have become increasingly popular in recent years. However, it may be addiction-prone and result in negative outcomes. Given that relevant research remains limited, this study attempts to address two research gaps in the extant information systems literature. First, research on the determinants of smartphone addiction remains scarce. Second, the role of individual characteristics (i.e., gender) in the formation of smartphone addiction is far from clear. To fill these research gaps, this study develops a research model of smartphone addiction from the functionalist perspective and highlights the moderating role of gender with the insight of social orientation. We propose four categories of motives, including enhancement (i.e., perceived enjoyment), social (i.e., social relationship), coping (i.e., mood regulation and pastime), and conformity motives (i.e., conformity). Empirical results from our online survey illustrate that perceived enjoyment, mood regulation, pastime, and conformity positively affect smartphone addiction, whereas social relationship has no significant effect. Moreover, we find that gender moderates the effects of perceived enjoyment, pastime, and conformity on smartphone addiction. We expect that this study can enrich the theoretical understanding of how motives play different roles in the development of smartphone addiction. Implications are offered for both research and practice.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/chenExaminingeffectsmotives2017.pdf}
}

@misc{chengGPT4GoodData2023,
  title = {Is {{GPT-4}} a {{Good Data Analyst}}?},
  author = {Cheng, Liying and Li, Xingxuan and Bing, Lidong},
  year = {2023},
  month = may,
  number = {arXiv:2305.15038},
  eprint = {2305.15038},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-02},
  abstract = {As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of ``is GPT-4 a good data analyst?'' in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that GPT-4 can replace data analysts. Our code, data and demo are available at: https://github.com/DAMO-NLP-SG/ GPT4-as-DataAnalyst.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/chengGPT4GoodData2023.pdf}
}

@misc{chenPaLIJointlyScaledMultilingual2023,
  title = {{{PaLI}}: {{A Jointly-Scaled Multilingual Language-Image Model}}},
  shorttitle = {{{PaLI}}},
  author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, A. J. and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Ding, Nan and Rong, Keran and Akbari, Hassan and Mishra, Gaurav and Xue, Linting and Thapliyal, Ashish and Bradbury, James and Kuo, Weicheng and Seyedhosseini, Mojtaba and Jia, Chao and Ayan, Burcu Karagol and Riquelme, Carlos and Steiner, Andreas and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
  year = {2023},
  month = jun,
  number = {arXiv:2209.06794},
  eprint = {2209.06794},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-05},
  abstract = {Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pre-training tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/chenPaLIJointlyScaledMultilingual2023.pdf}
}

@misc{chenPaLIXScalingMultilingual2023,
  title = {{{PaLI-X}}: {{On Scaling}} up a {{Multilingual Vision}} and {{Language Model}}},
  shorttitle = {{{PaLI-X}}},
  author = {Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and Shakeri, Siamak and Dehghani, Mostafa and Salz, Daniel and Lucic, Mario and Tschannen, Michael and Nagrani, Arsha and Hu, Hexiang and Joshi, Mandar and Pang, Bo and Montgomery, Ceslee and Pietrzyk, Paulina and Ritter, Marvin and Piergiovanni, A. J. and Minderer, Matthias and Pavetic, Filip and Waters, Austin and Li, Gang and Alabdulmohsin, Ibrahim and Beyer, Lucas and Amelot, Julien and Lee, Kenton and Steiner, Andreas Peter and Li, Yang and Keysers, Daniel and Arnab, Anurag and Xu, Yuanzhong and Rong, Keran and Kolesnikov, Alexander and Seyedhosseini, Mojtaba and Angelova, Anelia and Zhai, Xiaohua and Houlsby, Neil and Soricut, Radu},
  year = {2023},
  month = may,
  number = {arXiv:2305.18565},
  eprint = {2305.18565},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-11},
  abstract = {We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/chenPaLIXScalingMultilingual2023.pdf}
}

@misc{chenPix2seqLanguageModeling2022,
  title = {Pix2seq: {{A Language Modeling Framework}} for {{Object Detection}}},
  shorttitle = {Pix2seq},
  author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J. and Hinton, Geoffrey},
  year = {2022},
  month = mar,
  number = {arXiv:2109.10852},
  eprint = {2109.10852},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/chenPix2seqLanguageModeling2022.pdf}
}

@misc{chenUnifiedSequenceInterface2022,
  title = {A {{Unified Sequence Interface}} for {{Vision Tasks}}},
  author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Lin, Tsung-Yi and Fleet, David J. and Hinton, Geoffrey},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07669},
  eprint = {2206.07669},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-30},
  abstract = {While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of ``core'' computer vision tasks can also be unified if formulated in terms of a shared pixelto-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/chenUnifiedSequenceInterface2022.pdf}
}

@inproceedings{chenVisualGPTDataefficientAdaptation2022,
  title = {{{VisualGPT}}: {{Data-efficient Adaptation}} of {{Pretrained Language Models}} for {{Image Captioning}}},
  shorttitle = {{{VisualGPT}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  year = {2022},
  month = jun,
  pages = {18009--18019},
  publisher = {{IEEE}},
  address = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01750},
  urldate = {2023-09-21},
  abstract = {The limited availability of annotated data often hinders real-world applications of machine learning. To efficiently learn from small quantities of multimodal data, we leverage the linguistic knowledge from a large pre-trained language model (PLM) and quickly adapt it to new domains of image captioning. To effectively utilize a pretrained model, it is critical to balance the visual input and prior linguistic knowledge from pretraining. We propose VisualGPT, which employs a novel self-resurrecting encoderdecoder attention mechanism to quickly adapt the PLM with a small amount of in-domain image-text data. The proposed self-resurrecting activation unit produces sparse activations that prevent accidental overwriting of linguistic knowledge. When trained on 0.1\%, 0.5\% and 1\% of the respective training sets, VisualGPT surpasses the best baseline by up to 10.0\% CIDEr on MS COCO [45] and 17.9\% CIDEr on Conceptual Captions [69]. Furthermore, VisualGPT achieves the state-of-the-art result on IU X-ray [15], a medical report generation dataset. Our code is available at https://github.com/Vision-CAIR/ VisualGPT.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/chenVisualGPTDataefficientAdaptation2022.pdf}
}

@inproceedings{chowDEMONSintegratedframework2016,
  title = {{{DEMONS}}: An Integrated Framework for Examining Associations between Physiology and Self-Reported Affect Tied to Depressive Symptoms},
  shorttitle = {{{DEMONS}}},
  booktitle = {Proceedings of the 2016 {{ACM International Joint Conference}} on {{Pervasive}} and {{Ubiquitous Computing}}: {{Adjunct}}},
  author = {Chow, Philip and Bonelli, Wesley and Huang, Yu and Fua, Karl and Teachman, Bethany A. and Barnes, Laura E.},
  year = {2016},
  month = sep,
  pages = {1139--1143},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2968219.2968300},
  urldate = {2023-03-26},
  abstract = {Depression is a prevalent and debilitating disorder among college students. Advances in mobile technology afford the opportunity to collect heterogeneous data while people are in their natural settings. The aim of the current paper is to propose an integrated framework, DEMONS (DEpression MONitoring Study), for combining passive and active data sources using a wearable sensor and a smartphone application. The ability to combine passive and active longitudinal data with mobile devices allows for better understanding of the temporal relations between self-reported affect and physiological variables (e.g., heart rate variability) linked to depressive symptoms. Adoption of the proposed framework will provide crucial information regarding the development and maintenance of depression in college students, as well as increased opportunities for early detection and intervention.},
  isbn = {978-1-4503-4462-3},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/chowDEMONSintegratedframework2016.pdf}
}

@misc{christiancmehil-warnnixondiaryextraction2023,
  title = {Nixon-Diary-Extraction},
  author = {{Christian Cmehil-Warn}},
  year = {2023},
  month = may,
  journal = {nixon-diary-extraction},
  urldate = {2023-05-04},
  howpublished = {https://github.com/ChristianCme/nixon-diary-extraction}
}

@article{cuiContextAwareBlockNet2022,
  title = {Context-{{Aware Block Net}} for {{Small Object Detection}}},
  author = {Cui, Lisha and Lv, Pei and Jiang, Xiaoheng and Gao, Zhimin and Zhou, Bing and Zhang, Luming and Shao, Ling and Xu, Mingliang},
  year = {2022},
  month = apr,
  journal = {IEEE Trans. Cybern.},
  volume = {52},
  number = {4},
  pages = {2300--2313},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2020.3004636},
  urldate = {2023-03-21},
  abstract = {State-of-the-art object detectors usually progressively downsample the input image until it is represented by small feature maps, which loses the spatial information and compromises the representation of small objects. In this article, we propose a context-aware block net (CAB Net) to improve small object detection by building high-resolution and strong semantic feature maps. To internally enhance the representation capacity of feature maps with high spatial resolution, we delicately design the context-aware block (CAB). CAB exploits pyramidal dilated convolutions to incorporate multilevel contextual information without losing the original resolution of feature maps. Then, we assemble CAB to the end of the truncated backbone network (e.g., VGG16) with a relatively small downsampling factor (e.g., 8) and cast off all following layers. CAB Net can capture both basic visual patterns as well as semantical information of small objects, thus improving the performance of small object detection. Experiments conducted on the benchmark Tsinghua-Tencent 100K and the Airport dataset show that CAB Net outperforms other top-performing detectors by a large margin while keeping real-time speed, which demonstrates the effectiveness of CAB Net for small object detection.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/cuiContextAwareBlockNet2022.pdf}
}

@article{dahmenArtificialintelligencebot2023,
  title = {Artificial Intelligence Bot {{ChatGPT}} in Medical Research: The Potential Game Changer as a Double-Edged Sword},
  shorttitle = {Artificial Intelligence Bot {{ChatGPT}} in Medical Research},
  author = {Dahmen, Jari and Kayaalp, M. Enes and Ollivier, Matthieu and Pareek, Ayoosh and Hirschmann, Michael T. and Karlsson, Jon and Winkler, Philipp W.},
  year = {2023},
  month = apr,
  journal = {Knee Surg Sports Traumatol Arthrosc},
  volume = {31},
  number = {4},
  pages = {1187--1189},
  issn = {0942-2056, 1433-7347},
  doi = {10.1007/s00167-023-07355-6},
  urldate = {2023-06-22},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/dahmenArtificialintelligencebot2023.pdf}
}

@article{daiAugGPTLeveragingChatGPT,
  title = {{{AugGPT}}: {{Leveraging ChatGPT}} for {{Text Data Augmentation}}},
  author = {Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Cao, Yihan and Wu, Zihao and Zhao, Lin and Xu, Shaochen and Liu, Wei and Liu, Ninghao and Li, Sheng and Zhu, Dajiang and Cai, Hongmin and Sun, Lichao and Li, Quanzheng and Shen, Dinggang and Liu, Tianming and Li, Xiang},
  abstract = {Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/daiAugGPTLeveragingChatGPT.pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/devlinBERTPretrainingDeep2019.pdf}
}

@article{devriesSmartphoneBasedEcologicalMomentary2021,
  title = {Smartphone-{{Based Ecological Momentary Assessment}} of {{Well-Being}}: {{A Systematic Review}} and {{Recommendations}} for {{Future Studies}}},
  shorttitle = {Smartphone-{{Based Ecological Momentary Assessment}} of {{Well-Being}}},
  author = {{de Vries}, Lianne P. and Baselmans, Bart M. L. and Bartels, Meike},
  year = {2021},
  month = jun,
  journal = {J Happiness Stud},
  volume = {22},
  number = {5},
  pages = {2361--2408},
  issn = {1389-4978, 1573-7780},
  doi = {10.1007/s10902-020-00324-7},
  urldate = {2023-03-24},
  abstract = {Feelings of well-being and happiness fluctuate over time and contexts. Ecological Momentary Assessment (EMA) studies can capture fluctuations in momentary behavior, and experiences by assessing these multiple times per day. Traditionally, EMA was performed using pen and paper. Recently, due to technological advances EMA studies can be conducted more easily with smartphones, a device ubiquitous in our society. The goal of this review was to evaluate the literature on smartphone-based EMA in well-being research in healthy subjects. The systematic review was conducted according to the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines. Searching PubMed and Web of Science, we identified 53 studies using smartphone-based EMA of wellbeing. Studies were heterogeneous in designs, context, and measures. The average study duration was 12.8 days, with well-being assessed 2\textendash 12 times per day. Half of the studies included objective data (e.g. location). Only 47.2\% reported compliance, indicating a mean of 71.6\%. Well-being fluctuated daily and weekly, with higher well-being in evenings and weekends. These fluctuations disappeared when location and activity were accounted for. On average, being in nature and physical activity relates to higher well-being. Working relates to lower well-being, but workplace and company do influence well-being. The important advantages of using smartphones instead of other devices to collect EMAs are the easier data collection and flexible designs. Smartphone-based EMA reach far larger maximum sample sizes and more easily add objective data to their designs than palm-top/ PDA studies. Smartphone-based EMA research is feasible to gain insight in well-being fluctuations and its determinants and offers the opportunity for parallel objective data collection. Most studies currently focus on group comparisons, while studies on individual differences in well-being patterns and fluctuations are lacking. We provide recommendations for future smartphone-based EMA research regarding measures, objective data and analyses.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/devriesSmartphoneBasedEcologicalMomentary2021.pdf}
}

@article{dewiYoloV4Advanced2021,
  title = {Yolo {{V4}} for {{Advanced Traffic Sign Recognition With Synthetic Training Data Generated}} by {{Various GAN}}},
  author = {Dewi, Christine and Chen, Rung-Ching and Liu, Yan-Ting and Jiang, Xiaoyi and Hartomo, Kristoko Dwi},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {97228--97242},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3094201},
  urldate = {2023-03-20},
  abstract = {Convolutional Neural Networks (CNN) achieves perfection in traffic sign identification with enough annotated training data. The dataset determines the quality of the complete visual system based on CNN. Unfortunately, databases for traffic signs from the majority of the world's nations are few. In this scenario, Generative Adversarial Networks (GAN) may be employed to produce more realistic and varied training pictures to supplement the actual arrangement of images. The purpose of this research is to describe how the quality of synthetic pictures created by DCGAN, LSGAN, and WGAN is determined. Our work combines synthetic images with original images to enhance datasets and verify the effectiveness of synthetic datasets. We use different numbers and sizes of images for training. Likewise, the Structural Similarity Index (SSIM) and Mean Square Error (MSE) were employed to assess picture quality. Our study quantifies the SSIM difference between the synthetic and actual images. When additional images are used for training, the synthetic image exhibits a high degree of resemblance to the genuine image. The highest SSIM value was achieved when using 200 total images as input and 32 \texttimes{} 32 image size. Further, we augment the original picture dataset with synthetic pictures and compare the original image model to the synthesis image model. For this experiment, we are using the latest iterations of Yolo, Yolo V3, and Yolo V4. After mixing the real image with the synthesized image produced by LSGAN, the recognition performance has been improved, achieving an accuracy of 84.9\% on Yolo V3 and an accuracy of 89.33\% on Yolo V4.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/dewiYoloV4Advanced2021.pdf}
}

@misc{doryabExtractionBehavioralFeatures2019,
  title = {Extraction of {{Behavioral Features}} from {{Smartphone}} and {{Wearable Data}}},
  author = {Doryab, Afsaneh and Chikarsel, Prerna and Liu, Xinwen and Dey, Anind K.},
  year = {2019},
  month = jan,
  number = {arXiv:1812.10394},
  eprint = {1812.10394},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-03-27},
  abstract = {The rich set of sensors in smartphones and wearable devices provides the possibility to passively collect streams of data in the wild. The raw data streams, however, can rarely be directly used in the modeling pipeline. We provide a generic framework that can process raw data streams and extract useful features related to non-verbal human behavior. This framework can be used by researchers in the field who are interested in processing data from smartphones and Wearable devices.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/doryabExtractionBehavioralFeatures2019.pdf}
}

@inproceedings{fengImpactChatGPTStreaming2023,
  title = {The {{Impact}} of {{ChatGPT}} on {{Streaming Media}}: {{A Crowdsourced}} and {{Data-Driven Analysis}} Using {{Twitter}} and {{Reddit}}},
  shorttitle = {The {{Impact}} of {{ChatGPT}} on {{Streaming Media}}},
  booktitle = {2023 {{IEEE}} 9th {{Intl Conference}} on {{Big Data Security}} on {{Cloud}} ({{BigDataSecurity}}), {{IEEE Intl Conference}} on {{High Performance}} and {{Smart Computing}}, ({{HPSC}}) and {{IEEE Intl Conference}} on {{Intelligent Data}} and {{Security}} ({{IDS}})},
  author = {Feng, Yunhe and Poralla, Pradhyumna and Dash, Swagatika and Li, Kaicheng and Desai, Vrushabh and Qiu, Meikang},
  year = {2023},
  month = may,
  pages = {222--227},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}},
  doi = {10.1109/BigDataSecurity-HPSC-IDS58521.2023.00046},
  urldate = {2023-06-22},
  abstract = {ChatGPT, a general-purpose text generation AI model, is reshaping various domains ranging from education and software development to legal defense and novel writing. Despite its potential impact, there is a lack of research on how ChatGPT might influence streaming media, which is an essential part of everyday entertainment. As a result, it remains unclear how ChatGPT is changing the future of streaming media. To bridge such a research gap, in this paper, we propose a crowdsourced, data-driven framework that leverages two social media platforms, Twitter and Reddit, to explore the impact of ChatGPT on streaming media. Through extensive analysis of social media data collected from Twitter and Reddit, we reveal how ChatGPT is transforming streaming media from diverse perspectives. Our data analytics demonstrates that ChatGPT is sparking both fear and excitement in the context of the streaming media and enhancing the downstream visual generative models, such as DALLE-2 and Stable Diffusion Videos. To the best of our knowledge, this study is the first large-scale and systematical investigation into the effects of ChatGPT on streaming media. Hope our findings will inspire further research and discussions on this topic across academia and industry.},
  isbn = {9798350312935},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/fengImpactChatGPTStreaming2023.pdf}
}

@article{fengInvestigatingCodeGeneration,
  title = {Investigating {{Code Generation Performance}} of {{ChatGPT}} with {{Crowdsourcing Social Data}}},
  author = {Feng, Yunhe and Vanam, Sreecharan and Cherukupally, Manasa and Zheng, Weijian and Qiu, Meikang and Chen, Haihua},
  abstract = {The recent advancements in Artificial Intelligence, particularly in large language models and generative models, are reshaping the field of software engineering by enabling innovative ways of performing various tasks, such as programming, debugging, and testing. However, few existing works have thoroughly explored the potential of AI in code generation and users' attitudes toward AI-assisted coding tools. This knowledge gap leaves it unclear how AI is transforming software engineering and programming education. This paper presents a scalable crowdsourcing data-driven framework to investigate the code generation performance of generative large language models from diverse perspectives across multiple social media platforms. Specifically, we utilize ChatGPT, a popular generative large language model, as a representative example to reveal its insights and patterns in code generation. First, we propose a hybrid keyword word expansion method that integrates words suggested by topic modeling and expert knowledge to filter relevant social posts of interest on Twitter and Reddit. Then we collect 316K tweets and 3.2K Reddit posts about ChatGPT's code generation, spanning from Dec. 1, 2022 to January 31, 2023. Our data analytics show that ChatGPT has been used in more than 10 programming languages, with Python and JavaScript being the two most popular, for a diverse range of tasks such as code debugging, interview preparation, and academic assignment solving. Surprisingly, our analysis shows that fear is the dominant emotion associated with ChatGPT's code generation, overshadowing emotions of happiness, anger, surprise, and sadness. Furthermore, we construct a ChatGPT prompt and corresponding code dataset by analyzing the screenshots of ChatGPT code generation shared on social media. This dataset enables us to evaluate the quality of the generated code, and we have released this dataset to the public. We believe the insights gained from our work will provide valuable guidance for future research on AI-powered code generation.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/fengInvestigatingCodeGeneration.pdf}
}

@article{ferreiraAWAREMobileContext2015,
  title = {{{AWARE}}: {{Mobile Context Instrumentation Framework}}},
  shorttitle = {{{AWARE}}},
  author = {Ferreira, Denzil and Kostakos, Vassilis and Dey, Anind K.},
  year = {2015},
  month = apr,
  journal = {Front. ICT},
  volume = {2},
  issn = {2297-198X},
  doi = {10.3389/fict.2015.00006},
  urldate = {2023-03-22},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/ferreiraAWAREMobileContext2015.pdf}
}

@article{fritzDatafusionmobile2022,
  title = {Data Fusion of Mobile and Environmental Sensing Devices to Understand the Effect of the Indoor Environment on Measured and Self-Reported Sleep Quality},
  author = {Fritz, Hagen and Kinney, Kerry A. and Wu, Congyu and Schnyer, David M. and Nagy, Zoltan},
  year = {2022},
  month = apr,
  journal = {Building and Environment},
  volume = {214},
  pages = {108835},
  issn = {03601323},
  doi = {10.1016/j.buildenv.2022.108835},
  urldate = {2023-04-11},
  abstract = {The Indoor Air Quality (IAQ) of the bedroom environment has recently garnered attention since air pollution can affect sleep. Previous studies investigated IAQ and sleep quality in controlled environments which impacts both self-reported and measured sleep quality. Studies within a participant's home environment are ecologically valid and reduce participant bias. Here, we study 20 participants over 77 days in Austin, TX. We monitored five components of IAQ using the BEVO Beacon, a calibrated purpose-built environmental monitor, and measured participant sleep quality through wearable activity trackers and 4-question surveys sent four times a week. We found significant decreases in sleep quality during nights with elevated CO, CO2, and temperature. Elevated CO was associated with a mean increase in 0.9 self-reported awakenings and decreases in device-measured sleep time of 21.6 min and sleep efficiency of 0.6\%. Increased CO2 and temperature were associated with decreases in device-measured sleep time of 17.5 and 15.2 min, respectively. Elevated PM2.5 and TVOCs concentrations were associated with overall improvements in sleep quality. Participants reported a mean of 4.4 fewer awakenings and had a 1.1\% increased in measured sleep efficiency for nights with elevated PM2.5. Elevated TVOCs were associated with an increase in sleep time of 14.5 min. These findings indicate a need to study the relationship between these aggregate IAQ measures and sleep quality more closely. Our results also indicate that pollutants can independently affect sleep quality regardless of the CO2 measurements. Compared to literature, our study is the longest and includes the most IAQ parameters.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/fritzDatafusionmobile2022.pdf}
}

@article{fritzEvaluatingmachinelearning2022,
  title = {Evaluating Machine Learning Models to Classify Occupants' Perceptions of Their Indoor Environment and Sleep Quality from Indoor Air Quality},
  author = {Fritz, Hagen and Tang, Mengjia and Kinney, Kerry and Nagy, Zoltan},
  year = {2022},
  month = dec,
  journal = {Journal of the Air \& Waste Management Association},
  volume = {72},
  number = {12},
  pages = {1381--1397},
  issn = {1096-2247, 2162-2906},
  doi = {10.1080/10962247.2022.2105439},
  urldate = {2023-04-11},
  abstract = {A variety of factors can affect a person's perception of their environment and health, but one factor that is often overlooked in indoor settings is the air quality. To address this gap, we develop and evaluate four Machine Learning (ML) models on two disparate datasets using Indoor Air Quality (IAQ) parameters as primary features and components of self-reported IAQ satisfaction and sleep quality as target variables. In each case, we compare models to each other as well as to a simple model that always predicts the majority outcome. In the first analysis, we use open-source data collected from 93 California residences to predict occupant's satisfaction with their indoor environ\- ment. Results indicate building ventilation rate, Relative Humidity (RH), and formaldehyde are most influential when predicting IAQ perception and do so with an accuracy greater than the simplified model. The second analysis uses IAQ data gathered from a field study we conducted with 20 participants over 11 weeks to train similar models. We obtain accuracy and F1 scores similar to the simplified model where PM2.5 and TVOCs represent the most important predictors. Our results underscore the ability of IAQ to affect a person's perception of their built environment and health and highlight the utility of ML models to explore the strength of these relationships.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/fritzEvaluatingmachinelearning2022.pdf}
}

@article{gaggiolimobiledatacollection2013,
  title = {A Mobile Data Collection Platform for Mental Health Research},
  author = {Gaggioli, Andrea and Pioggia, Giovanni and Tartarisco, Gennaro and Baldus, Giovanni and Corda, Daniele and Cipresso, Pietro and Riva, Giuseppe},
  year = {2013},
  month = feb,
  journal = {Pers Ubiquit Comput},
  volume = {17},
  number = {2},
  pages = {241--251},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-011-0465-2},
  urldate = {2023-03-29},
  abstract = {Ubiquitous computing technologies offer exciting new possibilities for monitoring and analyzing user's experience in real time. In this paper, we describe the design and development of Psychlog, a mobile phone platform designed to collect users' psychological, physiological, and activity information for mental health research. The tool allows administering self-report questionnaires at specific times or randomly within a day. The system also permits to collect heart rate and activity information from a wireless electrocardiogram equipped with a three-axial accelerometer. By combining self-reports with heart rate and activity data, the application makes it possible to investigate the relationship between psychological, physiological, and behavioral variables, as well as to monitor their fluctuations over time. The software runs on Windows mobile operative system and is available as open source (http://sourceforge. net/projects/psychlog/).},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/gaggiolimobiledatacollection22.pdf}
}

@misc{gaoCollabCoderGPTPoweredWorkflow2023,
  title = {{{CollabCoder}}: {{A GPT-Powered Workflow}} for {{Collaborative Qualitative Analysis}}},
  shorttitle = {{{CollabCoder}}},
  author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.07366},
  eprint = {2304.07366},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-30},
  abstract = {The Collaborative Qualitative Analysis (CQA) process can be time-consuming and resource-intensive, requiring multiple discussions among team members to refine codes and ideas before reaching a consensus. To address these challenges, we introduce CollabCoder, a system leveraging Large Language Models (LLMs) to support three CQA stages: independent open coding, iterative discussions, and the development of a final codebook. In the independent open coding phase, CollabCoder provides AI-generated code suggestions on demand, and allows users to record coding decision-making information (e.g. keywords and certainty) as support for the process. During the discussion phase, CollabCoder helps to build mutual understanding and productive discussion by sharing coding decision-making information with the team. It also helps to quickly identify agreements and disagreements through quantitative metrics, in order to build a final consensus. During the code grouping phase, CollabCoder employs a top-down approach for primary code group recommendations, reducing the cognitive burden of generating the final codebook. An evaluation involving 16 users confirmed the usability and effectiveness of CollabCoder and offered empirical insights into the LLMs' roles in CQA.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/reyvababtista/Projects/Papers/gaoCollabCoderGPTPoweredWorkflow2023.pdf}
}

@misc{geOpenAGIWhenLLM2023,
  title = {{{OpenAGI}}: {{When LLM Meets Domain Experts}}},
  shorttitle = {{{OpenAGI}}},
  author = {Ge, Yingqiang and Hua, Wenyue and Mei, Kai and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng},
  year = {2023},
  month = aug,
  number = {arXiv:2304.04370},
  eprint = {2304.04370},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-21},
  abstract = {Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/geOpenAGIWhenLLM2023.pdf}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/goodfellowGenerativeAdversarialNetworks2014.pdf}
}

@inproceedings{guoDetectionOccludedRoad2019,
  title = {Detection of {{Occluded Road Signs}} on {{Autonomous Driving Vehicles}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Guo, Jingda and Cheng, Xianwei and Chen, Qi and Yang, Qing},
  year = {2019},
  month = jul,
  pages = {856--861},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/ICME.2019.00152},
  urldate = {2023-03-20},
  abstract = {Autonomous driving vehicle relies heavily on its perception system to sense surrounding environments and make driving decisions. One important task on autonomous driving vehicles is to correctly recognize different traffic signs. However, the traffic signs in the wild can be in various conditions, e.g., occluded, deteriorated, or vandalized, and not all of them are recognizable. In this work, we propose a novel system that leverages the perception system on autonomous vehicle to identify occluded road signs in real time. Based on transfer learning, we propose the occluded sign classification network (OSCN) that is able to achieve a precision of 96.34\% on a real-world dataset.},
  isbn = {978-1-5386-9552-4},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/guoDetectionOccludedRoad2019.pdf}
}

@article{hallIntroductionMultisensorData1997,
  title = {An {{Introduction}} to {{Multisensor Data Fusion}}},
  author = {Hall, David L and Llinas, James},
  year = {1997},
  journal = {PROCEEDINGS OF THE IEEE},
  volume = {85},
  number = {1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hallIntroductionMultisensorData1997.pdf}
}

@article{hanTransformerTransformer,
  title = {Transformer in {{Transformer}}},
  author = {Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  abstract = {Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16\texttimes 16) as ``visual sentences'' and present to further divide them into smaller patches (e.g., 4\texttimes 4) as ``visual words''. The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5\% top-1 accuracy on the ImageNet, which is about 1.7\% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/ tree/master/research/cv/TNT.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hanTransformerTransformer.pdf}
}

@article{harrisResearchelectronicdata2009,
  title = {Research Electronic Data Capture ({{REDCap}})\textemdash{{A}} Metadata-Driven Methodology and Workflow Process for Providing Translational Research Informatics Support},
  author = {Harris, Paul A. and Taylor, Robert and Thielke, Robert and Payne, Jonathon and Gonzalez, Nathaniel and Conde, Jose G.},
  year = {2009},
  month = apr,
  journal = {Journal of Biomedical Informatics},
  volume = {42},
  number = {2},
  pages = {377--381},
  issn = {15320464},
  doi = {10.1016/j.jbi.2008.08.010},
  urldate = {2023-03-29},
  abstract = {Research electronic data capture (REDCap) is a novel workflow methodology and software solution designed for rapid development and deployment of electronic data capture tools to support clinical and translational research. We present: (1) a brief description of the REDCap metadata-driven software toolset; (2) detail concerning the capture and use of study-related metadata from scientific research teams; (3) measures of impact for REDCap; (4) details concerning a consortium network of domestic and international institutions collaborating on the project; and (5) strengths and limitations of the REDCap system. REDCap is currently supporting 286 translational research projects in a growing collaborative network including 27 active partner institutions.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/harrisResearchelectronicdata22.pdf}
}

@misc{hassanChatGPTyourPersonal2023,
  title = {{{ChatGPT}} as Your {{Personal Data Scientist}}},
  author = {Hassan, Md Mahadi and Knipper, Alex and Santu, Shubhra Kanti Karmaker},
  year = {2023},
  month = may,
  number = {arXiv:2305.13657},
  eprint = {2305.13657},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-21},
  abstract = {The rise of big data has amplified the need for efficient, user-friendly automated machine learning (AutoML) tools. However, the intricacy of understanding domain-specific data and defining prediction tasks necessitates human intervention making the process time-consuming while preventing full automation. Instead, envision an intelligent agent capable of assisting users in conducting AutoML tasks through intuitive, natural conversations without requiring in-depth knowledge of the underlying machine learning (ML) processes. This agent's key challenge is to accurately comprehend the user's prediction goals and, consequently, formulate precise ML tasks, adjust data sets and model parameters accordingly, and articulate results effectively. In this paper, we take a pioneering step towards this ambitious goal by introducing a ChatGPT-based conversational data-science framework to act as a "personal data scientist". Precisely, we utilize Large Language Models (ChatGPT) to build a natural interface between the users and the ML models (Scikit-Learn), which in turn, allows us to approach this ambitious problem with a realistic solution. Our model pivots around four dialogue states: Data Visualization, Task Formulation, Prediction Engineering, and Result Summary and Recommendation. Each state marks a unique conversation phase, impacting the overall user-system interaction. Multiple LLM instances, serving as "micro-agents", ensure a cohesive conversation flow, granting us granular control over the conversation's progression. In summary, we developed an end-to-end system that not only proves the viability of the novel concept of conversational data science but also underscores the potency of LLMs in solving complex tasks. Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/hassanChatGPTyourPersonal2023.pdf}
}

@article{hassaniRoleChatGPTData2023,
  title = {The {{Role}} of {{ChatGPT}} in {{Data Science}}: {{How AI-Assisted Conversational Interfaces Are Revolutionizing}} the {{Field}}},
  shorttitle = {The {{Role}} of {{ChatGPT}} in {{Data Science}}},
  author = {Hassani, Hossein and Silva, Emmanuel Sirmal},
  year = {2023},
  month = mar,
  journal = {BDCC},
  volume = {7},
  number = {2},
  pages = {62},
  issn = {2504-2289},
  doi = {10.3390/bdcc7020062},
  urldate = {2023-06-16},
  abstract = {ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT's architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hassaniRoleChatGPTData2023.pdf}
}

@article{hautHyperspectralImageClassification2019,
  title = {Hyperspectral {{Image Classification Using Random Occlusion Data Augmentation}}},
  author = {Haut, Juan Mario and Paoletti, Mercedes E. and Plaza, Javier and Plaza, Antonio and Plaza, Li},
  year = {2019},
  month = nov,
  journal = {IEEE Geosci. Remote Sensing Lett.},
  volume = {16},
  number = {11},
  pages = {1751--1755},
  issn = {1545-598X, 1558-0571},
  doi = {10.1109/LGRS.2019.2909495},
  urldate = {2023-03-20},
  abstract = {Convolutional neural networks (CNNs) have become a powerful tool for remotely sensed hyperspectral image (HSI) classification due to their great generalization ability and high accuracy. However, owing to the huge amount of parameters that need to be learned and to the complex nature of HSI data itself, these approaches must deal with the important problem of overfitting, which can lead to inadequate generalization and loss of accuracy. In order to mitigate this problem, in this letter, we adopt random occlusion, a recently developed data augmentation (DA) method for training CNNs, in which the pixels of different rectangular spatial regions in the HSI are randomly occluded, generating training images with various levels of occlusion and reducing the risk of overfitting. Our results with two well-known HSIs reveal that the proposed method helps to achieve better classification accuracy with low computational cost.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hautHyperspectralImageClassification2019.pdf}
}

@misc{hawthorneGeneralpurposelongcontextautoregressive2022,
  title = {General-Purpose, Long-Context Autoregressive Modeling with {{Perceiver AR}}},
  author = {Hawthorne, Curtis and Jaegle, Andrew and Cangea, C{\u a}t{\u a}lina and Borgeaud, Sebastian and Nash, Charlie and Malinowski, Mateusz and Dieleman, Sander and Vinyals, Oriol and Botvinick, Matthew and Simon, Ian and Sheahan, Hannah and Zeghidour, Neil and Alayrac, Jean-Baptiste and Carreira, Jo{\~a}o and Engel, Jesse},
  year = {2022},
  month = jun,
  number = {arXiv:2202.07765},
  eprint = {2202.07765},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-10-24},
  abstract = {Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this longrange structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms. When trained on images or music, Perceiver AR generates outputs with clear long-term coherence and structure. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 \texttimes{} 64 ImageNet images and PG-19 books.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/reyvababtista/Projects/Papers/hawthorneGeneralpurposelongcontextautoregressive2022.pdf}
}

@inproceedings{hicksAndWellnessopenmobile2010,
  title = {{{AndWellness}}: An Open Mobile System for Activity and Experience Sampling},
  shorttitle = {{{AndWellness}}},
  booktitle = {Wireless {{Health}} 2010},
  author = {Hicks, John and Ramanathan, Nithya and Kim, Donnie and Monibi, Mohamad and Selsky, Joshua and Hansen, Mark and Estrin, Deborah},
  year = {2010},
  month = oct,
  pages = {34--43},
  publisher = {{ACM}},
  address = {{San Diego California}},
  doi = {10.1145/1921081.1921087},
  urldate = {2023-03-29},
  abstract = {Advances in mobile phone technology have allowed phones to become a convenient platform for real-time assessment of a participants health and behavior. AndWellness, a personal data collection system, uses mobile phones to collect and analyze data from both active, triggered user experience samples and passive logging of onboard environmental sensors. The system includes an application that runs on Android based mobile phones, server software that manages deployments and acts as a central repository for data, and a dashboard front end for both participants and researchers to visualize incoming data in real-time. Our system has gone through testing by researchers in preparation for deployment with participants, and we describe an initial qualitative study plus several planned future studies to demonstrate how our system can be used to better understand a user's health related habits and observations.},
  isbn = {978-1-60558-989-3},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hicksAndWellnessopenmobile22.pdf}
}

@article{hiltyFrameworkCompetenciesUse2020,
  title = {A {{Framework}} for {{Competencies}} for the {{Use}} of {{Mobile Technologies}} in {{Psychiatry}} and {{Medicine}}: {{Scoping Review}}},
  shorttitle = {A {{Framework}} for {{Competencies}} for the {{Use}} of {{Mobile Technologies}} in {{Psychiatry}} and {{Medicine}}},
  author = {Hilty, Donald and Chan, Steven and Torous, John and Luo, John and Boland, Robert},
  year = {2020},
  month = feb,
  journal = {JMIR Mhealth Uhealth},
  volume = {8},
  number = {2},
  pages = {e12229},
  issn = {2291-5222},
  doi = {10.2196/12229},
  urldate = {2023-03-26},
  abstract = {Background: To ensure quality care, clinicians need skills, knowledge, and attitudes related to technology that can be measured. Objective: This paper sought out competencies for mobile technologies and/or an approach to define them. Methods: A scoping review was conducted to answer the following research question, ``What skills are needed for clinicians and trainees to provide quality care via mHealth, have they been published, and how can they be made measurable and reproducible to teach and assess them?'' The review was conducted in accordance with the 6-stage scoping review process starting with a keyword search in PubMed/Medical Literature Analysis and Retrieval System Online, APA PsycNET, Cochrane, EMBASE, PsycINFO, Web of Science, and Scopus. The literature search focused on keywords in 4 concept areas: (1) competencies, (2) mobile technologies, (3) telemedicine mode, and (4) health. Moreover, 2 authors independently, in parallel, screened the search results for potentially relevant studies based on titles and abstracts. The authors reviewed the full-text articles for final inclusion based on inclusion/exclusion criteria. Inclusion criteria were keywords used from concept area 1 (competencies) and 2 (mobile technologies) and either 3 (telemedicine mode) or 4 (health). Exclusion criteria included, but were not limited to, keywords used from a concept area in isolation, discussion of skills abstractly, outline or listing of what clinicians need without detail, and listing immeasurable behaviors.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hiltyFrameworkCompetenciesUse2020.pdf}
}

@article{huckinsMentalHealthBehavior2020,
  title = {Mental {{Health}} and {{Behavior}} of {{College Students During}} the {{Early Phases}} of the {{COVID-19 Pandemic}}: {{Longitudinal Smartphone}} and {{Ecological Momentary Assessment Study}}},
  shorttitle = {Mental {{Health}} and {{Behavior}} of {{College Students During}} the {{Early Phases}} of the {{COVID-19 Pandemic}}},
  author = {Huckins, Jeremy F and {daSilva}, Alex W and Wang, Weichen and Hedlund, Elin and Rogers, Courtney and Nepal, Subigya K and Wu, Jialing and Obuchi, Mikio and Murphy, Eilis I and Meyer, Meghan L and Wagner, Dylan D and Holtzheimer, Paul E and Campbell, Andrew T},
  year = {2020},
  month = jun,
  journal = {J Med Internet Res},
  volume = {22},
  number = {6},
  pages = {e20185},
  issn = {1438-8871},
  doi = {10.2196/20185},
  urldate = {2023-03-23},
  abstract = {Background: The vast majority of people worldwide have been impacted by coronavirus disease (COVID-19). In addition to the millions of individuals who have been infected with the disease, billions of individuals have been asked or required by local and national governments to change their behavioral patterns. Previous research on epidemics or traumatic events suggests that this can lead to profound behavioral and mental health changes; however, researchers are rarely able to track these changes with frequent, near-real-time sampling or compare their findings to previous years of data for the same individuals.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/huckinsMentalHealthBehavior2020.pdf}
}

@misc{huSqueezeandExcitationNetworks2019,
  title = {Squeeze-and-{{Excitation Networks}}},
  author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  year = {2019},
  month = may,
  number = {arXiv:1709.01507},
  eprint = {1709.01507},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-25},
  abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the ``Squeeze-and-Excitation'' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of {$\sim$}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/huSqueezeandExcitationNetworks2019.pdf}
}

@misc{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  year = {2021},
  month = jun,
  number = {arXiv:2103.03206},
  eprint = {2103.03206},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-10-17},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domainspecific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver \textendash{} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/reyvababtista/Projects/Papers/jaeglePerceiverGeneralPerception2021.pdf}
}

@misc{jaeglePerceiverIOGeneral2022,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H{\'e}naff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Jo{\=a}o},
  year = {2022},
  month = mar,
  number = {arXiv:2107.14795},
  eprint = {2107.14795},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/reyvababtista/Projects/Papers/jaeglePerceiverIOGeneral2022.pdf}
}

@article{jiangImprovedYOLOv52022,
  title = {Improved {{YOLO}} v5 with Balanced Feature Pyramid and Attention Module for Traffic Sign Detection},
  author = {Jiang, Linfeng and Liu, Hui and Zhu, Hong and Zhang, Guangjian},
  editor = {Zhu, J.},
  year = {2022},
  journal = {MATEC Web Conf.},
  volume = {355},
  pages = {03023},
  issn = {2261-236X},
  doi = {10.1051/matecconf/202235503023},
  urldate = {2023-04-14},
  abstract = {With the development of automatic driving technology, traffic sign detection has become a very important task. However, it is a challenging task because of the complex traffic sign scene and the small size of the target. In recent years, a number of convolutional neural network (CNN) based object detection methods have brought great progress to traffic sign detection. Considering the still high false detection rate, as well as the high time overhead and computational overhead, the effect is not satisfactory. Therefore, we employ lightweight network model YOLO v5 (You Only Look Once) as our work foundation. In this paper, we propose an improved YOLO v5 method by using balances feature pyramid structure and global context block to enhance the ability of feature fusion and feature extraction. To verify our proposed method, we have conducted a lot of comparative experiments on the challenging dataset Tsinghua-Tencent-100K (TT100K). The experimental results demonstrate that the mAP@.5 and mAP@.5:0.95 are improved by 1.9\% and 2.1\%, respectively.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/jiangImprovedYOLOv52022.pdf}
}

@article{jiaoSurveyDeepLearningbased2019,
  title = {A {{Survey}} of {{Deep Learning-based Object Detection}}},
  author = {Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  eprint = {1907.09408},
  primaryclass = {cs},
  pages = {128837--128868},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2939201},
  urldate = {2023-04-11},
  abstract = {Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in peoples life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning networks for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline, thoroughly and deeply, in this survey, we first analyze the methods of existing typical detection models and describe the benchmark datasets. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/jiaoSurveyDeepLearningbased2019.pdf}
}

@misc{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/kingmaAutoEncodingVariationalBayes2022.pdf}
}

@article{kochSiameseNeuralNetworks,
  title = {Siamese {{Neural Networks}} for {{One-shot Image Recognition}}},
  author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
  abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/kochSiameseNeuralNetworks.pdf}
}

@article{kohonenselforganizingmap1990,
  title = {The Self-Organizing Map},
  author = {Kohonen, T.},
  year = {Sept./1990},
  journal = {Proc. IEEE},
  volume = {78},
  number = {9},
  pages = {1464--1480},
  issn = {00189219},
  doi = {10.1109/5.58325},
  urldate = {2023-09-05},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/kohonenselforganizingmap1990.pdf}
}

@article{kosterSnakemakescalablebioinformatics2012,
  title = {Snakemake\textemdash a Scalable Bioinformatics Workflow Engine},
  author = {K{\"o}ster, Johannes and Rahmann, Sven},
  year = {2012},
  month = oct,
  journal = {Bioinformatics},
  volume = {28},
  number = {19},
  pages = {2520--2522},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/bts480},
  urldate = {2023-03-22},
  abstract = {Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/kosterSnakemakescalablebioinformatics2012.pdf}
}

@article{kumarCenterExcellenceMobile2017,
  title = {Center of {{Excellence}} for {{Mobile Sensor Data-to-Knowledge}} ({{MD2K}})},
  author = {Kumar, Santosh and Abowd, Gregory and Abraham, William T. and {al'Absi}, Mustafa and Chau, Duen Horng and Ertin, Emre and Estrin, Deborah and Ganesan, Deepak and Hnat, Timothy and Hossain, Syed Monowar and Ives, Zachary and Kerr, Jacqueline and Marlin, Benjamin M. and Murphy, Susan and Rehg, James M. and {Nahum-Shani}, Inbal and Shetty, Vivek and Sim, Ida and Spring, Bonnie and Srivastava, Mani and Wetter, Dave},
  year = {2017},
  month = apr,
  journal = {IEEE Pervasive Comput.},
  volume = {16},
  number = {2},
  pages = {18--22},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2017.29},
  urldate = {2023-03-27},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/kumarCenterExcellenceMobile2017.pdf}
}

@misc{kumarMyCrunchGPTchatGPTassisted2023,
  title = {{{MyCrunchGPT}}: {{A chatGPT}} Assisted Framework for Scientific Machine Learning},
  shorttitle = {{{MyCrunchGPT}}},
  author = {Kumar, Varun and Gleyzer, Leonard and Kahana, Adar and Shukla, Khemraj and Karniadakis, George Em},
  year = {2023},
  month = jul,
  number = {arXiv:2306.15551},
  eprint = {2306.15551},
  primaryclass = {physics},
  publisher = {{arXiv}},
  urldate = {2023-09-07},
  abstract = {Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still timeconsuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate MyCrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of MyCrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the MyCrunchGPT, and create an infrastructure that can facilitate a broader vision, we built a webapp based guided user interface, that includes options for a comprehensive summary report. The overall objective is to extend MyCrunchGPT to handle diverse problems in computational mechanics, design, optimization and controls, and general scientific computing tasks involved in SciML, hence using it as a research assistant tool but also as an educational tool. While here the examples focus in fluid mechanics, future versions will target solid mechanics and materials science, geophysics, systems biology and bioinformatics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Physics - Physics and Society},
  file = {/Users/reyvababtista/Projects/Papers/kumarMyCrunchGPTchatGPTassisted2023.pdf}
}

@article{lecunDeeplearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2023-10-13},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/lecunDeeplearning2015.pdf}
}

@inproceedings{leeDataProcessingPipeline2023,
  title = {Data {{Processing Pipeline}} of {{Short-Term Depression Detection}} with {{Large-Scale Dataset}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  author = {Lee, Yonggeon and Noh, Youngtae and Lee, Uichin},
  year = {2023},
  month = feb,
  pages = {391--392},
  publisher = {{IEEE}},
  address = {{Jeju, Korea, Republic of}},
  doi = {10.1109/BigComp57234.2023.00095},
  urldate = {2023-03-26},
  abstract = {Depression is a common, recurring mental disorder that causes significant impairment in people's lives. In recent years, ubiquitous computing using mobile phones can monitor behavioral patterns relevant to depressive symptoms in-the-wild. In this paper, we propose data processing pipeline of short-term depression detection using mobile sensor data. We build a group model classified by depression severity for capturing depressive mood in a short-period time to handle data quality and data imbalance problem in a large-scale dataset. We expect the group model to identify and characterize digital phenotype representing each depressive group as a middle step toward personalization.},
  isbn = {978-1-66547-578-5},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/leeDataProcessingPipeline2023.pdf}
}

@inproceedings{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year = {2017},
  month = jul,
  pages = {936--944},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.106},
  urldate = {2023-05-08},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/linFeaturePyramidNetworks2017.pdf}
}

@misc{lingoRoleChatGPTDemocratizing2023,
  title = {The {{Role}} of {{ChatGPT}} in {{Democratizing Data Science}}: {{An Exploration}} of {{AI-facilitated Data Analysis}} in {{Telematics}}},
  shorttitle = {The {{Role}} of {{ChatGPT}} in {{Democratizing Data Science}}},
  author = {Lingo, Ryan},
  year = {2023},
  month = jul,
  number = {arXiv:2308.02045},
  eprint = {2308.02045},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-07},
  abstract = {The realm of data science, once reserved for specialists, is undergoing a revolution with the rapid emergence of generative AI, particularly through tools like ChatGPT. This paper posits ChatGPT as a pivotal bridge, drastically lowering the steep learning curve traditionally associated with complex data analysis. By generating intuitive data narratives and offering real-time assistance, ChatGPT democratizes the field, enabling a wider audience to glean insights from intricate datasets. A notable illustration of this transformative potential is provided through the examination of a synthetically generated telematics dataset, wherein ChatGPT aids in distilling complex patterns and insights. However, the journey to democratization is not without its hurdles. The paper delves into challenges presented by such AI, from potential biases in analysis to ChatGPT's limited reasoning capabilities. While the promise of a democratized data science landscape beckons, it is imperative to approach this transition with caution, cognizance, and an ever-evolving understanding of the tool's capabilities and constraints.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/reyvababtista/Projects/Papers/lingoRoleChatGPTDemocratizing2023.pdf}
}

@article{linTransferLearningBased2018,
  title = {Transfer {{Learning Based Traffic Sign Recognition Using Inception-v3 Model}}},
  author = {Lin, Chunmian and Li, Lin and Luo, Wenting and Wang, Kelvin C. P. and Guo, Jiangang},
  year = {2018},
  month = aug,
  journal = {Period. Polytech. Transp. Eng.},
  volume = {47},
  number = {3},
  pages = {242--250},
  issn = {1587-3811, 0303-7800},
  doi = {10.3311/PPtr.11480},
  urldate = {2023-03-20},
  abstract = {Traffic sign recognition is critical for advanced driver assistant system and road infrastructure survey. Traditional traffic sign recognition algorithms can't efficiently recognize traffic signs due to its limitation, yet deep learning-based technique requires huge amount of training data before its use, which is time consuming and labor intensive. In this study, transfer learning-based method is introduced for traffic sign recognition and classification, which significantly reduces the amount of training data and alleviates computation expense using Inception-v3 model. In our experiment, Belgium Traffic Sign Database is chosen and augmented by data pre-processing technique. Subsequently the layer-wise features extracted using different convolution and pooling operations are compared and analyzed. Finally transfer learning-based model is repetitively retrained several times with fine-tuning parameters at different learning rate, and excellent reliability and repeatability are observed based on statistical analysis. The results show that transfer learning model can achieve a high-level recognition performance in traffic sign recognition, which is up to 99.18 \% of recognition accuracy at 0.05 learning rate (average accuracy of 99.09 \%). This study would be beneficial in other traffic infrastructure recognition such as road lane marking and roadside protection facilities, and so on.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/linTransferLearningBased2018.pdf}
}

@article{liuIncorporatingChatGPTFinancial2023,
  title = {Incorporating {{ChatGPT}} into a {{Financial Data Science Course}} with {{Python Programming}}},
  author = {Liu, Yang and Miller, Laura and Niu, Xu},
  year = {2023},
  journal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4412371},
  urldate = {2023-06-16},
  abstract = {ChatGPT's artificial intelligence (AI) has generated considerable debate and even panic in higher education. However, AI is an unavoidable trend in education that has the potential to subvert many courses' pedagogical structure and methods. One such course is financial data science with Python programming, a course that is in high demand among finance majors. Incorporating ChatGPT into such a course can solve longstanding challenges and difficulties that instructors and students are facing, and it can shift the focus of the course from tedious Python coding back to financial data interpretations and real-world applications. This paper represents a first attempt at welcoming ChatGPT into financial education, by focusing on the pedagogical innovation of a financial data science course with Python programming. In particular, the paper provides specific examples of potential projects, with other insights and suggestions for using ChatGPT in data science courses for finance majors.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuIncorporatingChatGPTFinancial2023.pdf}
}

@article{liuMachineVisionBased2019,
  title = {Machine {{Vision Based Traffic Sign Detection Methods}}: {{Review}}, {{Analyses}} and {{Perspectives}}},
  shorttitle = {Machine {{Vision Based Traffic Sign Detection Methods}}},
  author = {Liu, Chunsheng and Li, Shuang and Chang, Faliang and Wang, Yinhai},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {86578--86596},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2924947},
  urldate = {2023-03-20},
  abstract = {Traffic signs recognition (TSR) is an important part of some advanced driver-assistance systems (ADASs) and auto driving systems (ADSs). As the first key step of TSR, traffic sign detection (TSD) is a challenging problem because of different types, small sizes, complex driving scenes, and occlusions. In recent years, there have been a large number of TSD algorithms based on machine vision and pattern recognition. In this paper, a comprehensive review of the literature on TSD is presented. We divide the reviewed detection methods into five main categories: color-based methods, shape-based methods, color- and shape-based methods, machine-learning-based methods, and LIDAR-based methods. The methods in each category are also classified into different subcategories for understanding and summarizing the mechanisms of different methods. For some reviewed methods that lack comparisons on public datasets, we reimplemented part of these methods for comparison. The experimental comparisons and analyses are presented on the reported performance and the performance of our reimplemented methods. Furthermore, future directions and recommendations of the TSD research are given to promote the development of the TSD.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuMachineVisionBased2019.pdf}
}

@article{liuMRCNNMultiScaleRegionBased2019,
  title = {{{MR-CNN}}: {{A Multi-Scale Region-Based Convolutional Neural Network}} for {{Small Traffic Sign Recognition}}},
  shorttitle = {{{MR-CNN}}},
  author = {Liu, Zhigang and Du, Juan and Tian, Feng and Wen, Jiazheng},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {57120--57128},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2913882},
  urldate = {2023-03-20},
  abstract = {Small traffic sign recognition is a challenging problem in computer vision, and its accuracy is important to the safety of intelligent transportation systems (ITS). In this paper, we propose the multiscale region-based convolutional neural network (MR-CNN). At the detection stage, MR-CNN uses a multiscale deconvolution operation to up-sample the features of the deeper convolution layers and concatenates them to those of the shallow layer to construct the fused feature map. The fused feature map has the ability to generate fewer region proposals and achieve higher recall values. At the classification stage, we leverage the multi-scale contextual regions to exploit the information surrounding a given object proposal and construct the fused feature for the fully connected layers. The fused feature map inside the region proposal network (RPN) focuses primarily on improving the image resolution and semantic information for small traffic sign detection, while outside the RPN, the fused feature enhances the feature representation by leveraging the contextual information. Finally, we evaluated MR-CNN on the largest dataset, TsinghuaTencent 100K, which is suitable for our problem and more challenging than the GTSDB and GTSRB datasets. The final experimental results indicate that the MR-CNN is superior at detecting small traffic signs, and that it achieves the state-of-the-art performance compared with other methods.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuMRCNNMultiScaleRegionBased2019.pdf}
}

@misc{liuPathAggregationNetwork2018,
  title = {Path {{Aggregation Network}} for {{Instance Segmentation}}},
  author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
  year = {2018},
  month = sep,
  number = {arXiv:1803.01534},
  eprint = {1803.01534},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/liuPathAggregationNetwork2018.pdf}
}

@article{liuTSingNetScaleawarecontextrich2021,
  title = {{{TSingNet}}: {{Scale-aware}} and Context-Rich Feature Learning for Traffic Sign Detection and Recognition in the Wild},
  shorttitle = {{{TSingNet}}},
  author = {Liu, Yuanyuan and Peng, Jiyao and Xue, Jing-Hao and Chen, Yongquan and Fu, Zhang-Hua},
  year = {2021},
  month = aug,
  journal = {Neurocomputing},
  volume = {447},
  pages = {10--22},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.03.049},
  urldate = {2023-03-20},
  abstract = {Traffic sign detection and recognition in the wild is a challenging task. Existing techniques are often incapable of detecting small or occluded traffic signs because of the scale variation and context loss, which causes semantic gaps between multiple scales. We propose a new traffic sign detection network (TSingNet), which learns scale-aware and context-rich features to effectively detect and recognize small and occluded traffic signs in the wild. Specifically, TSingNet first constructs an attention-driven bilateral feature pyramid network, which draws on both bottom-up and top-down subnets to dually circulate low-, mid-, and high-level foreground semantics in scale self-attention learning. This is to learn scaleaware foreground features and thus narrow down the semantic gaps between multiple scales. An adaptive receptive field fusion block with variable dilation rates is then introduced to exploit context-rich representation and suppress the influence of occlusion at each scale. TSingNet is end-to-end trainable by joint minimization of the scale-aware loss and multi-branch fusion losses, this adds a few parameters but significantly improves the detection performance. In extensive experiments with three challenging traffic sign datasets (TT100K, STSD and DFG), TSingNet outperformed state-of-the-art methods for traffic sign detection and recognition in the wild.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuTSingNetScaleawarecontextrich2021.pdf}
}

@techreport{louDCYOLOv8SmallSize2023,
  type = {Preprint},
  title = {{{DC-YOLOv8}}: {{Small Size Object Detection Algorithm Based}} on {{Camera Sensor}}},
  shorttitle = {{{DC-YOLOv8}}},
  author = {Lou, Haitong and Duan, Xuehu and Guo, Junmei and Liu, Haiying and Gu, Jason and Bi, Lingyun and Chen, Haonan},
  year = {2023},
  month = apr,
  institution = {{Engineering}},
  doi = {10.20944/preprints202304.0124.v1},
  urldate = {2023-04-14},
  abstract = {Traditional camera sensors rely on human eyes for observation. However, the human eye is prone to fatigue when observing targets of different sizes for a long time in complex scenes, and human cognition is limited, which often leads to judgment errors and greatly reduces the efficiency. Target recognition technology is an important technology to judge the target category in camera sensor. In order to solve this problem, a small size target detection algorithm for special scenarios was proposed by this paper. Its advantage is that this algorithm not only has higher precision for small size target detection, but also can ensure that the detection accuracy of each size is not lower than the existing algorithm. In this paper, a new down-sampling method was proposed, which could better preserve the context feature information. The feature fusion network was improved to effectively combine shallow information and deep information. A new network structure was proposed to effectively improve the detection accuracy of the model. In terms of accuracy, it is better than: YOLOX, YOLOXR, YOLOv3, scaled YOLOv5, YOLOv7-Tiny and YOLOv8.Three authoritative public data sets were used in this experiment: a) On Visdron data sets (small size targets), DC-YOLOv8 is 2.5\% more accurate than YOLOv8. b) On Tinyperson data sets (minimal size targets), DC-YOLOv8 is 1\% more accurate than YOLOv8. c) On PASCAL VOC2007 data sets (Normal size target), DC-YOLOv8 is 0.5\% more accurate than YOLOv8.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/louDCYOLOv8SmallSize2023.pdf}
}

@article{macdonaldCanChatGPTdraft2023,
  title = {Can {{ChatGPT}} Draft a Research Article? {{An}} Example of Population-Level Vaccine Effectiveness Analysis},
  shorttitle = {Can {{ChatGPT}} Draft a Research Article?},
  author = {Macdonald, Calum and Adeloye, Davies and Sheikh, Aziz and Rudan, Igor},
  year = {2023},
  month = feb,
  journal = {J Glob Health},
  volume = {13},
  pages = {01003},
  issn = {2047-2978, 2047-2986},
  doi = {10.7189/jogh.13.01003},
  urldate = {2023-08-21},
  abstract = {Background: The COVID-19 pandemic has had a significant impact on health care workers worldwide. Vaccination has been identified as a key strategy to reduce the risk of severe illness and death among health care workers, but the effectiveness of vaccination in preventing hospitalization among health care workers is not well understood. Methods: We used a simulated data set of 100\,000 health care workers to estimate the effect of vaccination on the risk of hospitalization among health care workers during the COVID-19 pandemic. The data set included information on the health care workers' ages, body mass index (BMI), and number of comorbidities. We simulated infections with a small probability of hospitalization due to COVID-19 and a subset of individuals were vaccinated with a fictional vaccine that reduced the risk of hospitalization after infection. We performed a survival analysis using the Cox proportional hazards model to estimate the hazard ratio of hospitalization among vaccinated health care workers compared with unvaccinated health care workers. Results: The hazard ratio (HR) for vaccination status was 0.48 (95\% confidence interval [CI] 0.28-0.86), indicating that the risk of hospitalization among vaccinated health care workers was about half that of unvaccinated health care workers. Other covariates in the model did not have a statistically significant effect on the hazard of hospitalization. Conclusion: Our study indicates that vaccination is associated with a significantly reduced risk of hospitalization among health care workers during the COVID-19 pandemic. These results support the use of vaccination as a key strategy to reduce the risk of severe illness and death among health care workers.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/macdonaldCanChatGPTdraft2023.pdf}
}

@article{maddiganChat2VISGeneratingData2023,
  title = {{{Chat2VIS}}: {{Generating Data Visualizations}} via {{Natural Language Using ChatGPT}}, {{Codex}} and {{GPT-3 Large Language Models}}},
  shorttitle = {{{Chat2VIS}}},
  author = {Maddigan, Paula and Susnjak, Teo},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {45181--45193},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3274199},
  urldate = {2023-08-30},
  abstract = {The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/maddiganChat2VISGeneratingData2023.pdf}
}

@article{merikangasRealtimeMobileMonitoring2019,
  title = {Real-Time {{Mobile Monitoring}} of the {{Dynamic Associations Among Motor Activity}}, {{Energy}}, {{Mood}}, and {{Sleep}} in {{Adults With Bipolar Disorder}}},
  author = {Merikangas, Kathleen Ries and Swendsen, Joel and Hickie, Ian B. and Cui, Lihong and Shou, Haochang and Merikangas, Alison K. and Zhang, Jihui and Lamers, Femke and Crainiceanu, Ciprian and Volkow, Nora D. and Zipunnikov, Vadim},
  year = {2019},
  month = feb,
  journal = {JAMA Psychiatry},
  volume = {76},
  number = {2},
  pages = {190},
  issn = {2168-622X},
  doi = {10.1001/jamapsychiatry.2018.3546},
  urldate = {2023-03-26},
  abstract = {OBJECTIVES To examine the directional associations among motor activity, energy, mood, and sleep using mobile monitoring in a community-identified sample, and to evaluate whether these within-day associations differ between people with a history of bipolar or other mood disorders and controls without mood disorders. DESIGN, SETTING, AND PARTICIPANTS This study used a nested case-control design of 242 adults, a subsample of a community-based sample of adults. Probands were recruited by mail from the greater Washington, DC, metropolitan area from January 2005 to June 2013. Enrichment of the sample for mood disorders was provided by volunteers or referrals from the National Institutes of Health Clinical Center or by participants in the National Institute of Mental Health Mood and Anxiety Disorders Program. The inclusion criteria were the ability to speak English, availability to participate, and consent to contact at least 2 living first-degree relatives. Data analysis was performed from June 2013 through July 2018. MAIN OUTCOMES AND MEASURES Motor activity and sleep duration data were obtained from minute-to-minute activity counts from an actigraphy device worn on the nondominant wrist for 2 weeks. Mood and energy levels were assessed by subjective analogue ratings on the ecological momentary assessment (using a personal digital assistant) by participants 4 times per day for 2 weeks. RESULTS Of the total 242 participants, 92 (38.1\%) were men and 150 (61.9\%) were women, with a mean (SD) age of 48 (16.9) years. Among the participants, 54 (22.3\%) had bipolar disorder (25 with bipolar I; 29 with bipolar II), 91 (37.6\%) had major depressive disorder, and 97 (40.1\%) were controls with no history of mood disorders. A unidirectional association was found between motor activity and subjective mood level ({$\beta$} = \textendash 0.018, P = .04). Bidirectional associations were observed between motor activity ({$\beta$} = 0.176; P = .03) and subjective energy level ({$\beta$} = 0.027; P = .03) as well as between motor activity ({$\beta$} = \textendash 0.027; P = .04) and sleep duration ({$\beta$} = \textendash 0.154; P = .04). Greater cross-domain reactivity was observed in bipolar disorder across all outcomes, including motor activity, sleep, mood, and energy. CONCLUSIONS AND RELEVANCE These findings suggest that interventions focused on motor activity and energy may have greater efficacy than current approaches that target depressed mood; both active and passive tracking of multiple regulatory systems are important in designing therapeutic targets.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/merikangasRealtimeMobileMonitoring2019.pdf}
}

@misc{mialonAugmentedLanguageModels2023,
  title = {Augmented {{Language Models}}: A {{Survey}}},
  shorttitle = {Augmented {{Language Models}}},
  author = {Mialon, Gr{\'e}goire and Dess{\`i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and {Dwivedi-Yu}, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.07842},
  eprint = {2302.07842},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-29},
  abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/mialonAugmentedLanguageModels2023.pdf}
}

@inproceedings{morrisonTigerAwareInnovativeMobile2018,
  title = {{{TigerAware}}: {{An Innovative Mobile Survey}} and {{Sensor Data Collection}} and {{Analytics System}}},
  shorttitle = {{{TigerAware}}},
  booktitle = {2018 {{IEEE Third International Conference}} on {{Data Science}} in {{Cyberspace}} ({{DSC}})},
  author = {Morrison, William and Guerdan, Luke and Kanugo, Jayanth and Trull, Timothy and Shang, Yi},
  year = {2018},
  month = jun,
  pages = {115--122},
  publisher = {{IEEE}},
  address = {{Guangzhou}},
  doi = {10.1109/DSC.2018.00025},
  urldate = {2023-03-22},
  abstract = {From health to community assessment, mobile phones have become a cornerstone of data collection across many areas of research. However, mobile phone-based studies are difficult to develop and deploy, often requiring in house development teams and large portions of research budgets. In this paper, an innovative system called TigerAware is presented to address this issue. TigerAware is developed to offer a generic and customizable tool, which allows researchers to create surveys to collect a wide range of data, including but not limited to question responses, on device sensor data, such as GPS data, and external sensor data, such as blood alcohol level from a Bluetooth breathalyzer. TigerAware is highly modular and uses advanced Web and mobile technologies to incorporate diverse data sources with a rich set of survey question types, requiring little development work by researchers for their individualized studies. TigerAware has been applied to a focus group and several pilot studies and shown excellent capabilities to be easily adapted and deployed for new types of data collection and analytics tasks and a wide range of research fields.},
  isbn = {978-1-5386-4210-8},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/morrisonTigerAwareInnovativeMobile2018.pdf}
}

@article{noeverNumeracyLiteracyData2023,
  title = {Numeracy from {{Literacy}}: {{Data Science}} as an {{Emergent Skill}} from {{Large Language Models}}},
  author = {Noever, David and McKee, Forrest},
  year = {2023},
  month = jan,
  abstract = {Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique testbeds for exploring the translation challenges of turning literacy into numeracy. Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic. The statistical analysis of four complex datasets described here combines arithmetic manipulations that cannot be memorized or encoded by simple rules. The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding. For example, the work highlights cases for descriptive statistics on in-memory datasets that the LLM initially loads from memory or generates randomly using python libraries. The resulting exploratory data analysis showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression. To extend the model's testable range, the research deletes and appends random rows such that recall alone cannot explain emergent numeracy.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/noeverNumeracyLiteracyData.pdf}
}

@misc{onnela-labForest2023,
  title = {Forest},
  author = {{onnela-lab}},
  year = {2023},
  month = mar,
  journal = {Forest},
  urldate = {2023-03-27},
  howpublished = {https://forest.beiwe.org/en/latest/}
}

@article{onnelaBeiwedatacollection2021,
  title = {Beiwe: {{A}} Data Collection Platform for High-Throughput Digital Phenotyping},
  shorttitle = {Beiwe},
  author = {Onnela, Jukka-Pekka and Dixon, Caleb and Griffin, Keary and Jaenicke, Tucker and Minowada, Leila and Esterkin, Sean and Siu, Alvin and Zagorsky, Josh and Jones, Eli},
  year = {2021},
  month = dec,
  journal = {JOSS},
  volume = {6},
  number = {68},
  pages = {3417},
  issn = {2475-9066},
  doi = {10.21105/joss.03417},
  urldate = {2023-03-23},
  abstract = {Beiwe is a high-throughput data collection platform for smartphone-based digital phenotyping. It has been in development and use since 2013. Beiwe consists of two native front-end applications: one for Android (written in Java and Kotlin) and one for iOS (written in Swift and Objective-C). The Beiwe back-end, which is based on Amazon Web Services (AWS), has been implemented primarily in Python 3.6, but it also makes use of Django for ORM and Flask for API and web servers. It uses several AWS services, such as S3 for flat file storage, EC2 virtual servers for data processing, Elastic Beanstalk for orchestration, and RDS for PostgreSQL database engine. Most smartphone applications use software development kits (SDKs) that generate unvalidated behavioral summary measures using closed proprietary algorithms. These applications do not meet the high standards of reproducible science, and often require researchers to modify their scientific questions based on what data happens to be available. In contrast, Beiwe collects raw sensor and phone use data, and its data collection parameters can be customized to address specific scientific questions of interest. Collection of raw data also improves reproducibility of studies and enables re-analyses of data and pooling of data across studies. Every aspect of Beiwe data collection is fully customizable, including which sensors to sample, how frequently to sample them, whether to add Gaussian noise to GPS location, whether to use Wi-Fi or cellular data for uploads, how frequently to upload data, specification of surveys and their response options, and skip logic. All study settings are captured in a JSON-formatted configuration file, which can be exported from and imported to Beiwe to enhance transparency and reproducibility of studies.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/onnelaBeiwedatacollection2021.pdf}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-21},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/openaiGPT4TechnicalReport2023.pdf}
}

@misc{openaiIntroducingChatGPT2022,
  title = {Introducing {{ChatGPT}}},
  author = {{OpenAI}},
  year = {2022},
  month = nov,
  journal = {Introducing ChatGPT},
  urldate = {2023-10-04}
}

@misc{openmmlabMMOCR2023,
  title = {{{MMOCR}}},
  author = {{OpenMMLab}},
  year = {2023},
  month = may,
  journal = {MMOCR},
  urldate = {2023-05-04},
  howpublished = {https://github.com/open-mmlab/mmocr}
}

@article{orrCentreCognitiveScience,
  title = {Centre for {{Cognitive Science}}, {{University}} of {{Edinburgh}}, 2, {{Buccleuch Place}}, {{Edinburgh EH8 9LW}}, {{Scotland April}} 1996},
  author = {Orr, Mark J L},
  abstract = {This document is an introduction to radial basis function (RBF) networks, a type of arti cial neural network for application to problems of supervised learning (e.g. regression, classi cation and time series prediction). It is now only available in PostScript2 (an older and now unsupported hyper-text version3 may be available for a while longer).},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/orrCentreCognitiveScience.pdf}
}

@misc{paddlepaddlePaddleOCR2023,
  title = {{{PaddleOCR}}},
  author = {{PaddlePaddle}},
  year = {2023},
  month = may,
  journal = {PaddleOCR},
  urldate = {2023-05-04},
  howpublished = {https://github.com/PaddlePaddle/PaddleOCR}
}

@misc{paperswithcodeHandwritingRecognition2023,
  title = {Handwriting {{Recognition}}},
  author = {{Papers with Code}},
  year = {2023},
  month = may,
  journal = {Handwriting Recognition},
  urldate = {2023-05-04},
  howpublished = {https://paperswithcode.com/task/handwriting-recognition}
}

@misc{paperswithcodeOpticalCharacterRecognition2023,
  title = {Optical {{Character Recognition}} ({{OCR}})},
  author = {{Papers with Code}},
  year = {2023},
  month = may,
  journal = {Optical Character Recognition (OCR)},
  urldate = {2023-05-04},
  howpublished = {https://paperswithcode.com/task/optical-character-recognition}
}

@misc{pythonsoftwarefoundationPython2023,
  title = {Python},
  author = {{Python Software Foundation}},
  year = {2023},
  month = may,
  journal = {Python},
  urldate = {2023-05-04},
  howpublished = {https://www.python.org}
}

@article{rabbiIncreasingEngagementSubstance2018,
  title = {Toward {{Increasing Engagement}} in {{Substance Use Data Collection}}: {{Development}} of the {{Substance Abuse Research Assistant App}} and {{Protocol}} for a {{Microrandomized Trial Using Adolescents}} and {{Emerging Adults}}},
  shorttitle = {Toward {{Increasing Engagement}} in {{Substance Use Data Collection}}},
  author = {Rabbi, Mashfiqui and Philyaw Kotov, Meredith and Cunningham, Rebecca and Bonar, Erin E and {Nahum-Shani}, Inbal and Klasnja, Predrag and Walton, Maureen and Murphy, Susan},
  year = {2018},
  month = jul,
  journal = {JMIR Res Protoc},
  volume = {7},
  number = {7},
  pages = {e166},
  issn = {1929-0748},
  doi = {10.2196/resprot.9850},
  urldate = {2023-03-26},
  abstract = {Background: Substance use is an alarming public health issue associated with significant morbidity and mortality. Adolescents and emerging adults are at particularly high risk because substance use typically initiates and peaks during this developmental period. Mobile health apps are a promising data collection and intervention delivery tool for substance-using youth as most teens and young adults own a mobile phone. However, engagement with data collection for most mobile health applications is low, and often, large fractions of users stop providing data after a week of use.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/rabbiIncreasingEngagementSubstance2018.pdf}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/radfordImprovingLanguageUnderstanding.pdf}
}

@article{rayChatGPTcomprehensivereview2023,
  title = {{{ChatGPT}}: {{A}} Comprehensive Review on Background, Applications, Key Challenges, Bias, Ethics, Limitations and Future Scope},
  shorttitle = {{{ChatGPT}}},
  author = {Ray, Partha Pratim},
  year = {2023},
  journal = {Internet of Things and Cyber-Physical Systems},
  volume = {3},
  pages = {121--154},
  issn = {26673452},
  doi = {10.1016/j.iotcps.2023.04.003},
  urldate = {2023-09-07},
  abstract = {In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/rayChatGPTcomprehensivereview2023.pdf}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  pages = {779--788},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.91},
  urldate = {2023-04-26},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/redmonYouOnlyLook2016.pdf}
}

@inproceedings{rogersDeepLearningYour2019,
  title = {Deep {{Learning}} at {{Your Fingertips}}},
  booktitle = {2019 16th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  author = {Rogers, Jonathan and Simmons, Dylan and Shah, Milesh and Rowland, Connor and Shang, Yi},
  year = {2019},
  month = jan,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CCNC.2019.8651868},
  urldate = {2023-03-22},
  abstract = {From SurveyMonkey to Google Forms, online surveys have become a cornerstone of modern research. However, these survey platforms lack the ability to provide advanced analysis to researchers, often requiring expensive third-party analytics software to rectify their shortcomings. We propose to solve this problem by adding data analysis capabilities onto TigerAware, an existing data collection platform. TigerAware offers a generic and customizable tool which allows researchers without technical expertise to create, manage, and deploy custom mobile surveys to participants in real-time. We seek to add data analysis functionalities to the TigerAware platform ranging from basic statistics functions to emotion recognition via deep learning. Our analysis platform uses data collected by TigerAware to give researchers real-time analytics throughout the duration of their study. Through our additions to the TigerAware platform, we present a novel all-in-one tool for researchers to facilitate effective survey creation, survey administration, data collection, and data analysis.},
  isbn = {978-1-5386-5553-5},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/rogersDeepLearningYour2019.pdf}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/ronnebergerUNetConvolutionalNetworks2015.pdf}
}

@misc{ropenscipdftools2023,
  title = {Pdftools},
  author = {{rOpenSci}},
  year = {2023},
  month = may,
  journal = {pdftools},
  urldate = {2023-05-04},
  howpublished = {https://docs.ropensci.org/pdftools/}
}

@misc{ropenscitesseract2023,
  title = {Tesseract},
  author = {{rOpenSci}},
  year = {2023},
  month = may,
  journal = {tesseract},
  urldate = {2023-05-04},
  howpublished = {https://docs.ropensci.org/tesseract/}
}

@misc{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = nov,
  number = {arXiv:1710.09829},
  eprint = {1710.09829},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/sabourDynamicRoutingCapsules2017.pdf}
}

@misc{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-29},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/reyvababtista/Projects/Papers/schickToolformerLanguageModels2023.pdf}
}

@misc{shenHuggingGPTSolvingAI2023,
  title = {{{HuggingGPT}}: {{Solving AI Tasks}} with {{ChatGPT}} and Its {{Friends}} in {{Hugging Face}}},
  shorttitle = {{{HuggingGPT}}},
  author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = may,
  number = {arXiv:2303.17580},
  eprint = {2303.17580},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-21},
  abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence 2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/shenHuggingGPTSolvingAI2023.pdf}
}

@article{shiffmanEcologicalMomentaryAssessment2008,
  title = {Ecological {{Momentary Assessment}}},
  author = {Shiffman, Saul and Stone, Arthur A. and Hufford, Michael R.},
  year = {2008},
  month = apr,
  journal = {Annu. Rev. Clin. Psychol.},
  volume = {4},
  number = {1},
  pages = {1--32},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev.clinpsy.3.022806.091415},
  urldate = {2023-03-24},
  abstract = {Assessment in clinical psychology typically relies on global retrospective self-reports collected at research or clinic visits, which are limited by recall bias and are not well suited to address how behavior changes over time and across contexts. Ecological momentary assessment (EMA) involves repeated sampling of subjects' current behaviors and experiences in real time, in subjects' natural environments. EMA aims to minimize recall bias, maximize ecological validity, and allow study of microprocesses that influence behavior in real-world contexts. EMA studies assess particular events in subjects' lives or assess subjects at periodic intervals, often by random time sampling, using technologies ranging from written diaries and telephones to electronic diaries and physiological sensors. We discuss the rationale for EMA, EMA designs, methodological and practical issues, and comparisons of EMA and recall data. EMA holds unique promise to advance the science and practice of clinical psychology by shedding light on the dynamics of behavior in real-world settings.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/shiffmanEcologicalMomentaryAssessment2008.pdf}
}

@article{straczkiewiczonesizefitsmostwalkingrecognition2023,
  title = {A ``One-Size-Fits-Most'' Walking Recognition Method for Smartphones, Smartwatches, and Wearable Accelerometers},
  author = {Straczkiewicz, Marcin and Huang, Emily J. and Onnela, Jukka-Pekka},
  year = {2023},
  month = feb,
  journal = {npj Digit. Med.},
  volume = {6},
  number = {1},
  pages = {29},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00745-z},
  urldate = {2023-03-27},
  abstract = {Abstract             The ubiquity of personal digital devices offers unprecedented opportunities to study human behavior. Current state-of-the-art methods quantify physical activity using ``activity counts,'' a measure which overlooks specific types of physical activities. We propose a walking recognition method for sub-second tri-axial accelerometer data, in which activity classification is based on the inherent features of walking: intensity, periodicity, and duration. We validate our method against 20 publicly available, annotated datasets on walking activity data collected at various body locations (thigh, waist, chest, arm, wrist). We demonstrate that our method can estimate walking periods with high sensitivity and specificity: average sensitivity ranged between 0.92 and 0.97 across various body locations, and average specificity for common daily activities was typically above 0.95. We also assess the method's algorithmic fairness to demographic and anthropometric variables and measurement contexts (body location, environment). Finally, we release our method as open-source software in Python and MATLAB.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/straczkiewiczonesizefitsmostwalkingrecognition2023.pdf}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/sutskeverSequenceSequenceLearning2014.pdf}
}

@article{tabernikDeepLearningLargeScale2020,
  title = {Deep {{Learning}} for {{Large-Scale Traffic-Sign Detection}} and {{Recognition}}},
  author = {Tabernik, Domen and Skocaj, Danijel},
  year = {2020},
  month = apr,
  journal = {IEEE Trans. Intell. Transport. Syst.},
  volume = {21},
  number = {4},
  pages = {1427--1440},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2019.2913588},
  urldate = {2023-03-20},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/tabernikDeepLearningLargeScale2020.pdf}
}

@article{temelTrafficSignDetection2020,
  title = {Traffic {{Sign Detection}} under {{Challenging Conditions}}: {{A Deeper Look Into Performance Variations}} and {{Spectral Characteristics}}},
  shorttitle = {Traffic {{Sign Detection}} under {{Challenging Conditions}}},
  author = {Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
  year = {2020},
  month = sep,
  journal = {IEEE Trans. Intell. Transport. Syst.},
  volume = {21},
  number = {9},
  eprint = {1908.11262},
  primaryclass = {cs, eess},
  pages = {3663--3673},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2019.2931429},
  urldate = {2023-03-20},
  abstract = {Traffic signs are critical for maintaining the safety and efficiency of our roads. Therefore, we need to carefully assess the capabilities and limitations of automated traffic sign detection systems. Existing traffic sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. To overcome the shortcomings in existing datasets, we introduced the CURE-TSDReal dataset, which is based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. We test the performance of two benchmark algorithms and show that severe conditions can result in an average performance degradation of 29\% in precision and 68\% in recall. We investigate the effect of challenging conditions through spectral analysis and show that challenging conditions can lead to distinct magnitude spectrum characteristics. Moreover, we show that mean magnitude spectrum of changes in video sequences under challenging conditions can be an indicator of detection performance. CURE-TSD-Real dataset is available online at https://github.com/olivesgatech/CURE-TSD.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,I.2,I.4,I.5},
  file = {/Users/reyvababtista/Projects/Papers/temelTrafficSignDetection2020.pdf}
}

@misc{tervenComprehensiveReviewYOLO2023,
  title = {A {{Comprehensive Review}} of {{YOLO}}: {{From YOLOv1}} to {{YOLOv8}} and {{Beyond}}},
  shorttitle = {A {{Comprehensive Review}} of {{YOLO}}},
  author = {Terven, Juan and {Cordova-Esparza}, Diana},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00501},
  eprint = {2304.00501},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-10},
  abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO to YOLOv8. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/tervenComprehensiveReviewYOLO2023.pdf}
}

@misc{tesseractTesseractOpenSource2023,
  title = {Tesseract {{Open Source OCR Engine}}},
  author = {{tesseract}},
  year = {2023},
  month = may,
  journal = {Tesseract Open Source OCR Engine},
  urldate = {2023-05-04},
  howpublished = {https://github.com/tesseract-ocr/tesseract}
}

@misc{therfoundationProjectStatisticalComputing2023,
  title = {The {{R Project}} for {{Statistical Computing}}},
  author = {{The R Foundation}},
  year = {2023},
  month = may,
  journal = {The R Project for Statistical Computing},
  urldate = {2023-05-04},
  howpublished = {https://www.r-project.org}
}

@misc{tiongPlugandPlayVQAZeroshot2023,
  title = {Plug-and-{{Play VQA}}: {{Zero-shot VQA}} by {{Conjoining Large Pretrained Models}} with {{Zero Training}}},
  shorttitle = {Plug-and-{{Play VQA}}},
  author = {Tiong, Anthony Meng Huat and Li, Junnan and Li, Boyang and Savarese, Silvio and Hoi, Steven C. H.},
  year = {2023},
  month = mar,
  number = {arXiv:2210.08773},
  eprint = {2210.08773},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-02},
  abstract = {Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNPVQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate questionguided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 (Goyal et al., 2017) and GQA (Hudson and Manning, 2019). With 11B parameters, it outperforms the 80Bparameter Flamingo model (Alayrac et al., 2022) by 8.5\% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1\% on GQA over FewVLM (Jin et al., 2022) with 740M PLM parameters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/tiongPlugandPlayVQAZeroshot2023.pdf}
}

@article{turakhiaRationaledesignlargescale2019,
  title = {Rationale and Design of a Large-Scale, App-Based Study to Identify Cardiac Arrhythmias Using a Smartwatch: {{The Apple Heart Study}}},
  shorttitle = {Rationale and Design of a Large-Scale, App-Based Study to Identify Cardiac Arrhythmias Using a Smartwatch},
  author = {Turakhia, Mintu P. and Desai, Manisha and Hedlin, Haley and Rajmane, Amol and Talati, Nisha and Ferris, Todd and Desai, Sumbul and Nag, Divya and Patel, Mithun and Kowey, Peter and Rumsfeld, John S. and Russo, Andrea M. and Hills, Mellanie True and Granger, Christopher B. and Mahaffey, Kenneth W. and Perez, Marco V.},
  year = {2019},
  month = jan,
  journal = {American Heart Journal},
  volume = {207},
  pages = {66--75},
  issn = {00028703},
  doi = {10.1016/j.ahj.2018.09.002},
  urldate = {2023-03-26},
  abstract = {Background Smartwatch and fitness band wearable consumer electronics can passively measure pulse rate from the wrist using photoplethysmography (PPG). Identification of pulse irregularity or variability from these data has the potential to identify atrial fibrillation or atrial flutter (AF, collectively). The rapidly expanding consumer base of these devices allows for detection of undiagnosed AF at scale. Methods The Apple Heart Study is a prospective, single arm pragmatic study that has enrolled 419,093 participants (NCT03335800). The primary objective is to measure the proportion of participants with an irregular pulse detected by the Apple Watch (Apple Inc, Cupertino, CA) with AF on subsequent ambulatory ECG patch monitoring. The secondary objectives are to: 1) characterize the concordance of pulse irregularity notification episodes from the Apple Watch with simultaneously recorded ambulatory ECGs; 2) estimate the rate of initial contact with a health care provider within 3 months after notification of pulse irregularity. The study is conducted virtually, with screening, consent and data collection performed electronically from within an accompanying smartphone app. Study visits are performed by telehealth study physicians via video chat through the app, and ambulatory ECG patches are mailed to the participants. Conclusions The results of this trial will provide initial evidence for the ability of a smartwatch algorithm to identify pulse irregularity and variability which may reflect previously unknown AF. The Apple Heart Study will help provide a foundation for how wearable technology can inform the clinical approach to AF identification and screening. (Am Heart J 2019;207:66-75.)},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/turakhiaRationaledesignlargescale2019.pdf}
}

@misc{ultralyticsTrainCustomData2023,
  title = {Train {{Custom Data}}},
  author = {{Ultralytics}},
  year = {2023},
  month = may,
  journal = {Train Custom Data},
  urldate = {2023-05-09}
}

@misc{ultralyticsYOLOv52023,
  title = {{{YOLOv5}}},
  author = {{Ultralytics}},
  year = {2023},
  month = may,
  journal = {YOLOv5},
  urldate = {2023-05-09}
}

@inproceedings{vaizmanExtraSensoryAppData2018,
  title = {{{ExtraSensory App}}: {{Data Collection In-the-Wild}} with {{Rich User Interface}} to {{Self-Report Behavior}}},
  shorttitle = {{{ExtraSensory App}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Vaizman, Yonatan and Ellis, Katherine and Lanckriet, Gert and Weibel, Nadir},
  year = {2018},
  month = apr,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Montreal QC Canada}},
  doi = {10.1145/3173574.3174128},
  urldate = {2023-03-26},
  abstract = {We introduce a mobile app for collecting in-the-wild data, including sensor measurements and self-reported labels describing people's behavioral context (e.g. driving, eating, in class, shower). Labeled data is necessary for developing contextrecognition systems that serve health monitoring, aging care, and more. Acquiring labels without observers is challenging and previous solutions compromised ecological validity, range of behaviors, or amount of data. Our user interface combines past and near-future self-reporting of combinations of relevant context-labels. We deployed the app on the personal smartphones of 60 users and analyzed quantitative data collected in-the-wild and qualitative user-experience reports. The interface's flexibility was important to gain frequent, detailed labels, support diverse behavioral situations, and engage different users: most preferred reporting their past behavior through a daily journal, but some preferred reporting what they're about to do. We integrated insights from this work back into the app, which we make available to researchers for conducting in-the-wild studies.},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/vaizmanExtraSensoryAppData2018.pdf}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-05},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/vaswaniAttentionAllYou2023.pdf}
}

@article{vegaReproducibleAnalysisPipeline2021,
  title = {Reproducible {{Analysis Pipeline}} for {{Data Streams}}: {{Open-Source Software}} to {{Process Data Collected With Mobile Devices}}},
  shorttitle = {Reproducible {{Analysis Pipeline}} for {{Data Streams}}},
  author = {Vega, Julio and Li, Meng and Aguillera, Kwesi and Goel, Nikunj and Joshi, Echhit and Khandekar, Kirtiraj and Durica, Krina C. and Kunta, Abhineeth R. and Low, Carissa A.},
  year = {2021},
  month = nov,
  journal = {Front. Digit. Health},
  volume = {3},
  pages = {769823},
  issn = {2673-253X},
  doi = {10.3389/fdgth.2021.769823},
  urldate = {2023-03-22},
  abstract = {Smartphone and wearable devices are widely used in behavioral and clinical research to collect longitudinal data that, along with ground truth data, are used to create models of human behavior. Mobile sensing researchers often program data processing and analysis code from scratch even though many research teams collect data from similar mobile sensors, platforms, and devices. This leads to significant inefficiency in not being able to replicate and build on others' work, inconsistency in quality of code and results, and lack of transparency when code is not shared alongside publications. We provide an overview of Reproducible Analysis Pipeline for Data Streams (RAPIDS), a reproducible pipeline to standardize the preprocessing, feature extraction, analysis, visualization, and reporting of data streams coming from mobile sensors. RAPIDS is formed by a group of R and Python scripts that are executed on top of reproducible virtual environments, orchestrated by a workflow management system, and organized following a consistent file structure for data science projects. We share open source, documented, extensible and tested code to preprocess, extract, and visualize behavioral features from data collected with any Android or iOS smartphone sensing app as well as Fitbit and Empatica wearable devices. RAPIDS allows researchers to process mobile sensor data in a rigorous and reproducible way. This saves time and effort during the data analysis phase of a project and facilitates sharing analysis workflows alongside publications.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/vegaReproducibleAnalysisPipeline2021.pdf}
}

@article{wangHOPESIntegrativeDigital2021,
  title = {{{HOPES}}: {{An Integrative Digital Phenotyping Platform}} for {{Data Collection}}, {{Monitoring}}, and {{Machine Learning}}},
  shorttitle = {{{HOPES}}},
  author = {Wang, Xuancong and Vouk, Nikola and Heaukulani, Creighton and Buddhika, Thisum and Martanto, Wijaya and Lee, Jimmy and Morris, Robert JT},
  year = {2021},
  month = mar,
  journal = {J Med Internet Res},
  volume = {23},
  number = {3},
  pages = {e23984},
  issn = {1438-8871},
  doi = {10.2196/23984},
  urldate = {2023-03-27},
  abstract = {The collection of data from a personal digital device to characterize current health conditions and behaviors that determine how an individual's health will evolve has been called digital phenotyping. In this paper, we describe the development of and early experiences with a comprehensive digital phenotyping platform: Health Outcomes through Positive Engagement and Self-Empowerment (HOPES). HOPES is based on the open-source Beiwe platform but adds a wider range of data collection, including the integration of wearable devices and further sensor collection from smartphones. Requirements were partly derived from a concurrent clinical trial for schizophrenia that required the development of significant capabilities in HOPES for security, privacy, ease of use, and scalability, based on a careful combination of public cloud and on-premises operation. We describe new data pipelines to clean, process, present, and analyze data. This includes a set of dashboards customized to the needs of research study operations and clinical care. A test use case for HOPES was described by analyzing the digital behavior of 22 participants during the SARS-CoV-2 pandemic.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/wangHOPESIntegrativeDigital2021.pdf}
}

@article{wuMultimodaldatacollection2021,
  title = {Multi-Modal Data Collection for Measuring Health, Behavior, and Living Environment of Large-Scale Participant Cohorts},
  author = {Wu, Congyu and Fritz, Hagen and Bastami, Sepehr and Maestre, Juan P and Thomaz, Edison and Julien, Christine and Castelli, Darla M and {de~Barbaro}, Kaya and Bearman, Sarah Kate and Harari, Gabriella M and Cameron~Craddock, R and Kinney, Kerry A and Gosling, Samuel D and Schnyer, David M and Nagy, Zoltan},
  year = {2021},
  month = jun,
  journal = {GigaScience},
  volume = {10},
  number = {6},
  pages = {giab044},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giab044},
  urldate = {2023-04-11},
  abstract = {Abstract                            Background               As mobile technologies become ever more sensor-rich, portable, and ubiquitous, data captured by smart devices are lending rich insights into users' daily lives with unprecedented comprehensiveness and ecological validity. A number of human-subject studies have been conducted to examine the use of mobile sensing to uncover individual behavioral patterns and health outcomes, yet minimal attention has been placed on measuring living environments together with other human-centered sensing data. Moreover, the participant sample size in most existing studies falls well below a few hundred, leaving questions open about the reliability of findings on the relations between mobile sensing signals and human outcomes.                                         Results               To address these limitations, we developed a home environment sensor kit for continuous indoor air quality tracking and deployed it in conjunction with smartphones, Fitbits, and ecological momentary assessments in a cohort study of up to 1,584 college student participants per data type for 3 weeks. We propose a conceptual framework that systematically organizes human-centric data modalities by their temporal coverage and spatial freedom. Then we report our study procedure, technologies and methods deployed, and descriptive statistics of the collected data that reflect the participants' mood, sleep, behavior, and living environment.                                         Conclusions               We were able to collect from a large participant cohort satisfactorily complete multi-modal sensing and survey data in terms of both data continuity and participant adherence. Our novel data and conceptual development provide important guidance for data collection and hypothesis generation in future human-centered sensing studies.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/wuMultimodaldatacollection2021.pdf}
}

@inproceedings{xiaoSupportingQualitativeAnalysis2023,
  title = {Supporting {{Qualitative Analysis}} with {{Large Language Models}}: {{Combining Codebook}} with {{GPT-3}} for {{Deductive Coding}}},
  shorttitle = {Supporting {{Qualitative Analysis}} with {{Large Language Models}}},
  booktitle = {28th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Xiao, Ziang and Yuan, Xingdi and Liao, Q. Vera and Abdelghani, Rania and Oudeyer, Pierre-Yves},
  year = {2023},
  month = mar,
  eprint = {2304.10548},
  primaryclass = {cs},
  pages = {75--78},
  doi = {10.1145/3581754.3584136},
  urldate = {2023-06-28},
  abstract = {Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However, this process is often labor-intensive, particularly when working with large datasets. While recent AI-based tools demonstrate utility, researchers may not have readily available AI resources and expertise, let alone be challenged by the limited generalizability of those task-specific models. In this study, we explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes. Instead of training task-specific models, a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning. Using a curiosity-driven questions coding task as a case study, we found, by combining GPT-3 with expert-drafted codebooks, our proposed approach achieved fair to substantial agreements with expert-coded results. We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/reyvababtista/Projects/Papers/xiaoSupportingQualitativeAnalysis2023.pdf}
}

@misc{yangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  year = {2023},
  month = sep,
  number = {arXiv:2309.03409},
  eprint = {2309.03409},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-13},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/yangLargeLanguageModels2023.pdf}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-29},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/yaoReActSynergizingReasoning2023.pdf}
}

@article{yuanFasterLightDetection2023,
  title = {Faster {{Light Detection Algorithm}} of {{Traffic Signs Based}} on {{YOLOv5s-A2}}},
  author = {Yuan, Xu and Kuerban, Alifu and Chen, Yixiao and Lin, Wenlong},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {19395--19404},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3204818},
  urldate = {2023-04-10},
  abstract = {Traffic sign recognition systems have been applied to advanced driving assistance and automatic driving systems to help drivers obtain important road information accurately. The current mainstream detection methods have high accuracy in this task, but the number of model parameters is large, and the detection speed is slow. Based on YOLOv5s as the basic framework, this paper proposes YOLOv5S-A2, which can improve the detection speed and reduce the model size at the cost of reducing the detection accuracy. Firstly, a data augmentation strategy is proposed by combining various operations to alleviate the problem of unbalanced class instances. Secondly, we proposed a path aggregation module for Feature Pyramid Network (FPN) to make new horizontal connections. It can enhance multi-scale feature representation capability and compensate for the loss of feature information. Thirdly, an attention detection head module is proposed to solve the aliasing effect in cross-scale fusion and enhance the representation of predictive features. Experiments on Tsinghua-Tencent 100K dataset (TT100K) show that our method can achieve more remarkable performance improvement and faster inference speed than other advanced technologies. Our method achieves 87.3\% mean average precision (mAP), surpassing the original model's 7.9\%, and the frames per second (FPS) value is maintained at 87.7. To show generality, we tested it on the German Traffic Sign Detection Benchmark (GTSDB) without tuning and obtained an average precision of 94.1\%, and the FPS value is maintained at about 105.3. In addition, the number of YOLOv5s-A2 parameters is about 7.9 M.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/yuanFasterLightDetection2023.pdf}
}

@misc{yuCoCaContrastiveCaptioners2022,
  title = {{{CoCa}}: {{Contrastive Captioners}} Are {{Image-Text Foundation Models}}},
  shorttitle = {{{CoCa}}},
  author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01917},
  eprint = {2205.01917},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-28},
  abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-theart performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and new state-of-the-art 91.0\% top-1 accuracy on ImageNet with a finetuned encoder.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/Users/reyvababtista/Projects/Papers/yuCoCaContrastiveCaptioners2022.pdf}
}

@article{yuPredictingquantitycannabis2023,
  title = {Predicting Quantity of Cannabis Smoked in Daily Life: {{An}} Exploratory Study Using Machine Learning},
  shorttitle = {Predicting Quantity of Cannabis Smoked in Daily Life},
  author = {Yu, Ching-Yun and Shang, Yi and Hough, Tionna M. and Bokshan, Anthony L. and Fleming, Megan N. and Haney, Alison M. and Trull, Timothy J.},
  year = {2023},
  month = nov,
  journal = {Drug and Alcohol Dependence},
  volume = {252},
  pages = {110964},
  issn = {03768716},
  doi = {10.1016/j.drugalcdep.2023.110964},
  urldate = {2023-10-06},
  abstract = {Background: Cannabis use is prevalent in the United States and is associated with a host of negative consequences. Importantly, a robust indicator of negative consequences is the amount of cannabis consumed. Methods: Data were obtained from fifty-two adult, regular cannabis flower users (3+ times per week) recruited from the community; participants completed multiple ecological momentary assessment (EMA) surveys each day for 14 days. In this exploratory study, we used various machine learning algorithms to build models to predict the amount of cannabis smoked since participants' last report including forty-three EMA measures of mood, impulsivity, pain, alcohol use, cigarette use, craving, cannabis potency, cannabis use motivation, subjective effects of cannabis, social context, and location in daily life. Results: Our best-fitting model (Gradient Boosted Trees; 71.15\% accuracy, 72.46\% precision) found that affects, subjective effects of cannabis, and cannabis use motives were among the best predictors of cannabis use amount in daily life. The social context of being with others, and particularly with a partner or friend, was moderately weighted in the final prediction model, but contextual items reflecting location were not strongly weighted in the final prediction model, the one exception being not at work. Conclusions: Machine learning approaches can help identify additional environmental and psychological phe\- nomena that may be clinically-relevant to cannabis use.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/yuPredictingquantitycannabis2023.pdf}
}

@inproceedings{yuReproducibleWorkflowsExploring2022,
  title = {Reproducible {{Workflows}} for {{Exploring}} and {{Modeling EMA Data}}},
  booktitle = {2022 {{IEEE}} 8th {{International Conference}} on {{Collaboration}} and {{Internet Computing}} ({{CIC}})},
  author = {Yu, Ching-Yun and Shang, Yi and Trull, Timothy},
  year = {2022},
  month = dec,
  pages = {117--124},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1109/CIC56439.2022.00026},
  urldate = {2023-03-22},
  abstract = {Improper use of substances like cannabis may lead to physical, emotional, economic, and social problems. Therefore, it is significant to elucidate the inter-individual and intraindividual influences along with contextual influences that predict the use of cannabis. TigerAware is a mobile survey data collection platform that holds unique promise to advance research in addiction and substance use. This paper presents a novel method to support Ecological Momentary Assessment (EMA) studies. We propose to extract useful information from TigerAware survey data using data mining and machine learning methods, and structure customizable survey analyses into reproducible workflows. Through our analysis pipeline for EMA, researchers are able to discover meaningful information from survey data with minimal duplication of effort and improve the efficiency and rigor of the process.},
  isbn = {978-1-66547-300-2},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/yuReproducibleWorkflowsExploring2022.pdf}
}

@misc{zengVLMAllInOnePretrained2023,
  title = {X\$\^2\$-{{VLM}}: {{All-In-One Pre-trained Model For Vision-Language Tasks}}},
  shorttitle = {X\$\^2\$-{{VLM}}},
  author = {Zeng, Yan and Zhang, Xinsong and Li, Hang and Wang, Jiawei and Zhang, Jipeng and Zhou, Wangchunshu},
  year = {2023},
  month = jul,
  number = {arXiv:2211.12402},
  eprint = {2211.12402},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-28},
  abstract = {Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multigrained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X2-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X2-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X2-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X2-VLM results in high transferability for it to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X2-VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training. The code and pre-trained models are available at github.com/zengyan-97/X2-VLM.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/zengVLMAllInOnePretrained2023.pdf}
}

@article{zhangAutomatedVisualRecognizability2019,
  title = {Automated {{Visual Recognizability Evaluation}} of {{Traffic Sign Based}} on {{3D LiDAR Point Clouds}}},
  author = {Zhang, Shanxin and Wang, Cheng and Lin, Lili and Wen, Chenglu and Yang, Chenhui and Zhang, Zhemin and Li, Jonathan},
  year = {2019},
  month = jun,
  journal = {Remote Sensing},
  volume = {11},
  number = {12},
  pages = {1453},
  issn = {2072-4292},
  doi = {10.3390/rs11121453},
  urldate = {2023-03-20},
  abstract = {Maintaining the high visual recognizability of traffic signs for traffic safety is a key matter for road network management. Mobile Laser Scanning (MLS) systems provide efficient way of 3D measurement over large-scale traffic environment. This paper presents a quantitative visual recognizability evaluation method for traffic signs in large-scale traffic environment based on traffic recognition theory and MLS 3D point clouds. We first propose the Visibility Evaluation Model (VEM) to quantitatively describe the visibility of traffic sign from any given viewpoint, then we proposed the concept of visual recognizability field and Traffic Sign Visual Recognizability Evaluation Model (TSVREM) to measure the visual recognizability of a traffic sign. Finally, we present an automatic TSVREM calculation algorithm for MLS 3D point clouds. Experimental results on real MLS 3D point clouds show that the proposed method is feasible and efficient.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/zhangAutomatedVisualRecognizability2019.pdf}
}

@misc{zhangAutoMLGPTAutomaticMachine2023,
  title = {{{AutoML-GPT}}: {{Automatic Machine Learning}} with {{GPT}}},
  shorttitle = {{{AutoML-GPT}}},
  author = {Zhang, Shujian and Gong, Chengyue and Wu, Lemeng and Liu, Xingchao and Zhou, Mingyuan},
  year = {2023},
  month = may,
  number = {arXiv:2305.02499},
  eprint = {2305.02499},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-21},
  abstract = {AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging AutoML-GPT's robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many AI tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/zhangAutoMLGPTAutomaticMachine2023.pdf}
}

@inproceedings{zhaoNewMethodUsing2023,
  title = {A {{New Method Using LLMs}} for {{Keypoints Generation}} in {{Qualitative Data Analysis}}},
  booktitle = {2023 {{IEEE Conference}} on {{Artificial Intelligence}} ({{CAI}})},
  author = {Zhao, Fengxiang and Yu, Fan and Trull, Timothy and Shang, Yi},
  year = {2023},
  month = jun,
  pages = {333--334},
  publisher = {{IEEE}},
  address = {{Santa Clara, CA, USA}},
  doi = {10.1109/CAI54212.2023.00147},
  urldate = {2023-08-30},
  abstract = {Qualitative data analysis (QDA) is useful for identifying patterns, themes, and relationships among data. In this paper, we propose a new method that uses large language models (LLMs), such as GPT-based Models, to improve QDA, in Ecological Momentary Assessment (EMA) studies as an example, by automating keypoints extraction and relevance evaluation. Experimental results on the IBM-ArgKP-2021 dataset show improved performance of the new method over existing work, achieving higher accuracy while reducing time and effort in the coding process of QDA, and demonstrate the effectiveness of our proposed method in various application settings.},
  isbn = {9798350339840},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/zhaoNewMethodUsing2023.pdf}
}

@misc{zhaTableGPTUnifyingTables2023,
  title = {{{TableGPT}}: {{Towards Unifying Tables}}, {{Nature Language}} and {{Commands}} into {{One GPT}}},
  shorttitle = {{{TableGPT}}},
  author = {Zha, Liangyu and Zhou, Junlin and Li, Liyao and Wang, Rui and Huang, Qingyi and Yang, Saisai and Yuan, Jing and Su, Changbao and Li, Xiang and Su, Aofeng and Zhang, Tao and Zhou, Chen and Shou, Kaizhe and Wang, Miao and Zhu, Wufang and Lu, Guoshan and Ye, Chao and Ye, Yali and Ye, Wentao and Zhang, Yiming and Deng, Xinglong and Xu, Jie and Wang, Haobo and Chen, Gang and Zhao, Junbo},
  year = {2023},
  month = aug,
  number = {arXiv:2307.08674},
  eprint = {2307.08674},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-08-30},
  abstract = {Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/zhaTableGPTUnifyingTables2023.pdf}
}

@article{zhouGraphneuralnetworks2020,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year = {2020},
  journal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2021.01.001},
  urldate = {2023-09-05},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/zhouGraphneuralnetworks2020.pdf}
}

@inproceedings{zhuTrafficSignDetectionClassification2016,
  title = {Traffic-{{Sign Detection}} and {{Classification}} in the {{Wild}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhu, Zhe and Liang, Dun and Zhang, Songhai and Huang, Xiaolei and Li, Baoli and Hu, Shimin},
  year = {2016},
  month = jun,
  pages = {2110--2118},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.232},
  urldate = {2023-03-20},
  abstract = {Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify trafficsigns. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available1.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/zhuTrafficSignDetectionClassification2016.pdf}
}

@misc{zouObjectDetection202023,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  year = {2023},
  month = jan,
  number = {arXiv:1905.05055},
  eprint = {1905.05055},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-11},
  abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today's object detection technique as a revolution driven by deep learning, then back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This paper extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/zouObjectDetection202023.pdf}
}
