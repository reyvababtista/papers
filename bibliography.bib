@article{bangquanRealTimeEmbeddedTraffic2019,
  title = {Real-{{Time Embedded Traffic Sign Recognition Using Efficient Convolutional Neural Network}}},
  author = {Bangquan, Xie and Xiao Xiong, Weng},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {53330--53346},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2912311},
  urldate = {2023-03-20},
  abstract = {Traffic sign recognition(TSR) based on deep learning is rapidly developing. Specifically, TSR contains two technologies, namely, traffic sign classification (TSC) and traffic sign detection (TSD). However, the challenge of TSR is to ensure its efficiency, which means adequate accuracy, generalization, and speed in real-time by a computationally limited platform. In this paper, we will introduce a new efficient TSC network called ENet (efficient network) and a TSD network called EmdNet (efficient network using multiscale operation and depthwise separable convolution). We used data mining and multiscale operation to improve the accuracy and generalization ability and used depthwise separable convolution (DSC) to improve the speed. The resulting ENet possesses 0.9 M parameters (1/15 the parameters of the start-of-the-art method) while still achieving an accuracy of 98.6 \% on the German Traffic Sign Recognition benchmark (GTSRB). In addition, we design EmdNet' s backbone network according to the principles of ENet. The EmdNet with the SDD Framework possesses only 6.3 M parameters, which is similar to MobileNet's scale.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bangquanRealTimeEmbeddedTraffic2019.pdf}
}

@inproceedings{bayzidiTrafficSignClassifiers2022,
  title = {Traffic {{Sign Classifiers Under Physical World Realistic Sticker Occlusions}}: {{A Cross Analysis Study}}},
  shorttitle = {Traffic {{Sign Classifiers Under Physical World Realistic Sticker Occlusions}}},
  booktitle = {2022 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Bayzidi, Yasin and Smajic, Alen and Huger, Fabian and Moritz, Ruby and Varghese, Serin and Schlicht, Peter and Knoll, Alois},
  year = {2022},
  month = jun,
  pages = {644--650},
  publisher = {{IEEE}},
  address = {{Aachen, Germany}},
  doi = {10.1109/IV51971.2022.9827143},
  urldate = {2023-03-20},
  abstract = {Recent adversarial attacks with real world applications are capable of deceiving deep neural networks (DNN), which often appear as printed stickers applied to objects in physical world. Though achieving high success rate in lab tests and limited field tests, such attacks have not been tested on multiple DNN architectures with a standard setup to unveil the common robustness and weakness points of both the DNNs and the attacks. Furthermore, realistic looking stickers applied by normal people as acts of vandalism are not studied to discover their potential risks as well the risk of optimizing the location of such realistic stickers to achieve the maximum performance drop. In this paper, (a) we study the case of realistic looking sticker application effects on traffic sign detectors performance; (b) we use traffic sign image classification as our use case and train and attack 11 of the modern architectures for our analysis; (c) by considering different factors like brightness, blurriness and contrast of the train images in our sticker application procedure, we show that simple image processing techniques can help realistic looking stickers fit into their background to mimic real world tests; (d) by performing structured synthetic and real-world evaluations, we study the difference of various traffic sign classes in terms of their crucial distinctive features among the tested DNNs.},
  isbn = {978-1-66548-821-1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bayzidiTrafficSignClassifiers2022.pdf}
}

@article{bentdigitalbiomarkerdiscovery2021,
  title = {The Digital Biomarker Discovery Pipeline: {{An}} Open-Source Software Platform for the Development of Digital Biomarkers Using {{mHealth}} and Wearables Data},
  shorttitle = {The Digital Biomarker Discovery Pipeline},
  author = {Bent, Brinnae and Wang, Ke and Grzesiak, Emilia and Jiang, Chentian and Qi, Yuankai and Jiang, Yihang and Cho, Peter and Zingler, Kyle and Ogbeide, Felix Ikponmwosa and Zhao, Arthur and Runge, Ryan and Sim, Ida and Dunn, Jessilyn},
  year = {2021},
  journal = {Journal of Clinical and Translational Science},
  volume = {5},
  number = {1},
  pages = {e19},
  issn = {2059-8661},
  doi = {10.1017/cts.2020.511},
  urldate = {2023-03-26},
  abstract = {Introduction: Digital health is rapidly expanding due to surging healthcare costs, deteriorating health outcomes, and the growing prevalence and accessibility of mobile health (mHealth) and wearable technology. Data from Biometric Monitoring Technologies (BioMeTs), including mHealth and wearables, can be transformed into digital biomarkers that act as indicators of health outcomes and can be used to diagnose and monitor a number of chronic diseases and conditions. There are many challenges faced by digital biomarker development, including a lack of regulatory oversight, limited funding opportunities, general mistrust of sharing personal data, and a shortage of open-source data and code. Further, the process of transforming data into digital biomarkers is computationally expensive, and standards and validation methods in digital biomarker research are lacking. Methods: In order to provide a collaborative, standardized space for digital biomarker research and validation, we present the first comprehensive, open-source software platform for end-to-end digital biomarker development: The Digital Biomarker Discovery Pipeline (DBDP). Results: Here, we detail the general DBDP framework as well as three robust modules within the DBDP that have been developed for specific digital biomarker discovery use cases. Conclusions: The clear need for such a platform will accelerate the DBDP's adoption as the industry standard for digital biomarker development and will support its role as the epicenter of digital biomarker collaboration and exploration.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bentdigitalbiomarkerdiscovery2021.pdf}
}

@article{bergerFieldEvaluationSmartphonebased2015,
  title = {Field {{Evaluation}} of the {{Smartphone-based Travel Behaviour Data Collection App}} ``{{SmartMo}}''},
  author = {Berger, Martin and Platzer, Mario},
  year = {2015},
  journal = {Transportation Research Procedia},
  volume = {11},
  pages = {263--279},
  issn = {23521465},
  doi = {10.1016/j.trpro.2015.12.023},
  urldate = {2023-03-26},
  abstract = {This paper outlines an innovative approach to the evaluation of a self-administered smartphone-based survey for the collection of travel behaviour data. For this approach, a traditional travel survey is modified to match mobile devices. The smartphone application ``SmartMo'' is designed in a multi-stage iterative development process. It is then implemented and evaluated through a number of field tests involving 97 participants. Results of the field evaluation will be discussed including the technical performance (e.g. secure data transfer and data management, energy consumption, map-matching), usability (e.g. comprehensibility, handling, joy of use) as well as user acceptance (e.g. willingness to participate, data protection and privacy). A brief overview of the SmartMo data collection system will also be provided.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/bergerFieldEvaluationSmartphonebased2015.pdf}
}

@inproceedings{chowDEMONSintegratedframework2016,
  title = {{{DEMONS}}: An Integrated Framework for Examining Associations between Physiology and Self-Reported Affect Tied to Depressive Symptoms},
  shorttitle = {{{DEMONS}}},
  booktitle = {Proceedings of the 2016 {{ACM International Joint Conference}} on {{Pervasive}} and {{Ubiquitous Computing}}: {{Adjunct}}},
  author = {Chow, Philip and Bonelli, Wesley and Huang, Yu and Fua, Karl and Teachman, Bethany A. and Barnes, Laura E.},
  year = {2016},
  month = sep,
  pages = {1139--1143},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2968219.2968300},
  urldate = {2023-03-26},
  abstract = {Depression is a prevalent and debilitating disorder among college students. Advances in mobile technology afford the opportunity to collect heterogeneous data while people are in their natural settings. The aim of the current paper is to propose an integrated framework, DEMONS (DEpression MONitoring Study), for combining passive and active data sources using a wearable sensor and a smartphone application. The ability to combine passive and active longitudinal data with mobile devices allows for better understanding of the temporal relations between self-reported affect and physiological variables (e.g., heart rate variability) linked to depressive symptoms. Adoption of the proposed framework will provide crucial information regarding the development and maintenance of depression in college students, as well as increased opportunities for early detection and intervention.},
  isbn = {978-1-4503-4462-3},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/chowDEMONSintegratedframework2016.pdf}
}

@article{cuiContextAwareBlockNet2022,
  title = {Context-{{Aware Block Net}} for {{Small Object Detection}}},
  author = {Cui, Lisha and Lv, Pei and Jiang, Xiaoheng and Gao, Zhimin and Zhou, Bing and Zhang, Luming and Shao, Ling and Xu, Mingliang},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Cybernetics},
  volume = {52},
  number = {4},
  pages = {2300--2313},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2020.3004636},
  urldate = {2023-03-21},
  abstract = {State-of-the-art object detectors usually progressively downsample the input image until it is represented by small feature maps, which loses the spatial information and compromises the representation of small objects. In this article, we propose a context-aware block net (CAB Net) to improve small object detection by building high-resolution and strong semantic feature maps. To internally enhance the representation capacity of feature maps with high spatial resolution, we delicately design the context-aware block (CAB). CAB exploits pyramidal dilated convolutions to incorporate multilevel contextual information without losing the original resolution of feature maps. Then, we assemble CAB to the end of the truncated backbone network (e.g., VGG16) with a relatively small downsampling factor (e.g., 8) and cast off all following layers. CAB Net can capture both basic visual patterns as well as semantical information of small objects, thus improving the performance of small object detection. Experiments conducted on the benchmark Tsinghua-Tencent 100K and the Airport dataset show that CAB Net outperforms other top-performing detectors by a large margin while keeping real-time speed, which demonstrates the effectiveness of CAB Net for small object detection.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/cuiContextAwareBlockNet2022.pdf}
}

@article{devriesSmartphoneBasedEcologicalMomentary2021,
  title = {Smartphone-{{Based Ecological Momentary Assessment}} of {{Well-Being}}: {{A Systematic Review}} and {{Recommendations}} for {{Future Studies}}},
  shorttitle = {Smartphone-{{Based Ecological Momentary Assessment}} of {{Well-Being}}},
  author = {{de Vries}, Lianne P. and Baselmans, Bart M. L. and Bartels, Meike},
  year = {2021},
  month = jun,
  journal = {Journal of Happiness Studies},
  volume = {22},
  number = {5},
  pages = {2361--2408},
  issn = {1389-4978, 1573-7780},
  doi = {10.1007/s10902-020-00324-7},
  urldate = {2023-03-24},
  abstract = {Feelings of well-being and happiness fluctuate over time and contexts. Ecological Momentary Assessment (EMA) studies can capture fluctuations in momentary behavior, and experiences by assessing these multiple times per day. Traditionally, EMA was performed using pen and paper. Recently, due to technological advances EMA studies can be conducted more easily with smartphones, a device ubiquitous in our society. The goal of this review was to evaluate the literature on smartphone-based EMA in well-being research in healthy subjects. The systematic review was conducted according to the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines. Searching PubMed and Web of Science, we identified 53 studies using smartphone-based EMA of wellbeing. Studies were heterogeneous in designs, context, and measures. The average study duration was 12.8 days, with well-being assessed 2\textendash 12 times per day. Half of the studies included objective data (e.g. location). Only 47.2\% reported compliance, indicating a mean of 71.6\%. Well-being fluctuated daily and weekly, with higher well-being in evenings and weekends. These fluctuations disappeared when location and activity were accounted for. On average, being in nature and physical activity relates to higher well-being. Working relates to lower well-being, but workplace and company do influence well-being. The important advantages of using smartphones instead of other devices to collect EMAs are the easier data collection and flexible designs. Smartphone-based EMA reach far larger maximum sample sizes and more easily add objective data to their designs than palm-top/ PDA studies. Smartphone-based EMA research is feasible to gain insight in well-being fluctuations and its determinants and offers the opportunity for parallel objective data collection. Most studies currently focus on group comparisons, while studies on individual differences in well-being patterns and fluctuations are lacking. We provide recommendations for future smartphone-based EMA research regarding measures, objective data and analyses.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/devriesSmartphoneBasedEcologicalMomentary2021.pdf}
}

@article{dewiYoloV4Advanced2021,
  title = {Yolo {{V4}} for {{Advanced Traffic Sign Recognition With Synthetic Training Data Generated}} by {{Various GAN}}},
  author = {Dewi, Christine and Chen, Rung-Ching and Liu, Yan-Ting and Jiang, Xiaoyi and Hartomo, Kristoko Dwi},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {97228--97242},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3094201},
  urldate = {2023-03-20},
  abstract = {Convolutional Neural Networks (CNN) achieves perfection in traffic sign identification with enough annotated training data. The dataset determines the quality of the complete visual system based on CNN. Unfortunately, databases for traffic signs from the majority of the world's nations are few. In this scenario, Generative Adversarial Networks (GAN) may be employed to produce more realistic and varied training pictures to supplement the actual arrangement of images. The purpose of this research is to describe how the quality of synthetic pictures created by DCGAN, LSGAN, and WGAN is determined. Our work combines synthetic images with original images to enhance datasets and verify the effectiveness of synthetic datasets. We use different numbers and sizes of images for training. Likewise, the Structural Similarity Index (SSIM) and Mean Square Error (MSE) were employed to assess picture quality. Our study quantifies the SSIM difference between the synthetic and actual images. When additional images are used for training, the synthetic image exhibits a high degree of resemblance to the genuine image. The highest SSIM value was achieved when using 200 total images as input and 32 \texttimes{} 32 image size. Further, we augment the original picture dataset with synthetic pictures and compare the original image model to the synthesis image model. For this experiment, we are using the latest iterations of Yolo, Yolo V3, and Yolo V4. After mixing the real image with the synthesized image produced by LSGAN, the recognition performance has been improved, achieving an accuracy of 84.9\% on Yolo V3 and an accuracy of 89.33\% on Yolo V4.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/dewiYoloV4Advanced2021.pdf}
}

@misc{doryabExtractionBehavioralFeatures2019,
  title = {Extraction of {{Behavioral Features}} from {{Smartphone}} and {{Wearable Data}}},
  author = {Doryab, Afsaneh and Chikarsel, Prerna and Liu, Xinwen and Dey, Anind K.},
  year = {2019},
  month = jan,
  number = {arXiv:1812.10394},
  eprint = {arXiv:1812.10394},
  publisher = {{arXiv}},
  urldate = {2023-03-27},
  abstract = {The rich set of sensors in smartphones and wearable devices provides the possibility to passively collect streams of data in the wild. The raw data streams, however, can rarely be directly used in the modeling pipeline. We provide a generic framework that can process raw data streams and extract useful features related to non-verbal human behavior. This framework can be used by researchers in the field who are interested in processing data from smartphones and Wearable devices.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/reyvababtista/Projects/Papers/doryabExtractionBehavioralFeatures2019.pdf}
}

@article{ferreiraAWAREMobileContext2015,
  title = {{{AWARE}}: {{Mobile Context Instrumentation Framework}}},
  shorttitle = {{{AWARE}}},
  author = {Ferreira, Denzil and Kostakos, Vassilis and Dey, Anind K.},
  year = {2015},
  month = apr,
  journal = {Frontiers in ICT},
  volume = {2},
  issn = {2297-198X},
  doi = {10.3389/fict.2015.00006},
  urldate = {2023-03-22},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/ferreiraAWAREMobileContext2015.pdf}
}

@article{fritzDatafusionmobile2022,
  title = {Data Fusion of Mobile and Environmental Sensing Devices to Understand the Effect of the Indoor Environment on Measured and Self-Reported Sleep Quality},
  author = {Fritz, Hagen and Kinney, Kerry A. and Wu, Congyu and Schnyer, David M. and Nagy, Zoltan},
  year = {2022},
  month = apr,
  journal = {Building and Environment},
  volume = {214},
  pages = {108835},
  issn = {03601323},
  doi = {10.1016/j.buildenv.2022.108835},
  urldate = {2023-04-11},
  abstract = {The Indoor Air Quality (IAQ) of the bedroom environment has recently garnered attention since air pollution can affect sleep. Previous studies investigated IAQ and sleep quality in controlled environments which impacts both self-reported and measured sleep quality. Studies within a participant's home environment are ecologically valid and reduce participant bias. Here, we study 20 participants over 77 days in Austin, TX. We monitored five components of IAQ using the BEVO Beacon, a calibrated purpose-built environmental monitor, and measured participant sleep quality through wearable activity trackers and 4-question surveys sent four times a week. We found significant decreases in sleep quality during nights with elevated CO, CO2, and temperature. Elevated CO was associated with a mean increase in 0.9 self-reported awakenings and decreases in device-measured sleep time of 21.6 min and sleep efficiency of 0.6\%. Increased CO2 and temperature were associated with decreases in device-measured sleep time of 17.5 and 15.2 min, respectively. Elevated PM2.5 and TVOCs concentrations were associated with overall improvements in sleep quality. Participants reported a mean of 4.4 fewer awakenings and had a 1.1\% increased in measured sleep efficiency for nights with elevated PM2.5. Elevated TVOCs were associated with an increase in sleep time of 14.5 min. These findings indicate a need to study the relationship between these aggregate IAQ measures and sleep quality more closely. Our results also indicate that pollutants can independently affect sleep quality regardless of the CO2 measurements. Compared to literature, our study is the longest and includes the most IAQ parameters.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/fritzDatafusionmobile2022.pdf}
}

@article{fritzEvaluatingmachinelearning2022,
  title = {Evaluating Machine Learning Models to Classify Occupants' Perceptions of Their Indoor Environment and Sleep Quality from Indoor Air Quality},
  author = {Fritz, Hagen and Tang, Mengjia and Kinney, Kerry and Nagy, Zoltan},
  year = {2022},
  month = dec,
  journal = {Journal of the Air \& Waste Management Association},
  volume = {72},
  number = {12},
  pages = {1381--1397},
  issn = {1096-2247, 2162-2906},
  doi = {10.1080/10962247.2022.2105439},
  urldate = {2023-04-11},
  abstract = {A variety of factors can affect a person's perception of their environment and health, but one factor that is often overlooked in indoor settings is the air quality. To address this gap, we develop and evaluate four Machine Learning (ML) models on two disparate datasets using Indoor Air Quality (IAQ) parameters as primary features and components of self-reported IAQ satisfaction and sleep quality as target variables. In each case, we compare models to each other as well as to a simple model that always predicts the majority outcome. In the first analysis, we use open-source data collected from 93 California residences to predict occupant's satisfaction with their indoor environ\- ment. Results indicate building ventilation rate, Relative Humidity (RH), and formaldehyde are most influential when predicting IAQ perception and do so with an accuracy greater than the simplified model. The second analysis uses IAQ data gathered from a field study we conducted with 20 participants over 11 weeks to train similar models. We obtain accuracy and F1 scores similar to the simplified model where PM2.5 and TVOCs represent the most important predictors. Our results underscore the ability of IAQ to affect a person's perception of their built environment and health and highlight the utility of ML models to explore the strength of these relationships.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/fritzEvaluatingmachinelearning2022.pdf}
}

@article{gaggiolimobiledatacollection2013,
  title = {A Mobile Data Collection Platform for Mental Health Research},
  author = {Gaggioli, Andrea and Pioggia, Giovanni and Tartarisco, Gennaro and Baldus, Giovanni and Corda, Daniele and Cipresso, Pietro and Riva, Giuseppe},
  year = {2013},
  month = feb,
  journal = {Personal and Ubiquitous Computing},
  volume = {17},
  number = {2},
  pages = {241--251},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-011-0465-2},
  urldate = {2023-03-29},
  abstract = {Ubiquitous computing technologies offer exciting new possibilities for monitoring and analyzing user's experience in real time. In this paper, we describe the design and development of Psychlog, a mobile phone platform designed to collect users' psychological, physiological, and activity information for mental health research. The tool allows administering self-report questionnaires at specific times or randomly within a day. The system also permits to collect heart rate and activity information from a wireless electrocardiogram equipped with a three-axial accelerometer. By combining self-reports with heart rate and activity data, the application makes it possible to investigate the relationship between psychological, physiological, and behavioral variables, as well as to monitor their fluctuations over time. The software runs on Windows mobile operative system and is available as open source (http://sourceforge. net/projects/psychlog/).},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/gaggiolimobiledatacollection22.pdf}
}

@inproceedings{guoDetectionOccludedRoad2019,
  title = {Detection of {{Occluded Road Signs}} on {{Autonomous Driving Vehicles}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Guo, Jingda and Cheng, Xianwei and Chen, Qi and Yang, Qing},
  year = {2019},
  month = jul,
  pages = {856--861},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/ICME.2019.00152},
  urldate = {2023-03-20},
  abstract = {Autonomous driving vehicle relies heavily on its perception system to sense surrounding environments and make driving decisions. One important task on autonomous driving vehicles is to correctly recognize different traffic signs. However, the traffic signs in the wild can be in various conditions, e.g., occluded, deteriorated, or vandalized, and not all of them are recognizable. In this work, we propose a novel system that leverages the perception system on autonomous vehicle to identify occluded road signs in real time. Based on transfer learning, we propose the occluded sign classification network (OSCN) that is able to achieve a precision of 96.34\% on a real-world dataset.},
  isbn = {978-1-5386-9552-4},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/guoDetectionOccludedRoad2019.pdf}
}

@article{harrisResearchelectronicdata2009,
  title = {Research Electronic Data Capture ({{REDCap}})\textemdash{{A}} Metadata-Driven Methodology and Workflow Process for Providing Translational Research Informatics Support},
  author = {Harris, Paul A. and Taylor, Robert and Thielke, Robert and Payne, Jonathon and Gonzalez, Nathaniel and Conde, Jose G.},
  year = {2009},
  month = apr,
  journal = {Journal of Biomedical Informatics},
  volume = {42},
  number = {2},
  pages = {377--381},
  issn = {15320464},
  doi = {10.1016/j.jbi.2008.08.010},
  urldate = {2023-03-29},
  abstract = {Research electronic data capture (REDCap) is a novel workflow methodology and software solution designed for rapid development and deployment of electronic data capture tools to support clinical and translational research. We present: (1) a brief description of the REDCap metadata-driven software toolset; (2) detail concerning the capture and use of study-related metadata from scientific research teams; (3) measures of impact for REDCap; (4) details concerning a consortium network of domestic and international institutions collaborating on the project; and (5) strengths and limitations of the REDCap system. REDCap is currently supporting 286 translational research projects in a growing collaborative network including 27 active partner institutions.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/harrisResearchelectronicdata22.pdf}
}

@article{hautHyperspectralImageClassification2019,
  title = {Hyperspectral {{Image Classification Using Random Occlusion Data Augmentation}}},
  author = {Haut, Juan Mario and Paoletti, Mercedes E. and Plaza, Javier and Plaza, Antonio and Plaza, Li},
  year = {2019},
  month = nov,
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {16},
  number = {11},
  pages = {1751--1755},
  issn = {1545-598X, 1558-0571},
  doi = {10.1109/LGRS.2019.2909495},
  urldate = {2023-03-20},
  abstract = {Convolutional neural networks (CNNs) have become a powerful tool for remotely sensed hyperspectral image (HSI) classification due to their great generalization ability and high accuracy. However, owing to the huge amount of parameters that need to be learned and to the complex nature of HSI data itself, these approaches must deal with the important problem of overfitting, which can lead to inadequate generalization and loss of accuracy. In order to mitigate this problem, in this letter, we adopt random occlusion, a recently developed data augmentation (DA) method for training CNNs, in which the pixels of different rectangular spatial regions in the HSI are randomly occluded, generating training images with various levels of occlusion and reducing the risk of overfitting. Our results with two well-known HSIs reveal that the proposed method helps to achieve better classification accuracy with low computational cost.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hautHyperspectralImageClassification2019.pdf}
}

@inproceedings{hicksAndWellnessopenmobile2010,
  title = {{{AndWellness}}: An Open Mobile System for Activity and Experience Sampling},
  shorttitle = {{{AndWellness}}},
  booktitle = {Wireless {{Health}} 2010},
  author = {Hicks, John and Ramanathan, Nithya and Kim, Donnie and Monibi, Mohamad and Selsky, Joshua and Hansen, Mark and Estrin, Deborah},
  year = {2010},
  month = oct,
  pages = {34--43},
  publisher = {{ACM}},
  address = {{San Diego California}},
  doi = {10.1145/1921081.1921087},
  urldate = {2023-03-29},
  abstract = {Advances in mobile phone technology have allowed phones to become a convenient platform for real-time assessment of a participants health and behavior. AndWellness, a personal data collection system, uses mobile phones to collect and analyze data from both active, triggered user experience samples and passive logging of onboard environmental sensors. The system includes an application that runs on Android based mobile phones, server software that manages deployments and acts as a central repository for data, and a dashboard front end for both participants and researchers to visualize incoming data in real-time. Our system has gone through testing by researchers in preparation for deployment with participants, and we describe an initial qualitative study plus several planned future studies to demonstrate how our system can be used to better understand a user's health related habits and observations.},
  isbn = {978-1-60558-989-3},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hicksAndWellnessopenmobile22.pdf}
}

@article{hiltyFrameworkCompetenciesUse2020,
  title = {A {{Framework}} for {{Competencies}} for the {{Use}} of {{Mobile Technologies}} in {{Psychiatry}} and {{Medicine}}: {{Scoping Review}}},
  shorttitle = {A {{Framework}} for {{Competencies}} for the {{Use}} of {{Mobile Technologies}} in {{Psychiatry}} and {{Medicine}}},
  author = {Hilty, Donald and Chan, Steven and Torous, John and Luo, John and Boland, Robert},
  year = {2020},
  month = feb,
  journal = {JMIR mHealth and uHealth},
  volume = {8},
  number = {2},
  pages = {e12229},
  issn = {2291-5222},
  doi = {10.2196/12229},
  urldate = {2023-03-26},
  abstract = {Background: To ensure quality care, clinicians need skills, knowledge, and attitudes related to technology that can be measured. Objective: This paper sought out competencies for mobile technologies and/or an approach to define them. Methods: A scoping review was conducted to answer the following research question, ``What skills are needed for clinicians and trainees to provide quality care via mHealth, have they been published, and how can they be made measurable and reproducible to teach and assess them?'' The review was conducted in accordance with the 6-stage scoping review process starting with a keyword search in PubMed/Medical Literature Analysis and Retrieval System Online, APA PsycNET, Cochrane, EMBASE, PsycINFO, Web of Science, and Scopus. The literature search focused on keywords in 4 concept areas: (1) competencies, (2) mobile technologies, (3) telemedicine mode, and (4) health. Moreover, 2 authors independently, in parallel, screened the search results for potentially relevant studies based on titles and abstracts. The authors reviewed the full-text articles for final inclusion based on inclusion/exclusion criteria. Inclusion criteria were keywords used from concept area 1 (competencies) and 2 (mobile technologies) and either 3 (telemedicine mode) or 4 (health). Exclusion criteria included, but were not limited to, keywords used from a concept area in isolation, discussion of skills abstractly, outline or listing of what clinicians need without detail, and listing immeasurable behaviors.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/hiltyFrameworkCompetenciesUse2020.pdf}
}

@article{huckinsMentalHealthBehavior2020,
  title = {Mental {{Health}} and {{Behavior}} of {{College Students During}} the {{Early Phases}} of the {{COVID-19 Pandemic}}: {{Longitudinal Smartphone}} and {{Ecological Momentary Assessment Study}}},
  shorttitle = {Mental {{Health}} and {{Behavior}} of {{College Students During}} the {{Early Phases}} of the {{COVID-19 Pandemic}}},
  author = {Huckins, Jeremy F and {daSilva}, Alex W and Wang, Weichen and Hedlund, Elin and Rogers, Courtney and Nepal, Subigya K and Wu, Jialing and Obuchi, Mikio and Murphy, Eilis I and Meyer, Meghan L and Wagner, Dylan D and Holtzheimer, Paul E and Campbell, Andrew T},
  year = {2020},
  month = jun,
  journal = {Journal of Medical Internet Research},
  volume = {22},
  number = {6},
  pages = {e20185},
  issn = {1438-8871},
  doi = {10.2196/20185},
  urldate = {2023-03-23},
  abstract = {Background: The vast majority of people worldwide have been impacted by coronavirus disease (COVID-19). In addition to the millions of individuals who have been infected with the disease, billions of individuals have been asked or required by local and national governments to change their behavioral patterns. Previous research on epidemics or traumatic events suggests that this can lead to profound behavioral and mental health changes; however, researchers are rarely able to track these changes with frequent, near-real-time sampling or compare their findings to previous years of data for the same individuals.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/huckinsMentalHealthBehavior2020.pdf}
}

@article{jiaoSurveyDeepLearningbased2019,
  title = {A {{Survey}} of {{Deep Learning-based Object Detection}}},
  author = {Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  eprint = {1907.09408},
  primaryclass = {cs},
  pages = {128837--128868},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2939201},
  urldate = {2023-04-11},
  abstract = {Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in peoples life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning networks for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline, thoroughly and deeply, in this survey, we first analyze the methods of existing typical detection models and describe the benchmark datasets. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/jiaoSurveyDeepLearningbased2019.pdf}
}

@article{kosterSnakemakescalablebioinformatics2012,
  title = {Snakemake\textemdash a Scalable Bioinformatics Workflow Engine},
  author = {K{\"o}ster, Johannes and Rahmann, Sven},
  year = {2012},
  month = oct,
  journal = {Bioinformatics},
  volume = {28},
  number = {19},
  pages = {2520--2522},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/bts480},
  urldate = {2023-03-22},
  abstract = {Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/kosterSnakemakescalablebioinformatics2012.pdf}
}

@article{kumarCenterExcellenceMobile2017,
  title = {Center of {{Excellence}} for {{Mobile Sensor Data-to-Knowledge}} ({{MD2K}})},
  author = {Kumar, Santosh and Abowd, Gregory and Abraham, William T. and {al'Absi}, Mustafa and Chau, Duen Horng and Ertin, Emre and Estrin, Deborah and Ganesan, Deepak and Hnat, Timothy and Hossain, Syed Monowar and Ives, Zachary and Kerr, Jacqueline and Marlin, Benjamin M. and Murphy, Susan and Rehg, James M. and {Nahum-Shani}, Inbal and Shetty, Vivek and Sim, Ida and Spring, Bonnie and Srivastava, Mani and Wetter, Dave},
  year = {2017},
  month = apr,
  journal = {IEEE Pervasive Computing},
  volume = {16},
  number = {2},
  pages = {18--22},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2017.29},
  urldate = {2023-03-27},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/kumarCenterExcellenceMobile2017.pdf}
}

@inproceedings{leeDataProcessingPipeline2023,
  title = {Data {{Processing Pipeline}} of {{Short-Term Depression Detection}} with {{Large-Scale Dataset}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BigComp}})},
  author = {Lee, Yonggeon and Noh, Youngtae and Lee, Uichin},
  year = {2023},
  month = feb,
  pages = {391--392},
  publisher = {{IEEE}},
  address = {{Jeju, Korea, Republic of}},
  doi = {10.1109/BigComp57234.2023.00095},
  urldate = {2023-03-26},
  abstract = {Depression is a common, recurring mental disorder that causes significant impairment in people's lives. In recent years, ubiquitous computing using mobile phones can monitor behavioral patterns relevant to depressive symptoms in-the-wild. In this paper, we propose data processing pipeline of short-term depression detection using mobile sensor data. We build a group model classified by depression severity for capturing depressive mood in a short-period time to handle data quality and data imbalance problem in a large-scale dataset. We expect the group model to identify and characterize digital phenotype representing each depressive group as a middle step toward personalization.},
  isbn = {978-1-66547-578-5},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/leeDataProcessingPipeline2023.pdf}
}

@article{linTransferLearningBased2018,
  title = {Transfer {{Learning Based Traffic Sign Recognition Using Inception-v3 Model}}},
  author = {Lin, Chunmian and Li, Lin and Luo, Wenting and Wang, Kelvin C. P. and Guo, Jiangang},
  year = {2018},
  month = aug,
  journal = {Periodica Polytechnica Transportation Engineering},
  volume = {47},
  number = {3},
  pages = {242--250},
  issn = {1587-3811, 0303-7800},
  doi = {10.3311/PPtr.11480},
  urldate = {2023-03-20},
  abstract = {Traffic sign recognition is critical for advanced driver assistant system and road infrastructure survey. Traditional traffic sign recognition algorithms can't efficiently recognize traffic signs due to its limitation, yet deep learning-based technique requires huge amount of training data before its use, which is time consuming and labor intensive. In this study, transfer learning-based method is introduced for traffic sign recognition and classification, which significantly reduces the amount of training data and alleviates computation expense using Inception-v3 model. In our experiment, Belgium Traffic Sign Database is chosen and augmented by data pre-processing technique. Subsequently the layer-wise features extracted using different convolution and pooling operations are compared and analyzed. Finally transfer learning-based model is repetitively retrained several times with fine-tuning parameters at different learning rate, and excellent reliability and repeatability are observed based on statistical analysis. The results show that transfer learning model can achieve a high-level recognition performance in traffic sign recognition, which is up to 99.18 \% of recognition accuracy at 0.05 learning rate (average accuracy of 99.09 \%). This study would be beneficial in other traffic infrastructure recognition such as road lane marking and roadside protection facilities, and so on.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/linTransferLearningBased2018.pdf}
}

@article{liuMachineVisionBased2019,
  title = {Machine {{Vision Based Traffic Sign Detection Methods}}: {{Review}}, {{Analyses}} and {{Perspectives}}},
  shorttitle = {Machine {{Vision Based Traffic Sign Detection Methods}}},
  author = {Liu, Chunsheng and Li, Shuang and Chang, Faliang and Wang, Yinhai},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {86578--86596},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2924947},
  urldate = {2023-03-20},
  abstract = {Traffic signs recognition (TSR) is an important part of some advanced driver-assistance systems (ADASs) and auto driving systems (ADSs). As the first key step of TSR, traffic sign detection (TSD) is a challenging problem because of different types, small sizes, complex driving scenes, and occlusions. In recent years, there have been a large number of TSD algorithms based on machine vision and pattern recognition. In this paper, a comprehensive review of the literature on TSD is presented. We divide the reviewed detection methods into five main categories: color-based methods, shape-based methods, color- and shape-based methods, machine-learning-based methods, and LIDAR-based methods. The methods in each category are also classified into different subcategories for understanding and summarizing the mechanisms of different methods. For some reviewed methods that lack comparisons on public datasets, we reimplemented part of these methods for comparison. The experimental comparisons and analyses are presented on the reported performance and the performance of our reimplemented methods. Furthermore, future directions and recommendations of the TSD research are given to promote the development of the TSD.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuMachineVisionBased2019.pdf}
}

@article{liuMRCNNMultiScaleRegionBased2019,
  title = {{{MR-CNN}}: {{A Multi-Scale Region-Based Convolutional Neural Network}} for {{Small Traffic Sign Recognition}}},
  shorttitle = {{{MR-CNN}}},
  author = {Liu, Zhigang and Du, Juan and Tian, Feng and Wen, Jiazheng},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {57120--57128},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2913882},
  urldate = {2023-03-20},
  abstract = {Small traffic sign recognition is a challenging problem in computer vision, and its accuracy is important to the safety of intelligent transportation systems (ITS). In this paper, we propose the multiscale region-based convolutional neural network (MR-CNN). At the detection stage, MR-CNN uses a multiscale deconvolution operation to up-sample the features of the deeper convolution layers and concatenates them to those of the shallow layer to construct the fused feature map. The fused feature map has the ability to generate fewer region proposals and achieve higher recall values. At the classification stage, we leverage the multi-scale contextual regions to exploit the information surrounding a given object proposal and construct the fused feature for the fully connected layers. The fused feature map inside the region proposal network (RPN) focuses primarily on improving the image resolution and semantic information for small traffic sign detection, while outside the RPN, the fused feature enhances the feature representation by leveraging the contextual information. Finally, we evaluated MR-CNN on the largest dataset, TsinghuaTencent 100K, which is suitable for our problem and more challenging than the GTSDB and GTSRB datasets. The final experimental results indicate that the MR-CNN is superior at detecting small traffic signs, and that it achieves the state-of-the-art performance compared with other methods.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuMRCNNMultiScaleRegionBased2019.pdf}
}

@article{liuTSingNetScaleawarecontextrich2021,
  title = {{{TSingNet}}: {{Scale-aware}} and Context-Rich Feature Learning for Traffic Sign Detection and Recognition in the Wild},
  shorttitle = {{{TSingNet}}},
  author = {Liu, Yuanyuan and Peng, Jiyao and Xue, Jing-Hao and Chen, Yongquan and Fu, Zhang-Hua},
  year = {2021},
  month = aug,
  journal = {Neurocomputing},
  volume = {447},
  pages = {10--22},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.03.049},
  urldate = {2023-03-20},
  abstract = {Traffic sign detection and recognition in the wild is a challenging task. Existing techniques are often incapable of detecting small or occluded traffic signs because of the scale variation and context loss, which causes semantic gaps between multiple scales. We propose a new traffic sign detection network (TSingNet), which learns scale-aware and context-rich features to effectively detect and recognize small and occluded traffic signs in the wild. Specifically, TSingNet first constructs an attention-driven bilateral feature pyramid network, which draws on both bottom-up and top-down subnets to dually circulate low-, mid-, and high-level foreground semantics in scale self-attention learning. This is to learn scaleaware foreground features and thus narrow down the semantic gaps between multiple scales. An adaptive receptive field fusion block with variable dilation rates is then introduced to exploit context-rich representation and suppress the influence of occlusion at each scale. TSingNet is end-to-end trainable by joint minimization of the scale-aware loss and multi-branch fusion losses, this adds a few parameters but significantly improves the detection performance. In extensive experiments with three challenging traffic sign datasets (TT100K, STSD and DFG), TSingNet outperformed state-of-the-art methods for traffic sign detection and recognition in the wild.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/liuTSingNetScaleawarecontextrich2021.pdf}
}

@article{merikangasRealtimeMobileMonitoring2019,
  title = {Real-Time {{Mobile Monitoring}} of the {{Dynamic Associations Among Motor Activity}}, {{Energy}}, {{Mood}}, and {{Sleep}} in {{Adults With Bipolar Disorder}}},
  author = {Merikangas, Kathleen Ries and Swendsen, Joel and Hickie, Ian B. and Cui, Lihong and Shou, Haochang and Merikangas, Alison K. and Zhang, Jihui and Lamers, Femke and Crainiceanu, Ciprian and Volkow, Nora D. and Zipunnikov, Vadim},
  year = {2019},
  month = feb,
  journal = {JAMA Psychiatry},
  volume = {76},
  number = {2},
  pages = {190},
  issn = {2168-622X},
  doi = {10.1001/jamapsychiatry.2018.3546},
  urldate = {2023-03-26},
  abstract = {OBJECTIVES To examine the directional associations among motor activity, energy, mood, and sleep using mobile monitoring in a community-identified sample, and to evaluate whether these within-day associations differ between people with a history of bipolar or other mood disorders and controls without mood disorders. DESIGN, SETTING, AND PARTICIPANTS This study used a nested case-control design of 242 adults, a subsample of a community-based sample of adults. Probands were recruited by mail from the greater Washington, DC, metropolitan area from January 2005 to June 2013. Enrichment of the sample for mood disorders was provided by volunteers or referrals from the National Institutes of Health Clinical Center or by participants in the National Institute of Mental Health Mood and Anxiety Disorders Program. The inclusion criteria were the ability to speak English, availability to participate, and consent to contact at least 2 living first-degree relatives. Data analysis was performed from June 2013 through July 2018. MAIN OUTCOMES AND MEASURES Motor activity and sleep duration data were obtained from minute-to-minute activity counts from an actigraphy device worn on the nondominant wrist for 2 weeks. Mood and energy levels were assessed by subjective analogue ratings on the ecological momentary assessment (using a personal digital assistant) by participants 4 times per day for 2 weeks. RESULTS Of the total 242 participants, 92 (38.1\%) were men and 150 (61.9\%) were women, with a mean (SD) age of 48 (16.9) years. Among the participants, 54 (22.3\%) had bipolar disorder (25 with bipolar I; 29 with bipolar II), 91 (37.6\%) had major depressive disorder, and 97 (40.1\%) were controls with no history of mood disorders. A unidirectional association was found between motor activity and subjective mood level ({$\beta$} = \textendash 0.018, P = .04). Bidirectional associations were observed between motor activity ({$\beta$} = 0.176; P = .03) and subjective energy level ({$\beta$} = 0.027; P = .03) as well as between motor activity ({$\beta$} = \textendash 0.027; P = .04) and sleep duration ({$\beta$} = \textendash 0.154; P = .04). Greater cross-domain reactivity was observed in bipolar disorder across all outcomes, including motor activity, sleep, mood, and energy. CONCLUSIONS AND RELEVANCE These findings suggest that interventions focused on motor activity and energy may have greater efficacy than current approaches that target depressed mood; both active and passive tracking of multiple regulatory systems are important in designing therapeutic targets.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/merikangasRealtimeMobileMonitoring2019.pdf}
}

@inproceedings{morrisonTigerAwareInnovativeMobile2018,
  title = {{{TigerAware}}: {{An Innovative Mobile Survey}} and {{Sensor Data Collection}} and {{Analytics System}}},
  shorttitle = {{{TigerAware}}},
  booktitle = {2018 {{IEEE Third International Conference}} on {{Data Science}} in {{Cyberspace}} ({{DSC}})},
  author = {Morrison, William and Guerdan, Luke and Kanugo, Jayanth and Trull, Timothy and Shang, Yi},
  year = {2018},
  month = jun,
  pages = {115--122},
  publisher = {{IEEE}},
  address = {{Guangzhou}},
  doi = {10.1109/DSC.2018.00025},
  urldate = {2023-03-22},
  abstract = {From health to community assessment, mobile phones have become a cornerstone of data collection across many areas of research. However, mobile phone-based studies are difficult to develop and deploy, often requiring in house development teams and large portions of research budgets. In this paper, an innovative system called TigerAware is presented to address this issue. TigerAware is developed to offer a generic and customizable tool, which allows researchers to create surveys to collect a wide range of data, including but not limited to question responses, on device sensor data, such as GPS data, and external sensor data, such as blood alcohol level from a Bluetooth breathalyzer. TigerAware is highly modular and uses advanced Web and mobile technologies to incorporate diverse data sources with a rich set of survey question types, requiring little development work by researchers for their individualized studies. TigerAware has been applied to a focus group and several pilot studies and shown excellent capabilities to be easily adapted and deployed for new types of data collection and analytics tasks and a wide range of research fields.},
  isbn = {978-1-5386-4210-8},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/morrisonTigerAwareInnovativeMobile2018.pdf}
}

@misc{onnela-labForest2023,
  title = {Forest},
  author = {{onnela-lab}},
  year = {2023},
  month = mar,
  journal = {Forest},
  urldate = {2023-03-27},
  howpublished = {https://forest.beiwe.org/en/latest/}
}

@article{onnelaBeiwedatacollection2021,
  title = {Beiwe: {{A}} Data Collection Platform for High-Throughput Digital Phenotyping},
  shorttitle = {Beiwe},
  author = {Onnela, Jukka-Pekka and Dixon, Caleb and Griffin, Keary and Jaenicke, Tucker and Minowada, Leila and Esterkin, Sean and Siu, Alvin and Zagorsky, Josh and Jones, Eli},
  year = {2021},
  month = dec,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {68},
  pages = {3417},
  issn = {2475-9066},
  doi = {10.21105/joss.03417},
  urldate = {2023-03-23},
  abstract = {Beiwe is a high-throughput data collection platform for smartphone-based digital phenotyping. It has been in development and use since 2013. Beiwe consists of two native front-end applications: one for Android (written in Java and Kotlin) and one for iOS (written in Swift and Objective-C). The Beiwe back-end, which is based on Amazon Web Services (AWS), has been implemented primarily in Python 3.6, but it also makes use of Django for ORM and Flask for API and web servers. It uses several AWS services, such as S3 for flat file storage, EC2 virtual servers for data processing, Elastic Beanstalk for orchestration, and RDS for PostgreSQL database engine. Most smartphone applications use software development kits (SDKs) that generate unvalidated behavioral summary measures using closed proprietary algorithms. These applications do not meet the high standards of reproducible science, and often require researchers to modify their scientific questions based on what data happens to be available. In contrast, Beiwe collects raw sensor and phone use data, and its data collection parameters can be customized to address specific scientific questions of interest. Collection of raw data also improves reproducibility of studies and enables re-analyses of data and pooling of data across studies. Every aspect of Beiwe data collection is fully customizable, including which sensors to sample, how frequently to sample them, whether to add Gaussian noise to GPS location, whether to use Wi-Fi or cellular data for uploads, how frequently to upload data, specification of surveys and their response options, and skip logic. All study settings are captured in a JSON-formatted configuration file, which can be exported from and imported to Beiwe to enhance transparency and reproducibility of studies.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/onnelaBeiwedatacollection2021.pdf}
}

@article{rabbiIncreasingEngagementSubstance2018,
  title = {Toward {{Increasing Engagement}} in {{Substance Use Data Collection}}: {{Development}} of the {{Substance Abuse Research Assistant App}} and {{Protocol}} for a {{Microrandomized Trial Using Adolescents}} and {{Emerging Adults}}},
  shorttitle = {Toward {{Increasing Engagement}} in {{Substance Use Data Collection}}},
  author = {Rabbi, Mashfiqui and Philyaw Kotov, Meredith and Cunningham, Rebecca and Bonar, Erin E and {Nahum-Shani}, Inbal and Klasnja, Predrag and Walton, Maureen and Murphy, Susan},
  year = {2018},
  month = jul,
  journal = {JMIR Research Protocols},
  volume = {7},
  number = {7},
  pages = {e166},
  issn = {1929-0748},
  doi = {10.2196/resprot.9850},
  urldate = {2023-03-26},
  abstract = {Background: Substance use is an alarming public health issue associated with significant morbidity and mortality. Adolescents and emerging adults are at particularly high risk because substance use typically initiates and peaks during this developmental period. Mobile health apps are a promising data collection and intervention delivery tool for substance-using youth as most teens and young adults own a mobile phone. However, engagement with data collection for most mobile health applications is low, and often, large fractions of users stop providing data after a week of use.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/rabbiIncreasingEngagementSubstance2018.pdf}
}

@inproceedings{rogersDeepLearningYour2019,
  title = {Deep {{Learning}} at {{Your Fingertips}}},
  booktitle = {2019 16th {{IEEE Annual Consumer Communications}} \& {{Networking Conference}} ({{CCNC}})},
  author = {Rogers, Jonathan and Simmons, Dylan and Shah, Milesh and Rowland, Connor and Shang, Yi},
  year = {2019},
  month = jan,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CCNC.2019.8651868},
  urldate = {2023-03-22},
  abstract = {From SurveyMonkey to Google Forms, online surveys have become a cornerstone of modern research. However, these survey platforms lack the ability to provide advanced analysis to researchers, often requiring expensive third-party analytics software to rectify their shortcomings. We propose to solve this problem by adding data analysis capabilities onto TigerAware, an existing data collection platform. TigerAware offers a generic and customizable tool which allows researchers without technical expertise to create, manage, and deploy custom mobile surveys to participants in real-time. We seek to add data analysis functionalities to the TigerAware platform ranging from basic statistics functions to emotion recognition via deep learning. Our analysis platform uses data collected by TigerAware to give researchers real-time analytics throughout the duration of their study. Through our additions to the TigerAware platform, we present a novel all-in-one tool for researchers to facilitate effective survey creation, survey administration, data collection, and data analysis.},
  isbn = {978-1-5386-5553-5},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/rogersDeepLearningYour2019.pdf}
}

@article{shiffmanEcologicalMomentaryAssessment2008,
  title = {Ecological {{Momentary Assessment}}},
  author = {Shiffman, Saul and Stone, Arthur A. and Hufford, Michael R.},
  year = {2008},
  month = apr,
  journal = {Annual Review of Clinical Psychology},
  volume = {4},
  number = {1},
  pages = {1--32},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev.clinpsy.3.022806.091415},
  urldate = {2023-03-24},
  abstract = {Assessment in clinical psychology typically relies on global retrospective self-reports collected at research or clinic visits, which are limited by recall bias and are not well suited to address how behavior changes over time and across contexts. Ecological momentary assessment (EMA) involves repeated sampling of subjects' current behaviors and experiences in real time, in subjects' natural environments. EMA aims to minimize recall bias, maximize ecological validity, and allow study of microprocesses that influence behavior in real-world contexts. EMA studies assess particular events in subjects' lives or assess subjects at periodic intervals, often by random time sampling, using technologies ranging from written diaries and telephones to electronic diaries and physiological sensors. We discuss the rationale for EMA, EMA designs, methodological and practical issues, and comparisons of EMA and recall data. EMA holds unique promise to advance the science and practice of clinical psychology by shedding light on the dynamics of behavior in real-world settings.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/shiffmanEcologicalMomentaryAssessment2008.pdf}
}

@article{straczkiewiczonesizefitsmostwalkingrecognition2023,
  title = {A ``One-Size-Fits-Most'' Walking Recognition Method for Smartphones, Smartwatches, and Wearable Accelerometers},
  author = {Straczkiewicz, Marcin and Huang, Emily J. and Onnela, Jukka-Pekka},
  year = {2023},
  month = feb,
  journal = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {29},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00745-z},
  urldate = {2023-03-27},
  abstract = {Abstract             The ubiquity of personal digital devices offers unprecedented opportunities to study human behavior. Current state-of-the-art methods quantify physical activity using ``activity counts,'' a measure which overlooks specific types of physical activities. We propose a walking recognition method for sub-second tri-axial accelerometer data, in which activity classification is based on the inherent features of walking: intensity, periodicity, and duration. We validate our method against 20 publicly available, annotated datasets on walking activity data collected at various body locations (thigh, waist, chest, arm, wrist). We demonstrate that our method can estimate walking periods with high sensitivity and specificity: average sensitivity ranged between 0.92 and 0.97 across various body locations, and average specificity for common daily activities was typically above 0.95. We also assess the method's algorithmic fairness to demographic and anthropometric variables and measurement contexts (body location, environment). Finally, we release our method as open-source software in Python and MATLAB.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/straczkiewiczonesizefitsmostwalkingrecognition2023.pdf}
}

@article{tabernikDeepLearningLargeScale2020,
  title = {Deep {{Learning}} for {{Large-Scale Traffic-Sign Detection}} and {{Recognition}}},
  author = {Tabernik, Domen and Skocaj, Danijel},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {21},
  number = {4},
  pages = {1427--1440},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2019.2913588},
  urldate = {2023-03-20},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/tabernikDeepLearningLargeScale2020.pdf}
}

@article{temelTrafficSignDetection2020,
  title = {Traffic {{Sign Detection}} under {{Challenging Conditions}}: {{A Deeper Look Into Performance Variations}} and {{Spectral Characteristics}}},
  shorttitle = {Traffic {{Sign Detection}} under {{Challenging Conditions}}},
  author = {Temel, Dogancan and Chen, Min-Hung and AlRegib, Ghassan},
  year = {2020},
  month = sep,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {21},
  number = {9},
  eprint = {1908.11262},
  primaryclass = {cs, eess},
  pages = {3663--3673},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2019.2931429},
  urldate = {2023-03-20},
  abstract = {Traffic signs are critical for maintaining the safety and efficiency of our roads. Therefore, we need to carefully assess the capabilities and limitations of automated traffic sign detection systems. Existing traffic sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. To overcome the shortcomings in existing datasets, we introduced the CURE-TSDReal dataset, which is based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. We test the performance of two benchmark algorithms and show that severe conditions can result in an average performance degradation of 29\% in precision and 68\% in recall. We investigate the effect of challenging conditions through spectral analysis and show that challenging conditions can lead to distinct magnitude spectrum characteristics. Moreover, we show that mean magnitude spectrum of changes in video sequences under challenging conditions can be an indicator of detection performance. CURE-TSD-Real dataset is available online at https://github.com/olivesgatech/CURE-TSD.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,I.2,I.4,I.5},
  file = {/Users/reyvababtista/Projects/Papers/temelTrafficSignDetection2020.pdf}
}

@misc{tervenComprehensiveReviewYOLO2023,
  title = {A {{Comprehensive Review}} of {{YOLO}}: {{From YOLOv1}} to {{YOLOv8}} and {{Beyond}}},
  shorttitle = {A {{Comprehensive Review}} of {{YOLO}}},
  author = {Terven, Juan and {Cordova-Esparza}, Diana},
  year = {2023},
  month = apr,
  number = {arXiv:2304.00501},
  eprint = {arXiv:2304.00501},
  publisher = {{arXiv}},
  urldate = {2023-04-10},
  abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO to YOLOv8. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/tervenComprehensiveReviewYOLO2023.pdf}
}

@article{turakhiaRationaledesignlargescale2019,
  title = {Rationale and Design of a Large-Scale, App-Based Study to Identify Cardiac Arrhythmias Using a Smartwatch: {{The Apple Heart Study}}},
  shorttitle = {Rationale and Design of a Large-Scale, App-Based Study to Identify Cardiac Arrhythmias Using a Smartwatch},
  author = {Turakhia, Mintu P. and Desai, Manisha and Hedlin, Haley and Rajmane, Amol and Talati, Nisha and Ferris, Todd and Desai, Sumbul and Nag, Divya and Patel, Mithun and Kowey, Peter and Rumsfeld, John S. and Russo, Andrea M. and Hills, Mellanie True and Granger, Christopher B. and Mahaffey, Kenneth W. and Perez, Marco V.},
  year = {2019},
  month = jan,
  journal = {American Heart Journal},
  volume = {207},
  pages = {66--75},
  issn = {00028703},
  doi = {10.1016/j.ahj.2018.09.002},
  urldate = {2023-03-26},
  abstract = {Background Smartwatch and fitness band wearable consumer electronics can passively measure pulse rate from the wrist using photoplethysmography (PPG). Identification of pulse irregularity or variability from these data has the potential to identify atrial fibrillation or atrial flutter (AF, collectively). The rapidly expanding consumer base of these devices allows for detection of undiagnosed AF at scale. Methods The Apple Heart Study is a prospective, single arm pragmatic study that has enrolled 419,093 participants (NCT03335800). The primary objective is to measure the proportion of participants with an irregular pulse detected by the Apple Watch (Apple Inc, Cupertino, CA) with AF on subsequent ambulatory ECG patch monitoring. The secondary objectives are to: 1) characterize the concordance of pulse irregularity notification episodes from the Apple Watch with simultaneously recorded ambulatory ECGs; 2) estimate the rate of initial contact with a health care provider within 3 months after notification of pulse irregularity. The study is conducted virtually, with screening, consent and data collection performed electronically from within an accompanying smartphone app. Study visits are performed by telehealth study physicians via video chat through the app, and ambulatory ECG patches are mailed to the participants. Conclusions The results of this trial will provide initial evidence for the ability of a smartwatch algorithm to identify pulse irregularity and variability which may reflect previously unknown AF. The Apple Heart Study will help provide a foundation for how wearable technology can inform the clinical approach to AF identification and screening. (Am Heart J 2019;207:66-75.)},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/turakhiaRationaledesignlargescale2019.pdf}
}

@inproceedings{vaizmanExtraSensoryAppData2018,
  title = {{{ExtraSensory App}}: {{Data Collection In-the-Wild}} with {{Rich User Interface}} to {{Self-Report Behavior}}},
  shorttitle = {{{ExtraSensory App}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Vaizman, Yonatan and Ellis, Katherine and Lanckriet, Gert and Weibel, Nadir},
  year = {2018},
  month = apr,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Montreal QC Canada}},
  doi = {10.1145/3173574.3174128},
  urldate = {2023-03-26},
  abstract = {We introduce a mobile app for collecting in-the-wild data, including sensor measurements and self-reported labels describing people's behavioral context (e.g. driving, eating, in class, shower). Labeled data is necessary for developing contextrecognition systems that serve health monitoring, aging care, and more. Acquiring labels without observers is challenging and previous solutions compromised ecological validity, range of behaviors, or amount of data. Our user interface combines past and near-future self-reporting of combinations of relevant context-labels. We deployed the app on the personal smartphones of 60 users and analyzed quantitative data collected in-the-wild and qualitative user-experience reports. The interface's flexibility was important to gain frequent, detailed labels, support diverse behavioral situations, and engage different users: most preferred reporting their past behavior through a daily journal, but some preferred reporting what they're about to do. We integrated insights from this work back into the app, which we make available to researchers for conducting in-the-wild studies.},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/vaizmanExtraSensoryAppData2018.pdf}
}

@article{vegaReproducibleAnalysisPipeline2021,
  title = {Reproducible {{Analysis Pipeline}} for {{Data Streams}}: {{Open-Source Software}} to {{Process Data Collected With Mobile Devices}}},
  shorttitle = {Reproducible {{Analysis Pipeline}} for {{Data Streams}}},
  author = {Vega, Julio and Li, Meng and Aguillera, Kwesi and Goel, Nikunj and Joshi, Echhit and Khandekar, Kirtiraj and Durica, Krina C. and Kunta, Abhineeth R. and Low, Carissa A.},
  year = {2021},
  month = nov,
  journal = {Frontiers in Digital Health},
  volume = {3},
  pages = {769823},
  issn = {2673-253X},
  doi = {10.3389/fdgth.2021.769823},
  urldate = {2023-03-22},
  abstract = {Smartphone and wearable devices are widely used in behavioral and clinical research to collect longitudinal data that, along with ground truth data, are used to create models of human behavior. Mobile sensing researchers often program data processing and analysis code from scratch even though many research teams collect data from similar mobile sensors, platforms, and devices. This leads to significant inefficiency in not being able to replicate and build on others' work, inconsistency in quality of code and results, and lack of transparency when code is not shared alongside publications. We provide an overview of Reproducible Analysis Pipeline for Data Streams (RAPIDS), a reproducible pipeline to standardize the preprocessing, feature extraction, analysis, visualization, and reporting of data streams coming from mobile sensors. RAPIDS is formed by a group of R and Python scripts that are executed on top of reproducible virtual environments, orchestrated by a workflow management system, and organized following a consistent file structure for data science projects. We share open source, documented, extensible and tested code to preprocess, extract, and visualize behavioral features from data collected with any Android or iOS smartphone sensing app as well as Fitbit and Empatica wearable devices. RAPIDS allows researchers to process mobile sensor data in a rigorous and reproducible way. This saves time and effort during the data analysis phase of a project and facilitates sharing analysis workflows alongside publications.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/vegaReproducibleAnalysisPipeline2021.pdf}
}

@article{wangHOPESIntegrativeDigital2021,
  title = {{{HOPES}}: {{An Integrative Digital Phenotyping Platform}} for {{Data Collection}}, {{Monitoring}}, and {{Machine Learning}}},
  shorttitle = {{{HOPES}}},
  author = {Wang, Xuancong and Vouk, Nikola and Heaukulani, Creighton and Buddhika, Thisum and Martanto, Wijaya and Lee, Jimmy and Morris, Robert JT},
  year = {2021},
  month = mar,
  journal = {Journal of Medical Internet Research},
  volume = {23},
  number = {3},
  pages = {e23984},
  issn = {1438-8871},
  doi = {10.2196/23984},
  urldate = {2023-03-27},
  abstract = {The collection of data from a personal digital device to characterize current health conditions and behaviors that determine how an individual's health will evolve has been called digital phenotyping. In this paper, we describe the development of and early experiences with a comprehensive digital phenotyping platform: Health Outcomes through Positive Engagement and Self-Empowerment (HOPES). HOPES is based on the open-source Beiwe platform but adds a wider range of data collection, including the integration of wearable devices and further sensor collection from smartphones. Requirements were partly derived from a concurrent clinical trial for schizophrenia that required the development of significant capabilities in HOPES for security, privacy, ease of use, and scalability, based on a careful combination of public cloud and on-premises operation. We describe new data pipelines to clean, process, present, and analyze data. This includes a set of dashboards customized to the needs of research study operations and clinical care. A test use case for HOPES was described by analyzing the digital behavior of 22 participants during the SARS-CoV-2 pandemic.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/wangHOPESIntegrativeDigital2021.pdf}
}

@article{wuMultimodaldatacollection2021,
  title = {Multi-Modal Data Collection for Measuring Health, Behavior, and Living Environment of Large-Scale Participant Cohorts},
  author = {Wu, Congyu and Fritz, Hagen and Bastami, Sepehr and Maestre, Juan P and Thomaz, Edison and Julien, Christine and Castelli, Darla M and {de~Barbaro}, Kaya and Bearman, Sarah Kate and Harari, Gabriella M and Cameron~Craddock, R and Kinney, Kerry A and Gosling, Samuel D and Schnyer, David M and Nagy, Zoltan},
  year = {2021},
  month = jun,
  journal = {GigaScience},
  volume = {10},
  number = {6},
  pages = {giab044},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giab044},
  urldate = {2023-04-11},
  abstract = {Abstract                            Background               As mobile technologies become ever more sensor-rich, portable, and ubiquitous, data captured by smart devices are lending rich insights into users' daily lives with unprecedented comprehensiveness and ecological validity. A number of human-subject studies have been conducted to examine the use of mobile sensing to uncover individual behavioral patterns and health outcomes, yet minimal attention has been placed on measuring living environments together with other human-centered sensing data. Moreover, the participant sample size in most existing studies falls well below a few hundred, leaving questions open about the reliability of findings on the relations between mobile sensing signals and human outcomes.                                         Results               To address these limitations, we developed a home environment sensor kit for continuous indoor air quality tracking and deployed it in conjunction with smartphones, Fitbits, and ecological momentary assessments in a cohort study of up to 1,584 college student participants per data type for 3 weeks. We propose a conceptual framework that systematically organizes human-centric data modalities by their temporal coverage and spatial freedom. Then we report our study procedure, technologies and methods deployed, and descriptive statistics of the collected data that reflect the participants' mood, sleep, behavior, and living environment.                                         Conclusions               We were able to collect from a large participant cohort satisfactorily complete multi-modal sensing and survey data in terms of both data continuity and participant adherence. Our novel data and conceptual development provide important guidance for data collection and hypothesis generation in future human-centered sensing studies.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/wuMultimodaldatacollection2021.pdf}
}

@article{yuanFasterLightDetection2023,
  title = {Faster {{Light Detection Algorithm}} of {{Traffic Signs Based}} on {{YOLOv5s-A2}}},
  author = {Yuan, Xu and Kuerban, Alifu and Chen, Yixiao and Lin, Wenlong},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {19395--19404},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3204818},
  urldate = {2023-04-10},
  abstract = {Traffic sign recognition systems have been applied to advanced driving assistance and automatic driving systems to help drivers obtain important road information accurately. The current mainstream detection methods have high accuracy in this task, but the number of model parameters is large, and the detection speed is slow. Based on YOLOv5s as the basic framework, this paper proposes YOLOv5S-A2, which can improve the detection speed and reduce the model size at the cost of reducing the detection accuracy. Firstly, a data augmentation strategy is proposed by combining various operations to alleviate the problem of unbalanced class instances. Secondly, we proposed a path aggregation module for Feature Pyramid Network (FPN) to make new horizontal connections. It can enhance multi-scale feature representation capability and compensate for the loss of feature information. Thirdly, an attention detection head module is proposed to solve the aliasing effect in cross-scale fusion and enhance the representation of predictive features. Experiments on Tsinghua-Tencent 100K dataset (TT100K) show that our method can achieve more remarkable performance improvement and faster inference speed than other advanced technologies. Our method achieves 87.3\% mean average precision (mAP), surpassing the original model's 7.9\%, and the frames per second (FPS) value is maintained at 87.7. To show generality, we tested it on the German Traffic Sign Detection Benchmark (GTSDB) without tuning and obtained an average precision of 94.1\%, and the FPS value is maintained at about 105.3. In addition, the number of YOLOv5s-A2 parameters is about 7.9 M.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/yuanFasterLightDetection2023.pdf}
}

@inproceedings{yuReproducibleWorkflowsExploring2022,
  title = {Reproducible {{Workflows}} for {{Exploring}} and {{Modeling EMA Data}}},
  booktitle = {2022 {{IEEE}} 8th {{International Conference}} on {{Collaboration}} and {{Internet Computing}} ({{CIC}})},
  author = {Yu, Ching-Yun and Shang, Yi and Trull, Timothy},
  year = {2022},
  month = dec,
  pages = {117--124},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1109/CIC56439.2022.00026},
  urldate = {2023-03-22},
  abstract = {Improper use of substances like cannabis may lead to physical, emotional, economic, and social problems. Therefore, it is significant to elucidate the inter-individual and intraindividual influences along with contextual influences that predict the use of cannabis. TigerAware is a mobile survey data collection platform that holds unique promise to advance research in addiction and substance use. This paper presents a novel method to support Ecological Momentary Assessment (EMA) studies. We propose to extract useful information from TigerAware survey data using data mining and machine learning methods, and structure customizable survey analyses into reproducible workflows. Through our analysis pipeline for EMA, researchers are able to discover meaningful information from survey data with minimal duplication of effort and improve the efficiency and rigor of the process.},
  isbn = {978-1-66547-300-2},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/yuReproducibleWorkflowsExploring2022.pdf}
}

@article{zhangAutomatedVisualRecognizability2019,
  title = {Automated {{Visual Recognizability Evaluation}} of {{Traffic Sign Based}} on {{3D LiDAR Point Clouds}}},
  author = {Zhang, Shanxin and Wang, Cheng and Lin, Lili and Wen, Chenglu and Yang, Chenhui and Zhang, Zhemin and Li, Jonathan},
  year = {2019},
  month = jun,
  journal = {Remote Sensing},
  volume = {11},
  number = {12},
  pages = {1453},
  issn = {2072-4292},
  doi = {10.3390/rs11121453},
  urldate = {2023-03-20},
  abstract = {Maintaining the high visual recognizability of traffic signs for traffic safety is a key matter for road network management. Mobile Laser Scanning (MLS) systems provide efficient way of 3D measurement over large-scale traffic environment. This paper presents a quantitative visual recognizability evaluation method for traffic signs in large-scale traffic environment based on traffic recognition theory and MLS 3D point clouds. We first propose the Visibility Evaluation Model (VEM) to quantitatively describe the visibility of traffic sign from any given viewpoint, then we proposed the concept of visual recognizability field and Traffic Sign Visual Recognizability Evaluation Model (TSVREM) to measure the visual recognizability of a traffic sign. Finally, we present an automatic TSVREM calculation algorithm for MLS 3D point clouds. Experimental results on real MLS 3D point clouds show that the proposed method is feasible and efficient.},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/zhangAutomatedVisualRecognizability2019.pdf}
}

@inproceedings{zhuTrafficSignDetectionClassification2016,
  title = {Traffic-{{Sign Detection}} and {{Classification}} in the {{Wild}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhu, Zhe and Liang, Dun and Zhang, Songhai and Huang, Xiaolei and Li, Baoli and Hu, Shimin},
  year = {2016},
  month = jun,
  pages = {2110--2118},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.232},
  urldate = {2023-03-20},
  abstract = {Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify trafficsigns. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available1.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/reyvababtista/Projects/Papers/zhuTrafficSignDetectionClassification2016.pdf}
}

@misc{zouObjectDetection202023,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  year = {2023},
  month = jan,
  number = {arXiv:1905.05055},
  eprint = {arXiv:1905.05055},
  publisher = {{arXiv}},
  urldate = {2023-04-11},
  abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today's object detection technique as a revolution driven by deep learning, then back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This paper extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/reyvababtista/Projects/Papers/zouObjectDetection202023.pdf}
}
